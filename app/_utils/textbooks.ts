export const textbooks: { [key: string]: { [key: string]: string[] } } = {
    "berkeley-cs-61b": {
        "23.-graph-traversals-and-implementations/23.1-bfs-and-dfs.md": [
            "1. # 23.1 BFS & DFS",
            '2. {% embed url="https://www.youtube.com/watch?v=YgPtwbWpLaM" %}\nProfessor Hug\'s Lecture on Graphs\n{% endembed %}',
            "3. In [Chapter 22.4](../22.-tree-traversals-and-graphs/22.4-graph-problems.md), we developed DFS (Depth First Search) Traversal for graphs. In DFS, we visit down the entire lineage of our first child before we even begin to look at our second child - we literally search _**depth first**_.",
            "4. Here, we will talk about BFS (Breadth First Search) (also known as Level Order Traversal). In BFS, we visit all of our immediate children before continuing on to any of our grandchildren. In other words, we visit all nodes 1 edges from our source. Then, all nodes 2 edges from our source, etc.",
            "5. The pseudocode for BFS is as follows:",
            "6. ```\nInitialize the fringe, an empty queue \n    add the starting vertex to the fringe\n    mark the starting vertex\n    while fringe is not empty:\n        remove vertex v from the fringe\n        for each neighbor n of vertex v:\n            if n is not marked:\n                add n to fringe\n                mark n\n                set edgeTo[n] = v\n                set distTo[n] = distTo[v] + 1\n```",
            "7. A _fringe_ is just a term we use for the data structure we are using to store the nodes on the frontier of our traversal's discovery process (the next nodes it is waiting to look at). For BFS, we use a queue for our fringe.",
            "8. `edgeTo[...]` is a map that helps us track how we got to node `n`; we got to it by following the edge from `v` to to `n`.",
            "9. `distTo[...]` is a map that helps us track how far `n` is from the starting vertex. Assuming that each edge is worth a distance of `1`, then the distance to `n` is just one more than the distance to get to `v`. Why? We can use the way we know how to get to `v`, then pay one more to arrive at `n` via the edge that necessarily exists between `v` and `n` (it must exist since in the `for` loop header, `n` is defined as a neighbor of `v`).",
            "10. This [slide deck](https://docs.google.com/presentation/d/1JoYCelH4YE6IkSMq\\_LfTJMzJ00WxDj7rEa49gYmAtc4/edit#slide=id.g76e0dad85\\_2\\_380) illustrates how this pseudocode can be carried out on an example graph.",
            '11. #### DFS vs BFS <a href="#dfs-vs-bfs" id="dfs-vs-bfs"></a>',
            "12. <details>",
            "13. <summary><strong>Question 18.1</strong>: What graph traversal algorithm uses a stack rather than a queue for its fringe?</summary>",
            "14. **Answer 18.1**: DFS traversal.",
            "15. </details>",
            "16. Note however that DFS and BFS differ in more than just their fringe data structure. They differ in the order of marking nodes. For DFS we mark nodes only once we visit a node - aka pop it from the fringe. As a result, it's possible to have multiple instances of the same node on the stack at a time if that node has been queued but not visited yet. With BFS we mark nodes as soon as we add them to the fringe so this is not possible.",
            "17. Recursive DFS implements this naturally via the recursive stack frames; iterative DFS implements it manually:",
            "18. ```\nInitialize the fringe, an empty stack\n    push the starting vertex on the fringe\n    while fringe is not empty:\n        pop a vertex off the fringe\n        if vertex is not marked:\n            mark the vertex\n            visit vertex\n            for each neighbor of vertex:\n                if neighbor not marked:\n                    push neighbor to fringe\n```",
            "19. \\\n",
        ],
        "23.-graph-traversals-and-implementations/README.md": [
            "1. ---\ndescription: By William Lee and Mihir Mirchandani\n---",
            "2. # 23. Graph Traversals and Implementations",
            "3. ",
        ],
        "23.-graph-traversals-and-implementations/23.2-representing-graphs.md":
            [
                "1. ---\ndescription: How do we create a graph in Java?\n---",
                "2. # 23.2 Representing Graphs",
                '3. {% embed url="https://www.youtube.com/watch?v=YgPtwbWpLaM" %}\nProfessor Hug\'s Lecture on Graphs\n{% endembed %}',
                "4. We will discuss our choice of **API**, and also the **underlying data structures** used to represent the graph. Our decisions can have profound implications on our _runtime_, _memory usage_, and _difficulty of implementing various graph algorithms_.",
                '5. ### Graph API <a href="#graph-api" id="graph-api"></a>',
                "6. An API (Application Programming Interface) is a list of methods available to a user of our class, including the method signatures (what arguments/parameters each function accepts) and information regarding their behaviors. You have already seen APIs from the Java developers for the classes they provide, such as the [Deque](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Deque.html).",
                "7. For our Graph API, let's use the common convention of assigning each unique node to an integer number. This can be done by maintaining a map which can tell us the integer assigned to each original node label. Doing so allows us to define our API to work with integers specifically, rather than introducing the need for generic types.",
                "8. We can then define our API to look something like this perhaps:",
                "9. ```\npublic class Graph {\n  public Graph(int V):               // Create empty graph with v vertices\n  public void addEdge(int v, int w): // add an edge v-w\n  Iterable<Integer> adj(int v):      // vertices adjacent to v\n  int V():                           // number of vertices\n  int E():                           // number of edges\n...\n```",
                "10. Clients (people who wish to use our Graph data structure), can then use any of the functions we provide to implement their own algorithms. The methods we provide can have a significant impact on how easy/difficult it may be for our clients to implement particular algorithms.",
                "11. ",
                "12. Now that we know how to draw a graph on paper and understand the basic concepts and definitions, we can now consider how a graph should be represented inside of a computer. We want to be able to get quick answers for the following questions about a graph:",
                "13. * Are given vertices `u` and `v` adjacent?\n* Is vertex `v` incident to a particular edge `e`?\n* What vertices are adjacent to `v`?\n* What edges are incident to `v`?",
                "14. Imagine that we want to represent a graph that looks like this:",
                "15. ![](<../.gitbook/assets/image (81).png>)",
                "16. One data structure we could use to implement this graph is called an _array of adjacency lists_.",
                "17. ## The Adjacency List",
                "18. In an adjacency list, an array is created that has the same size as the number of vertices in the graph. Each position in the array represents one of the vertices in the graph. Each of these positions point to a list. These lists are called adjacency lists, as each element in the list represents a neighbor of the vertex.",
                "19. The array of adjacency lists that represents the above graph looks like this:",
                "20. ![](<../.gitbook/assets/image (68).png>)",
                "21. \\\nAnother data structure we could use to represent the edges in a graph is called an _adjacency matrix_.",
                "22. ## The Adjacency Matrix",
                "23. In this data structure, we have a two dimensional array of size N\u00d7N (where N is the number of vertices) which contains boolean values. The (_i_, _j_)th entry of this matrix is true when there is an edge from _i_ to _j_ and false when no edge exists. Thus, each vertex has a row and a column in the matrix, and the value in that table says true or false whether or not that edge exists.",
                "24. The adjacency matrix that represents the above graph looks like this:",
                "25. ![](<../.gitbook/assets/image (34).png>)",
                '26. ## Efficiency <a href="#efficiency" id="efficiency"></a>',
                "27. Your choice of underlying data structure can impact the runtime and memory usage of your graph. This table from the [slides](https://docs.google.com/presentation/d/11iacyiFt3QUrzo1yAU\\_xoXAjGTH4UzV7o6CR04HYRrI/edit#slide=id.g54593997ea\\_0\\_422) summarizes the efficiencies of each representation for various operations. It is strongly not recommended to directly just copy this on to your cheatsheet for the exams without taking the time to first understand where and how these bounds fundamentally came to be. The lecture contains walkthroughs explaining the rationale in detail behind several of these cells.",
                "28. Further, DFS/BFS on a graph backed by adjacency lists runs in O(V+E), while on a graph backed by an adjacency matrix runs in O(V^2). See the [slides](https://docs.google.com/presentation/d/11iacyiFt3QUrzo1yAU\\_xoXAjGTH4UzV7o6CR04HYRrI/) for help in understanding why.\n",
            ],
        "23.-graph-traversals-and-implementations/23.3-summary.md": [
            "1. # 23.3 Summary",
            "2. ## Graph Traversals Overview",
            "3. * The same traversals that we used on trees can be generalized to graphs. Given a source vertex, we can visit vertices in:\n  * DFS preorder: the order in which DFS is called on each vertex.\n  * DFS postorder: the order in which DFS returns from each vertex.\n  * BFS: the order of distance from the source node (this is level-order in trees).&#x20;",
            "4. ## BFS",
            "5. * Unlike DFS, BFS has a natural solution that is iterative, not recursive. BFS visits a source vertex `s`, then every vertex at distance `1` from `s`, then every vertex at distance `2` from `s`, and so on.\n* BFS uses a _fringe_ of vertices that are next to be explored. In BFS, this fringe is a queue. We enqueue new vertices at the end, and dequeue vertices to visit from the front.\n* BFS can be used to solve the shortest paths problem, given that we want to minimize the number of edges from source to each other vertex. If we want to recover the shortest path from BFS, we need to track the `edgeTo` each vertex during our traversal.",
            "6. ## Graph Implementation",
            "7. * The choice of API for a graph determines how clients must write their code. Certain APIs make some tasks easier and other tasks harder. The choice of API can also affect runtime and memory.\n* Choice of graph implementations include adjacency matrices, lists of edges, and adjacency lists. An adjacency matrix is a 2D boolean array indicating whether any pair of vertices are adjacent. A list of edges is a collection of all edges in the graph.&#x20;\n* The most common approach to graph representation is an adjacency list. In this representation, we maintain a array of lists indexed by vertex number; each index stores all vertices connected to the given vertex.\n",
        ],
        "23.-graph-traversals-and-implementations/23.4-exercises.md": [
            "1. # 23.4 Exercises",
            "2. ## Factual",
            "3. 1. Suppose we want to find the vertex with minimal degree (fewest neighbors). Is an adjacency matrix or adjacency list better?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. An adjacency list would be better: we can just assess the size of each list in constant time versus iterating over all $$V$$ vertices in each row of the adjacency matrix to count the number of neighbors.",
            "7. </details>",
            "8. ## Procedural",
            "9. ![](<../.gitbook/assets/image (71).png>)",
            "10. 1. For the graph above, write the representation for the adjacency matrix representation.\n2. Repeat (1) for the list of edges representation.\n3. Repeat (1) for the adjacency list representation.\n4. Run BFS starting from A for the graph above, and list the order in which vertices are visited. Break ties alphabetically.\n5. Run DFS starting from A for the graph above, and list the postorder/postorder traversals. Break ties alphabetically.",
            "11. <details>",
            "12. <summary>Problem 1</summary>",
            "13. <pre><code><strong>    A  B  C  D  E  F\n</strong><strong>   \n</strong><strong>A   F  T  F  F  F  F\n</strong><strong>B   T  F  T  T  T  F\n</strong><strong>C   F  T  F  F  F  T\n</strong><strong>D   F  T  F  F  F  T\n</strong><strong>E   F  T  F  F  F  F\n</strong><strong>F   F  F  T  T  F  F\n</strong></code></pre>",
            "14. </details>",
            "15. <details>",
            "16. <summary>Problem 2</summary>",
            "17. ```\n[{A, B}, \n{B, A}, {B, C}, {B, D}, {B, E}, \n{C, B}, {C, F},\n{D, B}, {D, F},\n{E, B}, \n{F, C}, {F, D}]\n```",
            "18. </details>",
            "19. <details>",
            "20. <summary>Problem 3</summary>",
            "21. ```\nA: [B]\nB: [A, C, D, E]\nC: [B, F]\nD: [B, F]\nE: [B]\nF: [C, D]\n```",
            "22. </details>",
            "23. <details>",
            "24. <summary>Problem 4</summary>",
            "25. `[A, B, C, D, E, F]`",
            "26. </details>",
            "27. <details>",
            "28. <summary>Problem 5</summary>",
            "29. preorder: `[A, B, C, F, D, E]`",
            "30. postorder: `[D, F, C, E, B, A]`",
            "31. </details>",
            "32. ## Metacognitive",
            "33. 1. Suppose we find some shortest path from `a` to `b` using BFS. Consider a vertex `c` that is on the path between `a` and `b`. What can we say about the shortest path from `c` to `b`?\n2. Problem 4c from the [Spring 2015 Midterm 2](https://drive.google.com/file/d/1uE1QlF4YguWVp8m8UJ97R2xPC4b1NnQ5/view?usp=sharing).",
            "34. <details>",
            "35. <summary>Problem 1</summary>",
            "36. Call the original shortest path from `a` to `b` $$p_{ab}$$, and the path from `c` to `b` along this path $$p^{*}_{cb}$$. If there is some shorter path between `c` and `b` $$p'_{cb}$$, then we could simply take the path from `a` to `c` in the original $$p_{ab}$$, then take $$p'_{cb}$$ to find an even shorter path between `a` and `b`.&#x20;",
            "37. However, we stated earlier that $$p_{ab}$$ is the shortest path between `a` and `b`. This is a contradiction.",
            "38. Thus, the shortest path between `c` and `b` must be on the shortest path between `a` and `b`.",
            "39. </details>",
            "40. <details>",
            "41. <summary>Problem 2</summary>",
            "42. [Solutions](https://drive.google.com/file/d/1IYt4VbzdX4dTekh6cYAC8tigpJ\\_LgljV/view?usp=sharing) are linked here and on the course website.",
            "43. </details>\n",
        ],
        "36.-sorting-and-data-structures-conclusion/README.md": [
            "1. ---\ndescription: By Mihir Mirchandani and William Lee\n---",
            "2. # 36. Sorting and Data Structures Conclusion",
            "3. ",
        ],
        "36.-sorting-and-data-structures-conclusion/36.3-radix-sorting-integers.md":
            [
                "1. ---\ndescription: ft. Obama\n---",
                "2. # 36.3 Radix Sorting Integers",
                '3. {% embed url="https://www.youtube.com/watch?v=k4RRi_ntQc8&ab_channel=weedipikia" %}\nObama knows Bubble sort! This is amazing!\n{% endembed %}',
                '4. {% hint style="info" %}\nBTW: A 32-bit integer is a normal integer in Java and other programming languages that spans 4 billion values. This is something we learned in Hashing chapter 20 and is a 61C concept!\n{% endhint %}',
                "5. ",
                "6. ### Summary of Video above",
                "7. Barack Obama gets a Google Interview question: What is the most efficient way to sort a million 32-bit integers? Obama says that he would recommend not using Bubble sort! That's right folks! Obama knows his 61B :smile:.",
                "8. ### What's the answer?",
                "9. The answer to this question actually is Radix sort because we know that in a very large N limit, Radix sort is simply fastest as it is linear with runtime \u0398(WN) where W is the number of digits and N is the number of integers we are sorting. This is much faster than any Comparison Sort we learned about since the fastest runtimes seem to be \u0398(N\\*log(N)).",
                "10. ### But how do we do it?",
                "11. We don't have a charAt() for every integer. And if we converted all integers to strings, that's a very time expensive operation as well. So how would you LSD radix sort an array of integers? Instead of using charAt, maybe write a helper method like getDthDigit(int N, int d). Example: getDthDigit(15009, 2) = 5.",
                "12. ### LSD Radix Sort on Integers",
                "13. Note that we don't have to work with base 10. What we can instead do is increase this base to have less digits in our total number. This is really getting into 61C territory as we haven't really discussed binary representation of integers yet, but essentially what we want to do is convert to hexadecimal, a base 16 number that is represented with the following digits: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F}. Note that A is 10 and F is 15. 0 - 15 is 16 possible values for a digit! To convert a number from decimal (base 10) to hexadecimal (base 16), we do the following:",
                '14. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-19 at 1.18.26 AM.png" alt=""><figcaption><p>Decimal to Hexadecimal</p></figcaption></figure>',
                "15. Notice that our original number had 6 digits in base 10 and our resulting number has 5 digits! Notice we don't have to go to just base 16. We can go even bigger to base 256 to only have 3 digits! There are small yet amazing optimizations!",
                '16. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-19 at 1.21.11 AM.png" alt=""><figcaption><p>Decimal to Ducentohexaquinquagesimal (Base 256)</p></figcaption></figure>',
                '17. Please do not worry about memorizing what "ducentohexaquinquagesimal" is. The only important thing to learn here is that this conversion to higher bases may result in fewer digits for our LSD and MSD Radix sorts to have faster traversals.',
                "18. However, there is a tradeoff between W, the size of each integer, and the size of the array we need to maintain the counts. The most oprtimal is actually base 256!\n",
            ],
        "36.-sorting-and-data-structures-conclusion/36.4-summary.md": [
            "1. # 36.4 Summary",
            "2. **Radix Sort vs. Comparison Sorts.** In lecture, we used the number of characters examined as a cost model to compare radix sort and comparison sort. For MSD Radix Sort, the worst case is that each character is examined once for NM characters examined. For merge sort, MNlogN is the worst case characters examined. Thus, we can see that merge sort is slower by a factor of logN if character comparisons are an appropriate cost model. Using an empirical analysis, however, we saw that this does not hold true because of lots of background reasons such as the cache, optimized methods, extra copy operations, and overall because our cost model does not account for everything happening.",
            "3. **Just-In-Time Compiler.** The \u201cinterpreter\u201d studies your code as it runs so that when a sequence of code is run many times, it studies and re-implements based on what it learns while running to optimize it. For example, if a LinkedList is created many times in a loop and left unused, it eventually learns to stop creating the LinkedLists since they are never used. With the Just-In-Time compiler disabled, merge sort, from the previous section, is indeed slower than MSD Radix Sort.",
            "4. **Radix Sorting Integers.** When radix sorting integers, we no longer have a charAt method. There are lots of alternative options are stilizing mods and division to write your own getDigit() method or to make each Integer into a String. However, we don\u2019t actually have to stick to base 10 and can instead treat the numbers as base 16, 256, or even base 65536 numbers. Thus, we can reduce the number of digits, which can reduces the runtime since runtime for radix sort depends on alphabet size.\n",
        ],
        "36.-sorting-and-data-structures-conclusion/36.2-the-just-in-time-compiler.md":
            [
                "1. # 36.2 The Just-In-Time Compiler",
                "2. Java\u2019s Just-In-Time Compiler secretly optimizes your code when it runs.",
                "3. * The code you write is not necessarily the code that executes!\n* As your code runs, the \u201cinterpreter\u201d is watching everything that happens.\n  * If some segment of code is called many times, the interpreter actually studies and re-implements your code based on what it learned by watching WHILE ITS RUNNING (!!).\n    * Example: Performing calculations whose results are unused.\n    * See [this video](https://www.youtube.com/watch?v=oH4\\_unx8eJQ) if you\u2019re curious.",
                '4. <figure><img src="../.gitbook/assets/image (143).png" alt=""><figcaption></figcaption></figure>',
                "5. ## JIT Example",
                "6. The code below creates Linked Lists, 1000 at a time.",
                "7. * Repeating this 500 times yields an interesting result.",
                '8. <figure><img src="../.gitbook/assets/image (67).png" alt=""><figcaption></figcaption></figure>',
                "9. * First optimization: Not sure what it does.\n* Second optimization: Stops creating linked lists since we\u2019re not actually using them.",
                '10. <figure><img src="../.gitbook/assets/image (95).png" alt=""><figcaption></figcaption></figure>',
                "11. ## \u2026 So Which is Better? MSD or MergeSort?",
                "12. The performance of the merge sort algorithm is highly dependent on the presence of just-in-time (JIT) compilation. Specifically, when JIT is enabled, merge sort is faster in cases where the strings being sorted are equal, but slower when JIT is disabled. This suggests that merge sort is generally more effective for this specific case, given that JIT is typically enabled. However, there are numerous other scenarios to consider, including the sorting of almost equal strings, randomized strings, and real-world data from specific datasets. When assessing the effectiveness of merge sort for these alternative cases, it is important to conduct careful experimentation and profiling to determine which sorting algorithm is most suitable. The lectureCode repository provides code for running these experiments, allowing for more precise assessment of algorithm performance under various conditions. Ultimately, real-world applications will require a thorough analysis of different implementations on actual data in order to select the optimal algorithm for the task at hand.",
                "13. ## Bottom Line: Algorithms Can Be Hard to Compare",
                "14. Comparing algorithms that have the same order of growth can be a difficult task, as it requires conducting computational experiments to determine their relative performance. However, modern programming environments can introduce additional challenges to this process, as certain optimizations such as just-in-time (JIT) compilation in Java can impact the results of experiments. It is worth noting that even small optimizations to an algorithm can have a significant impact on its performance. For instance, a change to the Quicksort algorithm suggested by Vladimir Yaroslavskiy has been shown to provide notable improvements, as discussed briefly in the Quicksort lecture. Therefore, when comparing algorithms with similar growth rates, it is crucial to remain vigilant of potential optimizations and variations that may influence their performance.",
                "15. ## JIT Compilers Are Always Evolving",
                "16. The JIT (just-in-time) compiler is a highly complex and essential component of modern compilers, and is an active area of research and development in this field. However, the older JIT compiler known as C2 has become increasingly difficult to maintain and extend, with no major improvements implemented in recent years. The codebase for C2 is written in a specific dialect of C++, making it challenging for new engineers to understand and work with. As a result, the codebase is being abandoned in favor of newer and more maintainable alternatives. For individuals interested in this area of study, CS164 offers a course on compilers and there are opportunities for involvement in ongoing research. It is worth noting that the reasons for the improved performance of merge sort with the JIT are not yet fully understood, making it an interesting topic for further research.\n",
            ],
        "36.-sorting-and-data-structures-conclusion/36.1-radix-vs.-comparison-sorting.md":
            [
                "1. # 36.1 Radix vs. Comparison Sorting",
                "2. ## Intuitive Analysis",
                "3. ### Merge Sort Runtime",
                "4. Merge Sort requires \u0398(N log N) compares.",
                "5. ",
                "6. A key concept here is that Merge Sort\u2019s runtime on strings of length W is \u0398(N log N) if each comparison takes constant time or \u0398(WN log N) if each comparison takes \u0398(W) time.&#x20;",
                "7. Example of \u0398(N log N): Strings are all different in top character.&#x20;",
                "8. Example of \u0398(WN log N): Strings are all equal.",
                "9. ### LSD vs. Merge Sort",
                "10. The facts:",
                "11. Treating alphabet size as constant, LSD Sort has runtime \u0398(WN). Merge Sort has runtime between \u0398(N log N) and \u0398(WN log N).",
                "12. Which is better? It depends.",
                "13. #### When might LSD sort be faster?",
                "14. * When W is sufficiently smaller than N (or equivalently, N is really really big, or as you get to larger and larger numbers of strings, we expect LSD to pull ahead).\n* Worst case strings for mergesort, the more similar they are, the more we expect LSD sort to do better.&#x20;\n* Sufficiently large N.\n* If strings are very similar to each other.\n  * &#x20;Each Merge Sort comparison costs \u0398(W) time.",
                '15. <figure><img src="../.gitbook/assets/image (74).png" alt=""><figcaption></figcaption></figure>',
                "16. #### When might Merge Sort be faster?",
                "17. * Strings are different, especially at the beginning.\n* If strings are highly dissimilar from each other\n  * Each merge sort comparison is very fast",
                '18. <figure><img src="../.gitbook/assets/image (137).png" alt=""><figcaption></figcaption></figure>',
                "19. ## Cost Model Analysis",
                "20. ### Alternate Approach: Picking a Cost Model",
                "21. An alternate approach is to pick a cost model.",
                "22. * We\u2019ll use number of characters examined.\n* By \u201cexamined\u201d, we mean:\n  * Radix Sort: Calling charAt in order to count occurrences of each character.\n  * Merge Sort: Calling charAt in order to compare two Strings.",
                "23. ### MSD vs. Mergesort",
                "24. Suppose we have 100 strings of 1000 characters each.",
                "25. For MSD Radix Sort, in the worst case (all strings equal), every character is examined exactly once. Thus, we have exactly 100,000 total character examinations.",
                '26. <figure><img src="../.gitbook/assets/image (144).png" alt=""><figcaption></figcaption></figure>',
                "27. For Merge Sort, estimate the total number of characters examined if all strings are equal.",
                "28. Merging 100 items, assuming equal items results in always picking left:",
                "29. * Comparing A\\[0] to A\\[50]: 2000 character examinations.\n* Comparing A\\[1] to A\\[50]: 2000 character examinations.\n* \u2026 Comparing A\\[49] to A\\[50]: 2000 character examinations.\n* Total characters examined: 50 \\* 2000 = 100000.\n* Merging N strings of 1000 characters requires N/2 \\* 2000 = 1000N examinations.",
                '30. <figure><img src="../.gitbook/assets/screenshot 2023-04-19 at 12.46.58 AM.png" alt=""><figcaption></figcaption></figure>',
                "31. In total, we must examine approximately 1000N log2 N total characters.",
                "32. * 100000 + 50000\\*2 + 25000 \\* 4 + \u2026 = \\~660,000 characters.",
                '33. <figure><img src="../.gitbook/assets/image (156).png" alt=""><figcaption></figcaption></figure>',
                "34. ### MSD vs. Mergesort Character Examinations",
                "35. For N equal strings of length 1000, we found that:",
                "36. * MSD radix sort will examine \\~1000N characters (For N= 100: 100,000).\n*   Merge sort will examine \\~1000Nlog2(N) characters (For N=100: 660,000).\\",
                "37. \n    If character examination are an appropriate cost model, we\u2019d expect Merge Sort to be slower by a factor of log2N.\\",
                "38. \n    To see if we\u2019re right, we\u2019ll need to do a computational experiment.",
                "39. ## Empirical Study: Radix Sort vs. Comparison Sorting",
                "40. ### Computational Experiment Results (Your Answers)",
                "41. Computational experiment for W = 100.",
                "42. * [MSD](https://algs4.cs.princeton.edu/51radix/MSD.java.html) and [merge sort](https://algs4.cs.princeton.edu/22mergesort/MergeX.java.html) implementations are highly optimized versions taken from our optional algorithms textbook.\n* Does our data match our runtime hypothesis? No! Why not?\n  * Maybe when MSD makes partitions, the divide and conquer cost is not accounted for.\n  * Is mergesort optimized to do the timesort optimization? Unclear.\n  * Maybe this is caching performance, Josh mentioned MSD has bad cache performance.\n  * Maybe string comparison is not linear time somehow (keep in mind every string compare does the SAME thing. Repeated work on real machines these days tends to get optimized away).\n  * Our cost model isn\u2019t representative of everything that is happening.\n  * One particularly thorny issue: **The \u201cJust In Time\u201d Compiler.**",
                '43. <figure><img src="../.gitbook/assets/image (77).png" alt=""><figcaption></figcaption></figure>',
                '44. <figure><img src="../.gitbook/assets/screenshot 2023-04-19 at 1.01.54 AM.png" alt=""><figcaption></figcaption></figure>',
                "45. ",
            ],
        "36.-sorting-and-data-structures-conclusion/36.5-exercises.md": [
            "1. # 36.5 Exercises",
            "2. ## Factual",
            "3. 1. What happens when a Java file is compiled? HINT: Think about `.java` and `.class` files.\n2. Why can a block of code run faster the second time it executes as compared to the first time?\n3. What's the optimal way to sort an array of integers? Assume the integers are uniformly randomly distributed.",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. When a Java file is compiled, it is transformed from its human-readable `.java` source code format into byte code in a `.class` file format that can be executed by the Java Virtual Machine (JVM).",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. If some segment of code is called many times, the interpreter studies and re-implements your code based on what is observed while the code is running. For example, if you create many unused data structures, they might be optimized out of your code.",
            "11. </details>",
            "12. <details>",
            "13. <summary>Problem 3</summary>",
            "14. As we saw in lecture, LSD sort outperforms quicksort for an array of integers, particularly if we optimize for a better base than base 10.",
            "15. </details>",
            "16. ## Metacognitive",
            "17. 1. Under what conditions would LSD sort be faster than mergesort?",
            "18. <details>",
            "19. <summary>Problem 1</summary>",
            "20. There are two main cases when LSD sort is much faster than mergesort: when there are a large number of strings (large N), and when all strings are very similar to each other.",
            "21. When N is really large, we see the asymptotic behavior of LSD sort beat the asymptotic behavior of merge sort, which is slower with a runtime of $$N \\log N$$.",
            "22. When the strings are very similar, then each comparison in merge sort is slow, roughly linear time in the length of the string. Instead of comparing each letter in each string at every merge, LSD only examines the letters of each string once.",
            "23. </details>\n",
        ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/11.5-chapter-summary.md":
            [
                "1. # 11.5 Chapter Summary",
                "2. **Review: Typing Rules**",
                "3. * Compiler allows the memory box to hold any subtype.\n* Compiler allows calls based on static type.\n* Overriden non-static methods are selected at runtime based on dynamic type.\n* For overloaded methods, the method is selected at compile time.",
                "4. **Subtype Polymorphism** Consider a variable of static type `Deque`. The behavior of calling `deque.method()` depends on the dynamic type. Thus, we could have many subclasses the implement the `Deque` interface, all of which will be able to call `deque.method()`.",
                "5. **Subtype Polymorphism Example** Suppose we want to write a function `max()` that returns the max of any array regardless of type. If we write a method `max(Object[] items)`, where we use the \u2018>\u2019 operator to compare each element in the array, this will not work! Why is this the case?",
                "6. Well, this makes the assumption that all objects can be compared. But some objects cannot! Alternatively, we could write a `max()` function inside the Dog class, but now we have to write a `max()` function for each class that we want to compare! Remember, our goal is to write a \u201cone true max method\u201d that works for all comparable objects.",
                "7. **Solution: OurComparable Interface** The solution is to create an interface that contains a `compareTo(Object)` method; let\u2019s call this interface `OurComparable`. Now, our `max()` method can take a `OurComparable[]` parameter, and since we guarantee that any object which extends the interface has all the methods inside the interface, we guarantee that we will always be able to call a `compareTo` method, and that this method will correctly return some ordering of the objects.",
                "8. Now, we can specify a \u201cone true max method\u201d. Of course, any object that needs to be compared must implement the `compareTo` method. However, instead of re-implementing the `max` logic in every class, we only need to implement the logic for picking the ordering of the objects, given two objects.",
                "9. **Even Better: Java\u2019s In-Built Comparable** Java has an in-built `Comparable` interface that uses generics to avoid any weird casting issues. Plus, Comparable already works for things like `Integer`, `Character`, and `String`; moreover, these objects have already implemented a `max`, `min`, etc. method for you. Thus you do not need to re-do work that\u2019s already been done!",
                "10. **Comparators** The term \u201cNatural Order\u201d is used to refer to the ordering implied by a `Comparable`\u2019s `compareTo` method. However, what if we want to order our `Dog` objects by something other than `size`? We will instead pass in a `Comparator<T>` interface, which demands a `compare()` method. We can then implement the `compare()` method anyway we want to achieve our ordering.",
                "11. \\\n",
            ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/11.3-comparables.md":
            [
                "1. ---\ndescription: One Sizable Application of Subtype Polymorphism\n---",
                "2. # 11.3 Comparables",
                '3. #### Max Function <a href="#max-function" id="max-function"></a>',
                "4. Say we want to write a `max` function which takes in any array - regardless of type - and returns the maximum item in the array.",
                "5. **Exercise 4.3.1.** Your task is to determine how many compilation errors there are in the code below.",
                '6. ```java\npublic static Object max(Object[] items) {\n    int maxDex = 0;\n    for (int i = 0; i < items.length; i += 1) {\n        if (items[i] > items[maxDex]) {\n            maxDex = i;\n        }\n    }\n    return items[maxDex];\n}\n\npublic static void main(String[] args) {\n    Dog[] dogs = {new Dog("Elyse", 3), new Dog("Sture", 9), new Dog("Benjamin", 15)};\n    Dog maxDog = (Dog) max(dogs);\n    maxDog.bark();\n}\n```',
                '7. {% embed url="https://www.youtube.com/watch?embeds_euri=https://joshhug.gitbooks.io/&feature=emb_logo&time_continue=1&v=nYPPbbkKF1w" %}',
                "8. In the code above, there was only 1 error, found at this line:",
                "9. ```java\nif (items[i] > items[maxDex]) {\n```",
                "10. The reason why this results in a compilation error is because this line assumes that the `>` operator works with arbitrary Object types, when in fact it does not.",
                '11. Instead, one thing we could is define a `maxDog` function in the Dog class, and give up on writing a "one true max function" that could take in an array of any arbitrary type. We might define something like this:',
                "12. ```java\npublic static Dog maxDog(Dog[] dogs) {\n    if (dogs == null || dogs.length == 0) {\n        return null;\n    }\n    Dog maxDog = dogs[0];\n    for (Dog d : dogs) {\n        if (d.size > maxDog.size) {\n            maxDog = d;\n        }\n    }\n    return maxDog;\n}\n```",
                "13. While this would work for now, if we give up on our dream of making a generalized `max` function and let the Dog class define its own `max` function, then we'd have to do the same for any class we define later. We'd need to write a `maxCat` function, a `maxPenguin` function, a `maxWhale` function, etc., resulting in unnecessary repeated work and a lot of redundant code.",
                "14. The fundamental issue that gives rise to this is that Objects cannot be compared with `>`. This makes sense, as how could Java know whether it should use the String representation of the object, or the size, or another metric, to make the comparison? In Python or C++, the way that the `>` operator works could be redefined to work in different ways when applied to different types. Unfortunately, Java does not have this capability. Instead, we turn to interface inheritance to help us out.",
                "15. ",
                "16. ## Solution: Comparables",
                "17. We can create an interface that guarantees that any implementing class, like Dog, contains a comparison method, which we'll call `compareTo`.",
                "18. ![](https://joshhug.gitbooks.io/hug61b/content/assets/dog\\_comparable.png)",
                "19. Let's write our interface. We'll specify one method `compareTo`.",
                "20. ```java\npublic interface OurComparable {\n    public int compareTo(Object o);\n}\n```",
                "21. We will define its behavior like so:",
                "22. * Return -1 if `this` < o.\n* Return 0 if `this` equals o.\n* Return 1 if `this` > o.",
                "23. Now that we've created the `OurComparable` interface, we can require that our Dog class implements the `compareTo` method. First, we change Dog's class header to include `implements OurComparable`, and then we write the `compareTo` method according to its defined behavior above.",
                "24. **Exercise 4.3.2.** Implement the `compareTo` method for the Dog class.",
                "25. The `OurComparable` interface that we just built works, but it's not perfect. Here are some issues with it:",
                "26. * Awkward casting to/from Objects\n* We made it up.\n  * No existing classes implement OurComparable (e.g. String, etc.)\n  * No existing classes use OurComparable (e.g. no built-in max function that uses OurComparable)",
                "27. The solution? We'll take advantage of an interface that already exists called `Comparable`. `Comparable` is already defined by Java and is used by countless libraries.",
                "28. `Comparable` looks very similar to the OurComparable interface we made, but with one main difference. Can you spot it?",
                '29. <figure><img src="../.gitbook/assets/comparable_interface.png" alt=""><figcaption></figcaption></figure>',
                "30. Notice that `Comparable<T>` means that it takes a generic type. This will help us avoid having to cast an object to a specific type! Now, we will rewrite the Dog class to implement the Comparable interface, being sure to update the generic type `T` to Dog:",
                "31. ```java\npublic class Dog implements Comparable<Dog> {\n    ...\n    public int compareTo(Dog uddaDog) {\n        return this.size - uddaDog.size;\n    }\n}\n```",
                "32. Now all that's left is to change each instance of OurComparable in the Maximizer class to Comparable. Watch as the largest Dog says bark:",
                '33. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&embeds_euri=https://joshhug.gitbooks.io/&feature=emb_logo&v=QRPVJ7Wxxtk" %}',
                "34. We use the instance variable `size` to make our comparison.",
                '35. ```java\npublic class Dog implements OurComparable {\n    private String name;\n    private int size;\n\n    public Dog(String n, int s) {\n        name = n;\n        size = s;\n    }\n\n    public void bark() {\n        System.out.println(name + " says: bark");\n    }\n\n    public int compareTo(Object o) {\n        Dog uddaDog = (Dog) o;\n        if (this.size < uddaDog.size) {\n            return -1;\n        } else if (this.size == uddaDog.size) {\n            return 0;\n        }\n        return 1;\n    }\n}\n```',
                "36. **Notice that since `compareTo` takes in any arbitrary Object o, we have to **_**cast**_** the input to a Dog to make our comparison using the `size` instance variable.**",
                "37. Now we can generalize the `max` function we defined in exercise 4.3.1 to, instead of taking in any arbitrary array of objects, takes in `OurComparable` objects - which we know for certain all have the `compareTo` method implemented.",
                "38. ```java\npublic static OurComparable max(OurComparable[] items) {\n    int maxDex = 0;\n    for (int i = 0; i < items.length; i += 1) {\n        int cmp = items[i].compareTo(items[maxDex]);\n        if (cmp > 0) {\n            maxDex = i;\n        }\n    }\n    return items[maxDex];\n}\n```",
                "39. Great! Now our `max` function can take in an array of any `OurComparable` type objects and return the maximum object in the array. Now, this code is admittedly quite long, so we can make it much more succinct by modifying our `compareTo` method's behavior:",
                "40. * Return negative number if `this` < o.\n* Return 0 if `this` equals o.\n* Return positive number if `this` > o.",
                "41. Now, we can just return the difference between the sizes. If my size is 2, and uddaDog's size is 5, `compareTo` would return -3, a negative number indicating that I am smaller.",
                "42. ```java\npublic int compareTo(Object o) {\n    Dog uddaDog = (Dog) o;\n    return this.size - uddaDog.size;\n}\n```",
                "43. Using inheritance, we were able to generalize our maximization function. What are the benefits to this approach?",
                "44. * No need for maximization code in every class(i.e. no `Dog.maxDog(Dog[])` function required\n* We have code that operates on multiple types (mostly) gracefully",
                '45. #### Interfaces Quiz <a href="#interfaces-quiz" id="interfaces-quiz"></a>',
                "46. **Exercise 4.3.3.** Given the `Dog` class, `DogLauncher` class, `OurComparable` interface, and the `Maximizer` class, if we omit the compareTo() method from the Dog class, which file will fail to compile?",
                "47. ```java\npublic class DogLauncher {\n    public static void main(String[] args) {\n        ...\n        Dog[] dogs = new Dog[]{d1, d2, d3};\n        System.out.println(Maximizer.max(dogs));\n    }\n}\n\npublic class Dog implements OurComparable {\n    ...\n    public int compareTo(Object o) {\n        Dog uddaDog = (Dog) o;\n        if (this.size < uddaDog.size) {\n            return -1;\n        } else if (this.size == uddaDog.size) {\n            return 0;\n        }\n        return 1;\n    }\n    ...\n}\n\npublic class Maximizer {\n    public static OurComparable max(OurComparable[] items) {\n        ...\n        int cmp = items[i].compareTo(items[maxDex]);\n        ...\n    }\n}\n```",
                '48. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&t=3s&v=dbdbcbhe3Jk" %}',
                '49. In this case, the `Dog` class fails to compile. By declaring that it `implements OurComparable`, the Dog class makes a claim that it "is-an" OurComparable. As a result, the compiler checks that this claim is actually true, but sees that Dog doesn\'t implement `compareTo`.',
                "50. What if we were to omit `implements OurComparable` from the Dog class header? This would cause a compile error in DogLauncher due to this line:",
                "51. ```java\nSystem.out.println(Maximizer.max(dogs));\n```",
                "52. If Dog does not implement the OurComparable interface, then trying to pass in an array of Dogs to Maximizer's `max` function wouldn't be approved by the compiler. `max` only accepts an array of OurComparable objects.",
                "53. Instead of using our personally created interface `OurComparable`, we now use the real, built-in interface, `Comparable`. As a result, we can take advantage of all the libraries that already exist and use `Comparable`.",
                "54. ![](https://joshhug.gitbooks.io/hug61b/content/assets/comparable.png)",
                "55. \\\n",
            ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/11.1-a-review-of-dynamic-method-selection.md":
            [
                "1. # 11.1 A Review of Dynamic Method Selection",
                '2. In previous lectures we have gone over classes extending other classes, and to make sense of this we considered whether the sub class has an "is-a relationship" with the superclass. For example, if we had the two classes:',
                "3. * Dog: Implements bark() method\n* ShowDog: Extends Dog, overrides bark method",
                "4. This would be a valid extension as a ShowDog is-a Dog and a Dog is-an Object. These relationships satisfy the condition of a subclass being-an instance of the super class. &#x20;",
                "5. ![](<../.gitbook/assets/image (21).png>)",
                "6. ## With this in Mind, what are the Rules to Decide if the Code Would Run or Run into Errors?",
                "7. This is a particularly tricky problem to solve but the rules to look out for include:",
                "8. * Compliers will allow Memory Boxes to hold any subtype of itself\n  * For example, compliers will allow the Dog memory box to hold a ShowDog object as a ShowDog is a subtype of the Dog Class\n* Compliers will allow calls based on Static type.\n  * For example, if a variable were declared as a Dog it's static type would be a Dog and the complier would allowed it to call bark()\n* **Overridden non-static methods are selected at run time based on dynamic type.**\n  * **Everything else is based on static types,** inculding overloaded methods.",
                "9. ## Static Type vs. Dynamic Type",
                "10. Every variable in Java has a \u201ccompile-time type\u201d, a.k.a. \u201cstatic type\u201d.",
                "11. * This is the type specified at declaration. Never changes!",
                "12. \\\nVariables also have a \u201crun-time type\u201d, a.k.a. \u201cdynamic type\u201d.",
                "13. * This is the type specified at instantiation (e.g. when using new).\n* Equal to the type of the object being pointed at.",
                "14. ## Accompanying Lecture",
                '15. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=cUL1LWVv984" %}\n',
            ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/11.4-comparators.md":
            [
                "1. # 11.4 Comparators",
                "2. We've just learned about the comparable interface, which imbeds into each Dog the ability to compare itself to another Dog. Now, we will introduce a new interface that looks very similar called `Comparator`.",
                "3. Let's start off by defining some terminology.",
                "4. * Natural order - used to refer to the ordering implied in the `compareTo` method of a particular class.",
                "5. As an example, the natural ordering of Dogs, as we stated previously, is defined according to the value of size. What if we'd like to sort Dogs in a different way than their natural ordering, such as by alphabetical order of their name?",
                "6. Java's way of doing this is by using `Comparator`'s. Since a comparator is an object, the way we'll use `Comparator` is by writing a nested class inside Dog that implements the `Comparator` interface.",
                "7. But first, what's inside this interface?",
                "8. ```java\npublic interface Comparator<T> {\n    int compare(T o1, T o2);\n}\n```",
                "9. This shows that the `Comparator` interface requires that any implementing class implements the `compare` method. The rule for `compare` is just like `compareTo`:",
                "10. * Return negative number if o1 < o2.\n* Return 0 if o1 equals o2.\n* Return positive number if o1 > o2.",
                "11. Let's give Dog a NameComparator. To do this, we can simply defer to `String`'s already defined `compareTo` method.",
                "12. ```java\nimport java.util.Comparator;\n\npublic class Dog implements Comparable<Dog> {\n    ...\n    public int compareTo(Dog uddaDog) {\n        return this.size - uddaDog.size;\n    }\n\n    private static class NameComparator implements Comparator<Dog> {\n        public int compare(Dog a, Dog b) {\n            return a.name.compareTo(b.name);\n        }\n    }\n\n    public static Comparator<Dog> getNameComparator() {\n        return new NameComparator();\n    }\n}\n```",
                "13. Note that we've declared NameComparator to be a static class. A minor difference, but we do so because we do not need to instantiate a Dog to get a NameComparator. Let's see how this Comparator works in action.",
                '14. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=1oow3NGoExg" %}',
                "15. As you've seen, we can retrieve our NameComparator like so:",
                "16. ```java\nComparator<Dog> nc = Dog.getNameComparator();\n```",
                "17. All in all, we have a Dog class that has a private NameComparator class and a method that returns a NameComparator we can use to compare dogs alphabetically by name.",
                "18. Let's see how everything works in the inheritance hierarchy - we have a Comparator interface that's built-in to Java, which we can implement to define our own Comparators (`NameComparator`, `SizeComparator`, etc.) within Dog.",
                "19. ![](https://joshhug.gitbooks.io/hug61b/content/assets/comparator.png)",
                "20. To summarize, interfaces in Java provide us with the ability to make **callbacks**. Sometimes, a function needs the help of another function that might not have been written yet (e.g. `max` needs `compareTo`). A callback function is the helping function (in the scenario, `compareTo`). In some languages, this is accomplished using explicit function passing; in Java, we wrap the needed function in an interface.",
                '21. A Comparable says, "I want to compare myself to another object". It is imbedded within the object itself, and it defines the **natural ordering** of a type. A Comparator, on the other hand, is more like a third party machine that compares two objects to each other. Since there\'s only room for one `compareTo` method, if we want multiple ways to compare, we must turn to Comparator.\n',
            ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/README.md":
            [
                "1. # 11. Inheritance III: Subtype Polymorphism, Comparators, Comparable",
                "2. In this chapter of the textbook we will be:&#x20;",
                "3. * Reviewing Dynamic Method Selection\n* Defining Subtype Polymorphism and contrasting it against Explicit Higher Order Functions\n* Seeing Applications of Subtype Polymorphism in:\n  * Comparators\n  * Comparables",
                "4. The following videos for Lecture 10 correspond to the content for this chapter of the textbook.",
                '5. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&index=1&list=PL8FaHk7qbOD56r1sGUGifsfC0KRDAsuZ3&v=cUL1LWVv984" %}\n',
            ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/11.2-subtype-polymorphism-vs-explicit-higher-order-functions.md":
            [
                "1. # 11.2 Subtype Polymorphism vs Explicit Higher Order Functions",
                "2. We've seen how inheritance lets us reuse existing code in a superclass while implementing small modifications by overriding a superclass's methods or writing brand new methods in the subclass. Inheritance also makes it possible to design general data structures and methods using _polymorphism_.",
                "3. Polymorphism, at its core, means 'many forms'. In Java, polymorphism refers to how objects can have many forms or types. In object-oriented programming, polymorphism relates to how an object can be regarded as an instance of its own class, an instance of its superclass, an instance of its superclass's superclass, and so on.",
                "4. Consider a variable `deque` of static type Deque. A call to `deque.addFirst()` will be determined at the time of execution, depending on the run-time type, or dynamic type, of `deque` when `addFirst` is called. As we saw in the last chapter, Java picks which method to call using dynamic method selection.",
                "5. Suppose we want to write a python program that prints a string representation of the larger of two objects. There are two approaches to this.",
                "6. 1. Explicit HoF Approach",
                "7. ```python\ndef print_larger(x, y, compare, stringify):\n    if compare(x, y):\n        return stringify(x)\n    return stringify(y)\n```",
                "8. 2. Subtype Polymorphism Approach",
                "9. ```python\ndef print_larger(x, y):\n    if x.largerThan(y):\n        return x.str()\n    return y.str()\n```",
                "10. Using the explicit higher order function approach, you have a common way to print out the larger of two objects. In contrast, in the subtype polymorphism approach, the object _itself_ makes the choices. The `largerFunction` that is called is dependent on what x and y actually are.",
                '11. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=m2F-ekp_BRU" %}\n',
            ],
        "11.-inheritance-iii-subtype-polymorphism-comparators-comparable/11.6-exercises.md":
            [
                "1. # 11.6 Exercises",
                "2. ## Exercises",
                "3. 1. Which of the following are examples of subtype polymorphism?\n   * [ ] Having a `sqrt(int)` and `sqrt(double)` method in the same class.\n   * [ ] Having a `Dog` override the `makeSound` method that it inherits from `Animal`.\n   * [ ] Creating a class that implements the built-in `Comparable` interface.\n2. How would you compare two strings alphabetically in Java?\n3. Suppose we correctly define a `Comparator` class called `SixComparator` for integers that compares them based on the number of `6`s they contain. What will the code `(new SixComparator()).compare(12345678, 45666678)` return?&#x20;\n4. What is the main difference between the `Comparable` and `Comparator` interfaces?",
                "4. ## Solutions",
                "5. <details>",
                "6. <summary>Problem 1</summary>",
                "7. **Having a `Dog` override the `makeSound` method that it inherits from `Animal`.**",
                "8. Overriding `makeSound` allows us to have different implementations of the same method via subtypes.",
                "9. **Creating a class that implements the built-in `Comparable` interface.**",
                "10. Implementing the Comparable interface involves overriding Comparables\u2019 methods, allowing for differing behavior of the same method across types.",
                "11. </details>",
                "12. <details>",
                "13. <summary>Problem 2</summary>",
                "14. `s1.compareTo(s2)`",
                "15. </details>",
                "16. <details>",
                "17. <summary>Problem 3</summary>",
                "18. It will return some negative number, since `12345678` has less `6`'s than `45666678`. Note that there is no guarantee of what the negative number's value is, only that it is less than 0.",
                "19. </details>",
                "20. <details>",
                "21. <summary>Problem 4</summary>",
                "22. An object that implements `Comparable` can compare another object to itself, whereas a Comparator compares two objects other than itself.&#x20;",
                "23. A good way to remember this is that a `Comparable` has an inherent property of being _able_ _to be compared_, while a `Comparator` is an external source of truth.",
                "24. </details>",
                "25. ",
            ],
        "12.-inheritance-iv-iterators-object-methods/12.1-lists-and-sets-in-java.md":
            [
                "1. # 12.1 Lists and Sets in Java",
                "2. In this section, we will learn about how to use Java's built-in `List` and `Set` data structures as well as build our own `ArraySet`.",
                "3. ### Getting Started",
                '4. {% embed url="https://youtu.be/DWr8YNXPH6k" %}',
                "5. In this course, we've already built two kinds of lists: `AList` and `SLList`. We also built an interface `List61B` to enforce specific list methods `AList` and `SLList` had to implement. You can find the code at the following links:",
                "6. * [`List61B`](https://github.com/Berkeley-CS61B/lectureCode-sp23/blob/main/lec9\\_inheritance2/List61B.java)\n* [`AList`](https://github.com/Berkeley-CS61B/lectureCode-sp23/blob/main/lec8\\_inheritance1/AList.java)\n* [`SLList`](https://github.com/Berkeley-CS61B/lectureCode-sp23/blob/main/lec9\\_inheritance2/SLList.java)",
                "7. This is how we might use `List61B` type:",
                "8. ```java\nList61B<Integer> L = new AList<>();\nL.addLast(5);\nL.addLast(10);\nL.addLast(15);\nL.print();\n```",
                "9. ### Lists in Real Java Code",
                "10. We built a list from scratch, but Java provides a built-in `List` interface and several implementations, e.g. `ArrayList`. Remember, since `List` is an interface we can't instantiate it! We must instantiate one of its implementations.",
                "11. To access this, we can use the full name ('canonical name') of classes/interfaces:",
                "12. ```java\njava.util.List<Integer> L = new java.util.ArrayList<>();\n```",
                "13. However, this is a bit verbose. Instead, we can import java libraries:",
                "14. ```java\nimport java.util.List;\nimport java.util.ArrayList;\n\npublic class Example {\n    public static void main(String[] args) {\n        List<Integer> L = new ArrayList<>();\n        L.add(5);\n        L.add(10);\n        System.out.println(L);\n    }\n}\n```",
                "15. ### Sets",
                "16. Sets are a collection of unique elements - you can only have one copy of each element. Unlike Lists, there is also no sense of order: you can't index into a set, nor can you control where each element is inserted into the set.",
                "17. #### Java Sets",
                "18. Java has the `Set` interface along with implementations, e.g. `HashSet`. Remember to import them if you don't want to use the full name!",
                "19. ```java\nimport java.util.Set;\nimport java.util.HashSet;\n```",
                "20. Example use:",
                '21. ```java\nSet<String> s = new HashSet<>();\ns.add("Tokyo");\ns.add("Lagos");\nSystem.out.println(s.contains("Tokyo")); // true\n```',
                "22. #### Python Equivalent",
                "23. In python, we simply call `set()`. To check for `contains` we don't use a method but the keyword `in`. Here's an example:",
                '24. ```python\ns = set()\ns.add("Tokyo")\ns.add("Lagos")\nprint("Tokyo" in s) // True\n```',
                "25. ### DIY: ArraySet",
                "26. Our goal is to make our own set, `ArraySet`, with the following methods:",
                "27. * `add(value)`: add the value to the set if not already present\n* `contains(value)`: check to see if ArraySet contains the key\n* `size()`: return number of values",
                "28. If you would like to try it yourself, find 'Do It Yourself' `ArraySet starter code` [here](https://github.com/Berkeley-CS61B/lectureCode-sp23/blob/main/lec11\\_inheritance4/DIY/ArraySet.java). In the lecture clip below, Professor Hug goes develops the solution:",
                '29. {% embed url="https://youtu.be/gX9KFdZBg-k" %}\n',
            ],
        "12.-inheritance-iv-iterators-object-methods/12.4-object-methods.md": [
            "1. # 12.4 Object Methods",
            "2. All classes inherit from the overarching Object class. The methods that are inherited are as follows:",
            "3. * `String toString()`\n* `boolean equals(Object obj)`\n* `Class <?> getClass()`\n* `int hashCode()`\n* `protected Objectclone()`\n* `protected void finalize()`\n* `void notify()`\n* `void notifyAll()`\n* `void wait()`\n* `void wait(long timeout)`\n* `void wait(long timeout, int nanos)`",
            "4. We are going to focus on the first two in this chapter. We will take advantage of inheritance to override these two methods in our classes to behave in the ways we want them to.",
            '5. ## toString() <a href="#tostring" id="tostring"></a>',
            '6. {% embed url="https://www.youtube.com/watch?v=AKnMv0ootkg" %}',
            "7. The `toString()` method provides a string representation of an object. The `System.out.println()` function implicitly calls this method on whatever object is passed to it and prints the string returned. When you run `System.out.println(dog)`, it's actually doing this:",
            "8. ```java\nString s = dog.toString()\nSystem.out.println(s)\n```",
            "9. The default `Object` class' `toString()` method prints the location of the object in memory. This is a hexadecimal string. Classes like Arraylist and java arrays have their own overridden versions of the `toString()` method. This is why, when you were working with and writing tests for Arraylist, errors would always return the list in a nice format like this (1, 2, 3, 4) instead of returning the memory location.",
            "10. For classes that we've written by ourselves like `ArrayDeque`, `LinkedListDeque`, etc, we need to provide our own `toString()` method if we want to be able to see the objects printed in a readable format.",
            "11. Let's try to write this method for an `ArraySet` class. Read the `ArraySet` class below and make sure you understand what the various methods do. Feel free to plug the code into java visualizer to get a better understanding!",
            '12. ```java\nimport java.util.Iterator;\n\npublic class ArraySet<T> implements Iterable<T> {\n    private T[] items;\n    private int size; // the next item to be added will be at position size\n\n    public ArraySet() {\n        items = (T[]) new Object[100];\n        size = 0;\n    }\n\n    /* Returns true if this map contains a mapping for the specified key.\n     */\n    public boolean contains(T x) {\n        for (int i = 0; i < size; i += 1) {\n            if (items[i].equals(x)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /* Associates the specified value with the specified key in this map.\n       Throws an IllegalArgumentException if the key is null. */\n    public void add(T x) {\n        if (x == null) {\n            throw new IllegalArgumentException("can\'t add null");\n        }\n        if (contains(x)) {\n            return;\n        }\n        items[size] = x;\n        size += 1;\n    }\n\n    /* Returns the number of key-value mappings in this map. */\n    public int size() {\n        return size;\n    }\n\n    /** returns an iterator (a.k.a. seer) into ME */\n    public Iterator<T> iterator() {\n        return new ArraySetIterator();\n    }\n\n    private class ArraySetIterator implements Iterator<T> {\n        private int wizPos;\n\n        public ArraySetIterator() {\n            wizPos = 0;\n        }\n\n        public boolean hasNext() {\n            return wizPos < size;\n        }\n\n        public T next() {\n            T returnItem = items[wizPos];\n            wizPos += 1;\n            return returnItem;\n        }\n    }\n\n    @Override\n    public String toString() {\n        /* hmmm */\n    }\n\n\n    @Override\n    public boolean equals(Object other) {\n        /* hmmm */\n    }\n\n    public static void main(String[] args) {\n        ArraySet<Integer> aset = new ArraySet<>();\n        aset.add(5);\n        aset.add(23);\n        aset.add(42);\n\n        //iteration\n        for (int i : aset) {\n            System.out.println(i);\n        }\n\n        //toString\n        System.out.println(aset);\n\n        //equals\n        ArraySet<Integer> aset2 = new ArraySet<>();\n        aset2.add(5);\n        aset2.add(23);\n        aset2.add(42);\n\n        System.out.println(aset.equals(aset2));\n        System.out.println(aset.equals(null));\n        System.out.println(aset.equals("fish"));\n        System.out.println(aset.equals(aset));\n}\n```',
            "13. You can find the [solutions here (ArraySet.java)](https://github.com/Berkeley-CS61B/lectureCode-sp23/blob/main/lec11\\_inheritance4/ArraySet.java)",
            "14. **Exercise 6.4.1:** Write the toString() method so that when we print an ArraySet, it prints the elements separated by commas inside of curly braces. i.e {1, 2, 3, 4}. Remember, the toString() method should return a string.",
            "15. **Solution**",
            '16. ```java\npublic String toString() {\n    String returnString = "{";\n    for (int i = 0; i < size; i += 1) {\n        returnString += keys[i];\n        returnString += ", ";\n    }\n    returnString += "}";\n    return returnString;\n}\n```',
            "17. This solution, although seemingly simple and elegant, is actually very naive. This is because when you use string concatenation in Java like so: `returnString += keys[i];` you are actually not just appending to `returnString`, you are creating an entirely new string. This is incredibly inefficient because creating a new string object takes time too! Specifically, linear in the length of the string (we'll discuss why in a later chapter!).",
            "18. **Bonus Question:** Let's say concatenating one character to a string takes 1 second. If we have an ArraySet of size 5: `{1, 2, 3, 4, 5}`, how long would it take to run the `toString()` method?",
            '19. **Answer:** We set `returnString` to the left bracket which takes one second because this involves adding `{` to the empty string `""`. Adding the first element will involve creating an entirely new string, adding } and 1 which would take 2 seconds. Adding the second element takes 3 seconds because we need to add `{`, `1`, `2`. This process continues, so for the entire array set the total time is `1 + 2 + 3 + 4 + 5 + 6 + 7.`',
            "20. To remedy this, Java has a special class called `StringBuilder`. It creates a string object that is mutable, so you can continue appending to the same string object instead of creating a new one each time.",
            "21. **Exercise 6.4.2:** Rewrite the toString() method using StringBuilder.",
            "22. **Solution**",
            '23. ```java\npublic String toString() {\n        StringBuilder returnSB = new StringBuilder("{");\n        for (int i = 0; i < size - 1; i += 1) {\n            returnSB.append(items[i].toString());\n            returnSB.append(", ");\n        }\n        returnSB.append(items[size - 1]);\n        returnSB.append("}");\n        return returnSB.toString();\n    }\n```',
            "24. Now you've successfully overridden the `toString()` method! Try printing the ArraySet to see the fruits of your work.",
            "25. Next we will override another important object method: `equals()`",
            '26. ## equals() <a href="#equals" id="equals"></a>',
            '27. {% embed url="https://www.youtube.com/watch?v=qHuS1o97nfQ" %}',
            "28. `equals()` and `==` have different behaviors in Java. `==` Checks if two objects are actually the same object in memory. Remember, pass-by-value! `==` checks if two boxes hold the same thing. For primitives, this means checking if the values are equal. For objects, this means checking if the address/pointer is equal.",
            "29. Say we have this `Doge` class:",
            '30. ```java\npublic class Doge {\n\n   public int age;\n   public String name;\n\n   public Doge(int age, String name){\n      this.age = age;\n      this.name = name;\n   }\n   public static void main(String[] args) {\n\n      int x = 5;\n      int y = 5;\n      int z = 6;\n\n      Doge fido = new Doge(5, "Fido");\n      Doge doggo = new Doge(6, "Doggo");\n      Doge fidoTwin = new Doge(5, "Fido");\n      Doge fidoRealTwin = fido;\n   }\n}\n```',
            "31. If we plug this code into the java visualizer, we will see the box in pointer diagram shown below.",
            "32. ![](https://joshhug.gitbooks.io/hug61b/content/assets/Doge.png)",
            "33. **Exercise 6.4.3:** What would java return if we ran the following?",
            "34. * `x == y`\n* `x == z`\n* `fido == doggo`\n* `fido == fidoTwin`\n* `fido == fidoRealTwin`",
            "35. **Answers**",
            "36. * `True`\n* `False`\n* `False`\n* `False`\n* `True`",
            "37. `fido` and `fidoTwin` are not considered `==` because they point to different objects. However, this is quite silly since all their attributes are the same. You can see how `==` can cause some problems in Java testing. When we write tests for our ArrayList and want to check if expected is the same as what is returned by our function, we create expected as a new arraylist. If we used `==` in our test, it would always return false. This is what `equals(Object o)` is for.",
            '38. ## `equals(Object o)` <a href="#equalsobject-o" id="equalsobject-o"></a>',
            "39. `equals(Object o)` is a method in the Object class that, by default, acts like == in that it checks if the memory address of the this is the same as o. However, we can override it to define equality in whichever way we wish! For example, for two Arraylists to be considered equal, they just need to have the same elements in the same order.",
            "40. **Exercise 6.4.4:** Let's write an equals method for the ArraySet class. Remember, a set is an unordered collection of unique elements. So, for two sets to be considered equal, you just need to check if they have the same elements.",
            "41. **Solution**",
            "42. ```java\npublic boolean equals(Object other) {\n        if (this == other) {\n            return true;\n        }\n        if (other == null) {\n            return false;\n        }\n        if (other.getClass() != this.getClass()) {\n            return false;\n        }\n        ArraySet<T> o = (ArraySet<T>) other;\n        if (o.size() != this.size()) {\n            return false;\n        }\n        for (T item : this) {\n            if (!o.contains(item)) {\n                return false;\n            }\n        }\n        return true;\n    }\n```",
            "43. We added a few checks in the beginning of the method to make sure our `.equals()` can handle nulls and objects of a different class. We also optimized the function by returning true right away if the == methods returns true. This way, we avoid the extra work of iterating through the set.",
            "44. **Rules for Equals in Java:** Overriding a `.equals()` method may sometimes be trickier than it seems. A couple of rules to adhere to while implementing your `.equals()` method are as follows:",
            "45. 1.) `equals` must be an equivalence relation",
            "46. * **reflexive**: `x.equals(x)` is true\n* **symmetric**: `x.equals(y)` if and only if `y.equals(x)`\n* **transitive**: `x.equals(y)` and `y.equals(z)` implies `x.equals(z)`",
            "47. 2.) It must take an Object argument, in order to override the original `.equals()` method",
            "48. 3.) It must be consistent if `x.equals(y)`, then as long as `x` and `y` remain unchanged: `x` must continue to equal `y`",
            "49. 4.) It is never true for null `x.equals(null)` must be false",
            '50. ## Bonus video <a href="#bonus-video" id="bonus-video"></a>',
            "51. Create an even better `toString` method and `ArraySet.of`:",
            '52. {% embed url="https://www.youtube.com/watch?v=tjLpeVD0KWc" %}',
            "53. Link to the [bonus code](https://github.com/Berkeley-CS61B/lectureCode-sp23/blob/main/lec11\\_inheritance4/ArraySet.java)\n",
        ],
        "12.-inheritance-iv-iterators-object-methods/README.md": [
            "1. # 12. Inheritance IV: Iterators, Object Methods",
            "2. This section covers Lists and Sets, how to throw informative exceptions, iterations, and object methods. Its contents correspond to Lecture 11.\n",
        ],
        "12.-inheritance-iv-iterators-object-methods/12.3-iteration.md": [
            "1. # 12.3 Iteration",
            '2. {% embed url="https://www.youtube.com/watch?v=Gv6LjusNBU0" %}',
            "3. We can use a clean enhanced for loop with Java's `HashSet`",
            '4. ```java\nSet<String> s = new HashSet<>();\ns.add("Tokyo");\ns.add("Lagos");\nfor (String city : s) {\n    System.out.println(city);\n}\n```',
            "5. However, if we try to do the same with our `ArraySet`, we get an error. How can we enable this functionality?",
            '6. ## Enhanced For Loop <a href="#enhanced-for-loop" id="enhanced-for-loop"></a>',
            '7. Let\'s first understand what is happening when we use an enhanced for loop. We can "translate" an enhanced for loop into an ugly, manual approach.',
            "8. ```java\nSet<String> s = new HashSet<>();\n...\nfor (String city : s) {\n    ...\n}\n```",
            "9. The above code translates to:",
            "10. ```java\nSet<String> s = new HashSet<>();\n...\nIterator<String> seer = s.iterator();\nwhile (seer.hasNext()) {\n    String city = seer.next();\n    ...\n}\n```",
            "11. Let\u2019s strip away the magic so we can build our own classes that support this.",
            "12. The key here is an object called an _iterator_.",
            "13. For our example, in List.java we might define an `iterator()` method that returns an iterator object.",
            "14. ```java\npublic Iterator<E> iterator();\n```",
            "15. Now, we can use that object to loop through all the entries in our list:",
            "16. ```java\nList<Integer> friends = new ArrayList<Integer>();\n...\nIterator<Integer> seer = friends.iterator();\n\nwhile (seer.hasNext()) {\nSystem.out.println(seer.next());\n}\n```",
            "17. This code behaves identically to the foreach loop version above.",
            "18. There are three key methods in our iterator approach:",
            "19. First, we get a new iterator object with `Iterator<Integer> seer = friends.iterator();`",
            "20. Next, we loop through the list with our while loop. We check that there are still items left with `seer.hasNext()`, which will return true if there are unseen items remaining, and false if all items have been processed.",
            "21. Last, `seer.next()` does two things at once. It returns the next element of the list, and here we print it out. It also advances the iterator by one item. In this way, the iterator will only inspect each item once.&#x20;",
            '22. ## Implementing Iterators <a href="#implementing-iterators" id="implementing-iterators"></a>',
            "23. In this section, we are going to talk about how to build a class to support iteration.",
            "24. Let's start by thinking about what the compiler needs to know in order to successfully compile the following iterator example:",
            "25. ```java\nList<Integer> friends = new ArrayList<Integer>();\nIterator<Integer> seer = friends.iterator();\n\nwhile(seer.hasNext()) {\n    System.out.println(seer.next());\n}\n```",
            "26. We can look at the static types of each object that calls a relevant method. `friends` is a List, on which `iterator()` is called, so we must ask:",
            "27. * Does the List interface have an iterator() method?",
            "28. `seer` is an Iterator, on which `hasNext()` and `next()` are called, so we must ask:",
            "29. * Does the Iterator interface have next/hasNext() methods?",
            "30. So how do we implement these requirements?",
            "31. The List interface extends the Iterable interface, inheriting the abstract iterator() method. (Actually, List extends Collection which extends Iterable, but it's easier to codethink of this way to start.)",
            "32. ```java\npublic interface Iterable<T> {\n    Iterator<T> iterator();\n}\n```",
            "33. ```java\npublic interface List<T> extends Iterable<T>{\n    ...\n}\n```",
            "34. Next, the compiler checks that Iterators have `hasNext()` and `next()`. The Iterator interface specifies these abstract methods explicitly:",
            "35. ```java\npublic interface Iterator<T> {\n    boolean hasNext();\n    T next();\n}\n```",
            "36. **What if someone calls `next` when `hasNext` returns false?**",
            "37. ```\nThis behavior is undefined. However, a common convention is to throw a `NoSuchElementException`. See [Discussion 5](https://sp19.datastructur.es/materials/discussion/disc05sol.pdf) for examples.\n```",
            "38. **Will `hasNext` always be called before `next`?**",
            "39. ```\nNot necessarily. This is sometimes the case when someone using the iterator knows exactly how many elements are in the sequence. Thus, we can't rely on the user calling `hasNext` before `next`. However, you can always call `hasNext` from within your `next` function.\n```",
            "40. Specific classes will implement their own iteration behaviors for the interface methods. Let's look at an example. (Note: if you want to build this up from the start, follow along with the live coding in the video.)",
            "41. We are going to add iteration through keys to our `ArraySet` class. First, we write a new class called ArraySetIterator, nested inside of ArraySet:",
            "42. ```java\nprivate class ArraySetIterator implements Iterator<T> {\n    private int wizPos;\n\n    public ArraySetIterator() {\n        wizPos = 0;\n    }\n\n    public boolean hasNext() {\n        return wizPos < size;\n    }\n\n    public T next() {\n        T returnItem = items[wizPos];\n        wizPos += 1;\n        return returnItem;\n    }\n}\n```",
            "43. This ArraySetIterator implements `Iterator<T>`, which means it implements a `hasNext()` method, and a `next()` method, using a `wizPos` position as an index to keep track of its position in the array. For a different data structure, we might implement these two methods differently.",
            "44. **Thought Exercise:** How would you design `hasNext()` and `next()` for a linked list?",
            "45. Now that we have the appropriate methods, we could use a ArraySetIterator to iterate through an `ArraySet`:",
            "46. ```java\nArraySet<Integer> aset = new ArraySet<>();\naset.add(5);\naset.add(23);\naset.add(42);\n\nIterator<Integer> iter = aset.iterator();\n\nwhile(iter.hasNext()) {\n    System.out.println(iter.next());\n}\n```",
            "47. We still want to be able to support the enhanced for loop, though, to make our calls cleaner. So, we need to make `ArraySet` implement the Iterable interface. The essential method of the Iterable interface is `iterator()`, which returns an Iterator object for that class. All we have to do is return an instance of our `ArraySetIterator` that we just wrote!",
            "48. ```java\npublic Iterator<T> iterator() {\n    return new ArraySetIterator();\n}\n```",
            "49. Now we can use enhanced for loops with our `ArrraySet`!",
            "50. ```java\nArraySet<Integer> aset = new ArraySet<>();\n...\nfor (int i : aset) {\n    System.out.println(i);\n}\n```",
            "51. Here we've seen **Iterable**, the interface that makes a class able to be iterated on, and requires the method `iterator()`, which returns an Iterator object. And we've seen **Iterator**, the interface that defines the object with methods to actually do that iteration. You can think of an Iterator as a machine that you put onto an iterable that facilitates the iteration. Any iterable is the object on which the iterator is performing.",
            "52. With these two components, you can make fancy for loops for your classes!",
            "53. `ArraySet` code with iteration support is below:",
        ],
        "12.-inheritance-iv-iterators-object-methods/12.6-exercises.md": [
            "1. # 12.6 Exercises",
            "2. ### Factual",
            "3. 1. What methods are required for a class that is Iterable?\n2. Which of the following is true about the `java.util.Se`t and the `java.util.List` interfaces?\n   * [ ] If we add `String[][]` objects to a `Set` and a `List`, the size of the set will always be less than or equal to the size of the list.\n   * [ ] The `java.util.ArrayList` class is an implementation of the `java.util.List` interface.\n   * [ ] The `Set` and `List` interfaces extend the `Iterator` interface.\n   * [ ] The `Set` and `List` interfaces extend the `Iterable` interface.\n3. Suppose we have a class that implements `Iterator`. What methods must it override in order to compile?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. An `Iterable` is required to have the `iterator()` method, which returns an `Iterator`.",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. * **If we add `String[][]` objects to a `Set` and a `List`, the size of the set will always be less than or equal to the size of the list.** Sets only have unique items, while lists can have duplicates, so if we add the same elements to both the list will always have at least as many elements as the set.\n* **The `java.util.ArrayList` class is an implementation of the `java.util.List` interface.** One implementation of the `List` interface in Java is the ArrayList class.\n* **The `Set` and `List` interfaces extend the `Iterable` interface.** Sets and Lists in Java can be used in enhanced for loops, which means that they are `Iterable`.",
            "11. </details>",
            "12. <details>",
            "13. <summary>Problem 3</summary>",
            "14. An `Iterator` must override `hasNext()`, which returns a boolean indicating whether there are more elements in the `Iterator`, and `next()`, which returns the next item.",
            "15. </details>",
            "16. ### Conceptual",
            "17. 1. Why do we want to override the `.equals` method?",
            "18. <details>",
            "19. <summary>Problem 1</summary>",
            "20. The `.equals()` method inherited from `Object` only checks if two items have the same memory address. This is undesireable behavior for many user-written classes in Java.",
            "21. </details>",
            "22. ## Metacognitive",
            "23. 1. In lecture, you built the `ArraySetIterator` class. Modify the lecture class to take in a `Comparator<T>` and an item of generic type `T` called `ref` in the constructor. This new iterator should only return items greater than `T`. For reference, the code for `ArraySetIterator` is included below.",
            "24. ```java\nprivate class ArraySetIterator implements Iterator<T> {\n\tprivate int pos;\n\t\n\tpublic ArraySetIterator() { \n\t\tpos = 0; \n\t}\n\t\n\tpublic boolean hasNext() { \n\t\treturn pos < size; \n\t}\n\t\n\tpublic T next() {\n\t \tT returnItem = items[pos];\n\t \tpos += 1;\n\t \treturn returnItem;\n\t}\n}\n```",
            "25. 2. Problem 7 from the Spring 2018 Midterm 2",
            "26. <details>",
            "27. <summary>Problem 1</summary>",
            "28. ```java\npublic class ArraySetGreaterIterator implements Iterator<T> {\n    private int pos;\n    private T ref;\n        \n    private Comparator<T> comp;\n\n    public ArraySetGreaterIterator(T ref, Comparator<T> comp) {\n        this.ref = ref;\n        this.comp = comp;\n    }\n\n    @Override\n    public boolean hasNext() {\n        return pos < size;\n    }\n\n    @Override\n    public T next() {\n        T returnItem = items[pos];\n        while (comp.compare(returnItem, ref) <= 0) {\n            pos += 1;\n            returnItem = items[pos];\n        }\n        return returnItem;\n    }\n}\n```",
            "29. </details>",
            "30. <details>",
            "31. <summary>Problem 2</summary>",
            "32. [Solutions](https://drive.google.com/file/d/1LIyFXwHYCWXNqIgKTsTyKiOYnB79\\_ykk/view) and [walkthrough](https://www.youtube.com/watch?v=nMZn4EV0gGw) are linked here and on the course website.",
            "33. </details>\n",
        ],
        "12.-inheritance-iv-iterators-object-methods/12.2-exceptions.md": [
            "1. # 12.2 Exceptions",
            "2. In this section, we will learn how to throw exceptions to effectively handle errors that may arise in our code.",
            '3. {% embed url="https://youtu.be/r5hp67RfWaY" %}',
            "4. Our `ArraySet` implementation from the previous section has a small error. When we add `null` to our ArraySet, we get a NullPointerException.",
            "5. The probelm lies in the `contains` method where we check `items[i].equals(x)`. If the value at `items[i]` is null, then we are calling `null.equals(x)` -> NullPointerException.",
            "6. Exceptions cause normal flow of control to stop. We can in fact choose to throw our own exceptions. In python you may have seen this with the `raise` keyword. In Java, Exceptions are objects and we throw exceptions using the following format:",
            "7. `throw new ExceptionObject(parameter1, ...)`",
            "8. \\\nLet's throw an exception when a user tries to add null to our `ArraySet`. We'll throw an `IllegalArgumentException` which takes in one parameter (a `String` message).",
            "9. Our updated `add` method:",
            '10. ```java\n/* Associates the specified value with the specified key in this map.\n   Throws an IllegalArgumentException if the key is null. */\npublic void add(T x) {\n    if (x == null) {\n        throw new IllegalArgumentException("can\'t add null");\n    }\n    if (contains(x)) {\n        return;\n    }\n    items[size] = x;\n    size += 1;\n}\n```',
            "11. We get an Exception either way - why does this better?",
            "12. 1. We have control of our code: we consciously decide at what point to stop the flow of our program\n2. More useful Exception type and helpful error message for those using our code",
            "13. However, it would be better if the program doesn't crash at all. There are different things we could do in this case. Here are some below:",
            "14. **Approach 1**: Don't add `null` to the array if it is passed into `add`&#x20;",
            "15. **Approach 2**: Change the `contains` method to account for the case if `items[i] == null`.",
            "16. Whatever you decide, it is important that users know what to expect. That is why documentation (such as comments about your methods) is very important.\\\n",
        ],
        "12.-inheritance-iv-iterators-object-methods/12.5-chapter-summary.md": [
            "1. ---\ndescription: Summary of the main points in this chapter.\n---",
            "2. # 12.5 Chapter Summary",
            "3. You can find the code from this lecture [here](https://github.com/Berkeley-CS61B/lectureCode-sp23/tree/main/lec12\\_inheritance4).&#x20;",
            "4. ### Exceptions",
            "5. Most likely you have encountered an exception in your code such as a `NullPointerException` or an `IndexOutOfBoundsException`. Now we will learn about how we can \u201cthrow\u201d exceptions ourselves. Here is an example of an exception that we throw:",
            '6. ```\nthrow new RuntimeException("For no reason.");\n```',
            "7. This is useful to ensure reasonable functioning of our code, even when facing unexpected behavior.",
            "8. ### Iteration",
            "9. #### Difference between Iterators and Iterables",
            "10. These two words are very closely related, but have two different meanings that are often easy to confuse. The first thing to know is that these are both Java interfaces, with different methods that need to be implemented. Here is a simplified interface for Iterator:",
            "11. ```\npublic interface Iterator<T> {\n  boolean hasNext();\n  T next();\n}\n```",
            "12. Here is a simplified interface for Iterable:",
            "13. ```\npublic interface Iterable<T> {\n    Iterator<T> iterator();\n}\n```",
            "14. Notice that in order for an object (for example an ArrayList or LinkedList) to be _iterable_, it must include a method that returns an _iterator_. The iterator is the object that actively steps through an iterable object. Keep this relationship and distinction in mind as you work with these two interfaces.",
            "15. ### Object Methods",
            "16. #### toString",
            "17. The `toString()` method returns a string representation of objects. For example, `System.out.println(someObject)` calls the `toString()` method of `someObject`, and prints to console whatever string it returns.&#x20;",
            "18. This is most helpful when we are debugging, as it allows us to much more easily understand the current state of our Objects.",
            "19. #### == vs .equals",
            "20. We have two concepts of equality in Java- \u201c==\u201d and the \u201c.equals()\u201d method. The key difference is that when using ==, we are checking if two objects have the same address in memory (that they point to the same instance or object). On the other hand, .equals() is a method that can be overridden by a class and can be used to define some custom way of determining equality. This permits the class to utilize the additional knowledge it has about itself to more accurately answer questions of equality.",
            "21. For example, say we wanted to check if two stones are equal:",
            "22. ```\npublic class Stone{\n  int weight;\n  public Stone(int weight){\n    this.weight = weight;\n  }\n}\nStone s = new Stone(100);\nStone r = new Stone(100);\n```",
            "23. If we want to consider s and r equal because they have the same weight. If we do check equality using ==, these Stones would not be considered equal because they do not have the same memory address.",
            "24. On the other hand, if you override the equals method of Stone as follows",
            "25. ```\npublic boolean equals(Object o){\n  return this.weight == ((Stone) o).weight\n}\n```",
            "26. We would have that the stones would be considered equal because they have the same weight.\n",
        ],
        "32.-more-quick-sort-sorting-summary/32.4-summary.md": [
            "1. # 32.4 Summary",
            "2. ## Quicksort",
            "3. What can we do to avoid the worst case runtime of $$\\theta(N^2)$$ for Quicksort?&#x20;",
            "4. 1. Randomness: pick a random pivot point, shuffle items before sorting\n2. Smarter Pivot Selection: constant time pivot pick (ex: pick a few and then choose the best out of them), linear time pivot pick like (ex: median)\n3. Introspection: switch to a different sort if current sort hits recursion depth threshold",
            "5. Quicksort vs. Mergesort",
            "6. * Mergesort is faster than Quicksort L3S (leftmost pivot, 3-scan partition) and QuickSort PickTH (median pivot, Tony Hoare partition)\n* Quicksort LTHS (leftmost pivot, Tony Hoare partition) is faster than Mergesort!\n* Tony Hoare's partitioning:\n  * two pointers, one at each end of the items array, walking towards each other\n  * left pointer hates larger or equal items, right pointer hates smaller or equal items\n  * swapping anything they don\u2019t like; stop when the two pointers cross each other\n  * new pivot = Item at right pointer.",
            "7. ## Quick Select",
            "8. Quick select helps us quickly find the median with partitioning. Median of an length n array will be around index n /2.&#x20;",
            "9. 1. Initialize array with the leftmost item as the pivot.&#x20;\n2. Partition around pivot.&#x20;\n3. Partition the subproblem. Repeat the process.\n4. Stop when the pivot is at the median index.&#x20;",
            "10. Expected runtime: $$\\theta(N)$$. Worst case runtime (when array is in sorted order): $$\\theta(N^2)$$.",
            "11. ## Stability, Adaptiveness, and Optimization",
            "12. Stability: A sort is stable if the order of equivalent elements is preserved.",
            "13. Optimizations:&#x20;",
            "14. * Adaptiveness - sort that exploits the existing order of the array.\n* Switch to Insertion Sort - when a subproblem reaches size 15 or lower\n* Exploit restrictions on set of keys\n* Switch from QuickSort - if the recursion goes too deep\n",
        ],
        "32.-more-quick-sort-sorting-summary/README.md": [
            "1. ---\ndescription: By Teresa Luo and Nathalys Pham\n---",
            "2. # 32. More Quick Sort, Sorting Summary",
            "3. ",
        ],
        "32.-more-quick-sort-sorting-summary/32.5-exercises.md": [
            "1. # 32.5 Exercises",
            "2. ## Factual",
            "3. 1. Suppose we have the array `[17, 15, 19, 32, 2, 26, 41, 17, 17]`, and we partition it using 3-scan partitioning using `17` as the pivot. What array do we end up with?\n2. Suppose we have the array `[17, 15, 19, 32, 2, 26, 41, 17, 17]`, and we partition it using Tony Hoare partitioning using 17 as the pivot. What array do we end up with?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. `[15, 2, 17, 17, 17, 19, 32, 26, 41]`. First we scan for the smaller elements (15, 2), then the equal elements (17, 17, 17), and finally the larger elements (19, 32, 26, 41) in the order they appear from left to right in the original array.",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. `[2, 15, 17, 17, 17, 26, 41, 32, 19]`. See [here](https://docs.google.com/presentation/d/1DOnWS59PJOa-LaBfttPRseIpwLGefZkn450TMSSUiQY/pub?start=false\\&loop=false\\&delayms=3000\\&slide=id.g463de7561\\_042) for a demo.",
            "11. </details>",
            "12. ## Conceptual",
            "13. 1. Which of the following arrays will result in $$N^2$$ behavior if we use quicksort which always uses the leftmost item as pivot, and which uses a stable partitioning strategy?\n   * [ ] `[1, 1, 1, 1, 1, 1, ..., 1]`\n   * [ ] `[1, 2, 3, 4, 5, 6, ..., N]`\n   * [ ] `[N, N - 1, N - 2, N - 3, ... 2, 1]`\n2. Repeat the exercise above, but assume that we shuffle the array before partitioning.\n3. Suppose we try to use Quick Select to find the median of `[17, 15, 19, 32, 2, 26, 41, 5, 9]`. How many total partitioning operations will we need to complete to find the median?\n4. Which of the following sorts are stable?\n   * [ ] Insertion sort\n   * [ ] Quicksort L3 - use leftmost item as pivot, use 3scan partitioning\n   * [ ] Quicksort L3S - use leftmost item as pivot, use 3scan partitioning, shuffle before starting\n   * [ ] Quicksort LTH - use leftmost item as pivot, use tony hoare partitioning\n   * [ ] Quicksort PickTH - use median (computed with PICK) as pivot, use tony hoare partitioning",
            "14. <details>",
            "15. <summary>Problem 1</summary>",
            "16. All three of these arrays are worst-case inputs for quicksort, since the size of the largest partition will only decrease by 1 each round of partitioning.",
            "17. * [x] `[1, 1, 1, 1, 1, 1, ..., 1]`\n* [x] `[1, 2, 3, 4, 5, 6, ..., N]`\n* [x] `[N, N - 1, N - 2, N - 3, ... 2, 1]`",
            "18. </details>",
            "19. <details>",
            "20. <summary>Problem 2</summary>",
            '21. If we shuffle, the two arrays with integers up to `N` are no longer guaranteed to have bad pivots since the pivot could be any item in the array with equal probability. However, the array of all `1`s will still always choose a "bad" pivot, since all items in the array are equal and will end up in the left partition.',
            "22. * [x] `[1, 1, 1, 1, 1, 1, ..., 1]`\n* [ ] `[1, 2, 3, 4, 5, 6, ..., N]`\n* [ ] `[N, N - 1, N - 2, N - 3, ... 2, 1]`",
            "23. </details>",
            "24. <details>",
            "25. <summary>Problem 3</summary>",
            "26. 1; after a single partitioning operation, we end up with 17 in the middle of the array. Thus, we will get 17 into this middle position and be done immediately. Note that it doesn't matter what partitioning strategy we use.",
            "27. </details>",
            "28. <details>",
            "29. <summary>Problem 4</summary>",
            "30. Insertion sort is stable, as mentioned in previous chapters.",
            "31. 3-scan alone is stable.",
            "32. After shuffling, we cannot guarantee any ordering of items, so any partitioning strategy involving shuffling is not stable. Also, Hoare partitioning is inherently unstable, so any sort involve Hoare partiioning is also not a stable sort.",
            "33. * [x] Insertion sort\n* [x] Quicksort L3 - use leftmost item as pivot, use 3scan partitioning\n* [ ] Quicksort L3S - use leftmost item as pivot, use 3scan partitioning, shuffle before starting\n* [ ] Quicksort LTH - use leftmost item as pivot, use Hoare partitioning\n* [ ] Quicksort PickTH - use median (computed with PICK) as pivot, use Hoare partitioning",
            "34. </details>",
            "35. ## Metacognitive",
            "36. 1. Why does Java\u2019s built-in `Array.sort` method use Quicksort for `int`, `long`, `char`, or other primitive arrays, but Mergesort for all `Object` arrays?\n2. Given that quicksort runs fastest if we can always somehow pick the median item as the pivot, why don\u2019t we use quickselect to find the median to optimize our pivot selection (as opposed to using the leftmost item)?",
            "37. <details>",
            "38. <summary>Problem 1</summary>",
            "39. This is because primitives don't require stability--an `int` is indistinguishable from any other `int` if they are equal by `.equals()`. However, this is not true for `Object`s, since two different `Object`s at different memory addresses can still be equal, and stability may be desireable when sorting objects.",
            "40. </details>",
            "41. <details>",
            "42. <summary>Problem 2</summary>",
            "43. The problem with finding the median before partioning each time is that this increases the runtime of quicksort by a significant constant factor, resulting in a much slower algorithm in practice. The very low probability of running into a worst-case quicksort means that using a time-consuming algorithm to find the median is not worth it in the majority of sorting cases.",
            "44. </details>\n",
        ],
        "32.-more-quick-sort-sorting-summary/32.3-stability-adaptiveness-and-optimization.md":
            [
                "1. # 32.3 Stability, Adaptiveness, and Optimization",
                "2. ## Stability",
                "3. A sort is stable if the order of equivalent elements is preserved. The following is an example of a stable sort. After sorting by section, notice how Bas, Jana, Jouni, and Rosella are in the same order as before sorting. If we want records sorted by section and then by name within each section, we can sort by name and then by section as below.",
                '4. <figure><img src="../.gitbook/assets/Screenshot 2023-04-10 at 12.16.47 AM.png" alt=""><figcaption></figcaption></figure>',
                "5. The following example is an unstable sort. It can make things really annoying! If we want records sorted by section and then by name within each section, we can't just sort by name and then by section as before. After an unstable sort, the previous ordering is not maintained.",
                '6. <figure><img src="../.gitbook/assets/Screenshot 2023-04-10 at 12.28.10 AM.png" alt=""><figcaption></figcaption></figure>',
                "7. Are some of the sorts we learned stable? Insertion sort is stable! Equivalent elements move past their equivalent brethren. MergeSort is stable. HeapSort is not stable. QuickSort can be stable depending on its partitioning scheme, but its stability cannot be assumed since many of its popular partitioning schemes, like Hoare, are unstable.",
                "8. ## Optimizations",
                "9. Adaptiveness - A sort that is adaptive exploits the existing order of the array. Examples are InsertionSort, SmoothSort, and TimSort.",
                "10. Switch to Insertion Sort - When a subproblem reaches size 15 or lower, use insertion sort. It is very very fast for inputs of small sizes.",
                "11. Exploit restrictions on set of keys - For example, if the number of keys is some constant, we can use this constraint to sort faster by applying 3-way QuickSort.",
                "12. Switch from QuickSort - If the recursion goes too deep, switch to a different type of sort.\n",
            ],
        "32.-more-quick-sort-sorting-summary/32.2-quick-select.md": [
            "1. # 32.2 Quick Select",
            '2. Dissatisfied with the result that Quicksort is unable to completely defeat Mergesort and truly claim the title for "fastest comparison-based sorting" algorithm, let\'s shoot our one last shot to overturn the result: use Quick Select to identify the median.&#x20;',
            "3. ## Quick Select Algorithm",
            "4. Goal: find the median using partioning.&#x20;",
            "5. Key idea: Median of an length n array will be around index n /2.",
            "6. 1. Initialize array with the leftmost item as the pivot.&#x20;",
            '7. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.49.15 AM.png" alt=""><figcaption><p>Initial Array</p></figcaption></figure>',
            "8. For this example, the index of the median should be 9 / 2 = 4.&#x20;",
            "9. 2. Partition around pivot.&#x20;",
            '10. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.50.44 AM.png" alt=""><figcaption><p>Pivot lands at index 2</p></figcaption></figure>',
            "11. Since the pivot is at index 2, which is not the median. Therefore, need to continue. However, will only partition the **right** subproblem because median can't be to the left! (index n/2 > 2)",
            "12. 3. Partition the subproblem. Repeat the process.",
            '13. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.53.07 AM.png" alt=""><figcaption></figcaption></figure>',
            "14. Pivot is at index 6, which means that it is not at the median. Continue.&#x20;",
            "15. 4. Stop when the pivot is at the median index.&#x20;",
            '16. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.54.52 AM.png" alt=""><figcaption></figcaption></figure>',
            "17. Since pivot is at index 4, we are done.&#x20;",
            "18. ## Quick Select Performance",
            "19. ### Worst Case Runtime",
            "20. The worst case is when the array is in sorted order, which yields a runtime of $$\\theta(N^2)$$.",
            "21. ![](<../.gitbook/assets/Screen Shot 2023-04-10 at 12.47.27 AM.png>)",
            "22. ### Expected Runtime",
            "23. The expected runtime for the Quick Select Algorithm is $$\\theta(N)$$. Here's an intuitive diagram that justifies this result:&#x20;",
            '24. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.45.39 AM.png" alt=""><figcaption></figcaption></figure>',
            "25. Using one of our favorite sums, we can see the runtime is:",
            "26. $$\nN + N/2 + N/4 +.... + 1 = \\theta(N)\n$$",
            "27. ## Compared to MergeSort?",
            "28. Unfortunately, even using Quickselect to find the exact median, the resulting algorithm is still quite slow.&#x20;",
            "29. Team Mergesort wins, sadly.&#x20;\n",
        ],
        "32.-more-quick-sort-sorting-summary/32.1-quicksort-flavors-vs.-mergesort.md":
            [
                "1. # 32.1 Quicksort Flavors vs. MergeSort",
                "2. ## Quicksort Flavors: Motivation",
                "3. Before we dive into the wonderful ideas of how to improve the Quicksort Algorithm, here's a [review](../30.-quicksort/30.2-quicksort-algorithm.md) on how the Quicksort (or Partition Sort) works.",
                "4. Recall, our version of Quicksort has the following properties:",
                "5. * Leftmost item is always the pivot\n* Our partitioning algorithm always preserves the relative order of <= and >= items.",
                '6. And remember that even though if we have a pivot that is always "good", which leads to a best case runtime of $$\\theta(NlogN)$$, it is still _possible_ that the rare worst case runtime $$\\theta(N^2)$$ will happen in practice. Consequently, Quicksort will have a worse runtime than [MergeSort](../15.-asymptotics-ii/15.4-mergesort.md), losing its title as the "fastest sorting algorithm".&#x20;',
                "7. Therefore,  a natural question to ask is: **What can we do to avoid running into the worst case for Quicksort?** So it can beat MergeSort for sure? This will be the main motivation of exploration for this section.",
                "8. ## Quicksort Flavors: Philosophies",
                "9. Here are some general philosophies to follow to brainstorm ways to improve Quicksort&#x20;",
                "10. ### Philosophy 1: Randomness",
                "11. Some possible reasons that the rare worst case runtime still happen in practice include:",
                "12. * Bad ordering: Arrays already in sorted order\n* Bad elements: Arrays with all duplicates",
                "13. In both of the examples above, assuming we are using the strategy that the leftmost item is always the pivot, each partition of the elements around the pivot **fails to reduce the size of the resulting subarrays **_**significantly**_, which leads to the consequence of going through the items many many times.&#x20;",
                "14. One solution to the bad ordering problem is randomness, which is encoded by these two strategies below:",
                "15. 1. Pick pivots randomly.\n2. Shuffle items before sort.",
                "16. Note: Strategy 2 requires care in partitioning code to avoid $$\\theta(N^2)$$ behavior on arrays of duplicates.",
                "17. Turns out, this randomized version of Quicksort is the best sort you can get with extremely good performance.&#x20;",
                "18. Just like the care-free, motorcycle rider from the 1992 TV Show _Renegade,_ this version of the Quicksort algorithm is able to thrive with a devil-may-care attitude, even without the luxurious hair.&#x20;",
                '19. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-09 at 11.34.34 PM.png" alt=""><figcaption><p>Spirit of Randomized Quicksort Algorithm.</p></figcaption></figure>',
                "20. ### Philosophy 2: Smarter Pivot Selection",
                '21. Another approach to the problem is to choose a "good" pivot effectively, somehow. &#x20;',
                "22. #### 2a. Constant time pivot pick",
                '23. One of such approach is to pick a few items, and then among them, choose the "best" one as the pivot. This type of approach exemplifies a type of pivot selection procedure that is both _deterministic_ and _constant_ time.',
                '24. However, a big drawback of such procedure is that the resulting Quicksort has a family of "dangerous inputs"---inputs that will lead to the worst case runtime or break the algorithm---that an adversary could easily generate.',
                "25. #### 2b. Linear time pivot pick",
                "26. Since we are partioning multiple times, always selecting the median as the pivot seems like a good idea, because each partition will allow us to cut the size of the resulting subarrays by half.&#x20;",
                "27. Therefore, to improve upon the idea from part a, we can calculate the _median_ of a given array, which can be done in linear time, then select that as the pivot.",
                '28. The "exact median QuickSort" algorithm will not have the technical vulnerabilities as the 2a approach, and has a runtime of $$\\theta(NlogN)$$. However, it is still slower than MergeSort.',
                "29. ### Philosophy 3: Introspection",
                "30. This philosophy simply relies on introspecting the recursive depth of our algorithm, and switch to MergeSort once we hit the depth threshold.&#x20;",
                "31. Although this is a reasonable approach, it is not common to use in practice.",
                "32. ## Quicksort Flavors vs. MergeSort: Who's the best?",
                "33. ### **Candidate One: Quicksort L3S**",
                "34. Quicksort L3S is the Quicksort that is introduced last time, with the following properties:",
                "35. * Always leftmost pivot selection\n* Partioning\n* Shuffle before starting",
                '36. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.00.46 AM.png" alt=""><figcaption></figcaption></figure>',
                "37. Unfortunately, 0 : 1 with Mergesort in the lead.&#x20;",
                "38. ### Candidate Two: Quicksort LTHS",
                "39. Quicksort LTHS is very similar to Quicksort L3S, except with a different partioning scheme: [**Tony Hoare's In-Place Partioning Scheme**](https://docs.google.com/presentation/d/1DOnWS59PJOa-LaBfttPRseIpwLGefZkn450TMSSUiQY/pub?start=false\\&loop=false\\&delayms=3000\\&slide=id.g12b16fb6b6\\_0\\_2).",
                "40. #### Tony Hoare's Partioning",
                "41. Imagine two pointers, one at each end of the items array, walking towards each other:",
                "42. * Left pointer loves small items.\n* Right pointer loves large items.\n* Big idea: Walk towards each other, swapping anything they don\u2019t like; stop when the two pointers cross each other.&#x20;\n  * Left pointer hates larger or equal items\n  * Right pointer hates smaller or equal items\n* New pivot = Item at right pointer.\n* End result is that things on left of pivot (originally the leftmost item) are \u201csmall\u201d and things on the right of pivot are \u201clarge\u201d.",
                "43. #### TH Partioning: An Intuitive Analogy",
                '44. Intuitively, you can think of each pointer as a little "police" for the "less than pivot" territory and "greater than pivot" territory. And think of the pivot as the "border line" between the two territories.&#x20;',
                '45. Their job is to find and arrest any "suspects", or items that don\'t belong to their respective territories, and trade the arrested suspects with the "police" on the other side.&#x20;',
                '46. Specifically, the left pointer is the "police" for the "less than pivot" territory, looking for any "suspects" (items greater than or equal to the pivot) that are not supposed to be on this side of the border line. Analogously, the right pointer looks for "suspects" (items less than or equal to the pivot) in the "greater than territory".&#x20;',
                '47. When the two "polices" meet, mission is done as they both have sufficiently searched across their representative territories.',
                "48. #### All this work for nothing?",
                "49. Not at all! All this work with a new partioning scheme did pay off, and using the TH partitioning scheme yields a very fast Quicksort: one that is faster than MergeSort!",
                "50. It's important to also note that:&#x20;",
                "51. * Faster schemes have been found since.\n* Overall runtime still depends crucially on pivot selection strategy!",
                '52. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.05.30 AM.png" alt=""><figcaption></figcaption></figure>',
                "53. 1 : 1 with a win for QuickSort Flavors!&#x20;",
                "54. ### Candidate Three: QuickSort PickTH",
                "55. #### Median Identification: Linear Time",
                "56. Recall from philosophy 2b that the idea of identifying the median and use it as the pivot is inefficient because finding the median itself is a costly process. (Runtime: $$\\theta(NlogN)$$",
                '57. Turns out, it is possible to find the median in $$\\theta(N)$$ time instead, with an algorithm called "[PICK](https://www.cs.princeton.edu/\\~wayne/cs423/lectures/selection-4up.pdf)".',
                "58. Will this improved version of Exact Median Quicksort perform better than Mergesort?",
                '59. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-10 at 12.35.14 AM.png" alt=""><figcaption></figcaption></figure>',
                "60. Sadly, no. And it was terrible! Likely for the following reasons:",
                "61. * Cost to compute medians is too high.\n* Have to live with worst case $$\\theta(N^2)$$ if we want good practical performance.",
                "62. 1 : 2 with Mergesort as the winner.&#x20;\n",
            ],
        "31.-software-engineering-ii/31.3-modular-design.md": [
            "1. ---\ndescription: A tool for managing complexity.\n---",
            "2. # 31.3 Modular Design",
            "3. Modular design is a powerful tool for managing complexity because it divides the project complexity into manageable pieces. One way to implement modular design is to create helper methods or interfaces. This way, the programmer can individually handle each component of complexity rather than having to always keep track of the details of every piece of code that they write.&#x20;",
            "4. ### Modules should be simple:",
            "5. In an ideal world, every module is totally independent from one another. Unfortunately, this is not possible because code from each module needs to call other modules. However, we can still try to  minimize these dependencies between modules! In other words, we want to minimize how many _things_ you need to know about a given module in order to use it. This is exactly what we mean when we talk about the difference between _implementation_ versus _interface_. A good module will not require the user to know the specific implementation in order to use it. Rather, it should be sufficient to just know the interface of the module. Changing the implementation of a module should not affect the interface.",
            '6. John Ousterhout once said: "The best modules are those whose interfaces are much simpler than their implementation." This is a good rule to swear by, and putting it into practice will save a lot of headache.&#x20;',
            "7. One other technique of minimizing complexity is to restrict what the user can do. If a user does not _need_ to interact with an instance variable, then don't give them access to it.&#x20;",
            "8. ### Interface rules:",
            "9. Interfaces have a further set of rules. These rules are divided into _formal_ and _informal_ rules. The difference between the two is that informal rules are not enforced by the compiler.&#x20;",
            "10. Formal rules are the list of method signatures. If a method is not implemented in a class that implements the interface, then the compiler will give an error.&#x20;",
            "11. Some examples of informal rules are:",
            "12. * If your iterator class does not call hasNext() on its own (for some reason) and instead requires the user to call it.&#x20;\n* Any exceptions that are thrown.\n* Any runtime specifications.",
            "13. Be especially wary of informal rules! They are hard to keep track of.&#x20;",
            "14. ### Modules should be deep",
            "15. Another cool idea is that Modules should be deep. Their simple interfaces but powerful functionality. We do this a lot like thats the 61B story! A set for example is a deep module that has power functioanlity and simple interfaces. So Red Black BSTs is very deep. I can add, contain, and delete, and there is nothing informal I need to know, it's all under the hood. Powerful functionality means that all operations are efficient. Tree balancing is maintained using sophisticated yet subtle rules. They are tricky and we hide them under the surface. The most important way to keep modules deep is by practicing information hiding.",
            "16. ### Information Hiding",
            "17. That is, make your variables private, don't let anyone see what's inside the module as much as you can. Embed all the cleverness inside the modules. So that will keep your interfaces simple. And also it would keep it easy to modify your system. If I made a mistake, I can go fix that without thinking about it in another context. The opposite of hiding information is leaking information.",
            "18. ### Leaking Information",
            "19. This occurs when design decisions are reflected across multiple modules.",
            "20. * Any change to one module requires a change to all modules\n* Information leakage is one of the most important red flags in software design\n* One of the best skills you can learn as a software designed is a high level of sensitivity to information leakage",
            "21. ### Temporal Decomposotion",
            '22. One of the biggest causes of information leaking is "temporal decomposition," especially in BYOW. The structure of your system very much reflects the order in which events occur.',
            "23. For example, student often do the following in BYOW:",
            "24. * Game is started with an input string, so call interactWithInputString()\n  * Parse the String and find the seed by extracting N#####S (example code that contains the seed.)\u00b9\n  * Generate the world\n  * Process each character using move(World, char)\n  * etc.\n* Game is started with no input String, so call interactWithKeyboard\n  * Display a menu and collect the seed (number we are using to generate the world).\u00b9\n  * Generate the world\n  * Until done, call moveWithKeyboard(World)\n  * etc.",
            "25. \u00b9 Because the temporal discussion of when you worked on the project and the temporal decomposition of when these things happen, you don't really recognize that they should be sharing code that collects and extracts the seed for example.",
            "26. ### Summary",
            "27. * Buld classes that provide functionality needed in many places in your code.\n* Create deep modules, classes with simple interfaces that do complicated things\n* avoid over-reliance on temporal decomposition where your decomposition is driven primarily by the order in which things occur.\n  * It\u2019s OK to use some temporal decomposition, but try to fix any information leakage that occurs!\n* Be strategic, not tactical.\n* Most importantly: Hide information from yourself when unneeded!",
            "28. ",
            "29. ",
        ],
        "31.-software-engineering-ii/31.5-exerises.md": [
            "1. # 31.5 Exerises",
            "2. ## Factual",
            "3. 1. For each of the following, state whether they prevent, exacerbate, or have no effect on information leakage.\n   * using unnecessary print statements to show the state of the program while it's running\n   * employing temporal decomposition when architecting your code\n   * creating deep modules\n   * using helper methods and classes\n2. In Woolley's study, which factors correlated with the quality of a team's output?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. The key to remember is that information leakage is _not_ related to the state of the program. Instead, it's when multiple pieces of code reflect a single design decision/module, introducing unnecessary complexity and dependencies.",
            "7. * **Using unnecessary print statements to show the state of the program while it's running:** no effect on information leakage.&#x20;\n* **Employing temporal decomposition when architecting your code**: exacerbates information leakage.\n* **Creating deep modules**: prevents information leakage.\n* **Using helper methods and classes**: prevents information leakage.",
            "8. </details>",
            "9. <details>",
            "10. <summary>Problem 2</summary>",
            "11. The two most important factors were _turn-taking_ during conversations and the average ability of the group members to _recognize emotional state_ from a person's eyes.",
            "12. </details>",
            "13. ## Metacognitive",
            "14. 1. Give some examples of unnecessary obscurity that contributes to complexity.\n2. Give some examples of unnnecessary dependencies that contribute to complexity.",
            "15. <details>",
            "16. <summary>Problem 1</summary>",
            "17. There are many kinds of obscurity that can contribute to complexity. These include putting too much code into one function or module, having a large amount of variables that need to be maniuplated, and not breaking up your code or having helper functions.",
            "18. </details>",
            "19. <details>",
            "20. <summary>Problem 2</summary>",
            "21. One example is copy-pasting code across different modules. This means that to change this block of code, you have to change every location that this code appears in.",
            "22. </details>\n",
        ],
        "31.-software-engineering-ii/31.2-sources-of-complexity.md": [
            "1. # 31.2 Sources of Complexity",
            "2. There are two primary sources of complexity:",
            "3. * Dependencies: when a piece of code cannot be read, understood, or modified independently.\n* Obscurity: when important information is not obvious.&#x20;",
            "4. In the last section, we showed a code snippet from Project 3 BYoW:",
            '5. <figure><img src="../.gitbook/assets/complex code.png" alt=""><figcaption></figcaption></figure>',
            "6. The complex manual computation of whether a move is valid is an example of obscurity. In order to understand the code, we would also need to understand things about the structure of the `world` array. We also do not know if `player.yycenter+1` is for moving in the up direction or down direction. The repetitive code is an example of dependencies. The updates cannot be modified independently--they all have to be modified in conjunction, or else the behavior is incorrect.",
            "7. In order to create good systems that last, it is best to keep dependencies and obscurity at a minimum.",
            "8. ### Tactical Programming",
            "9. Tactical programming describes a linear approach to writing code, where functions are written only when they are needed. In other words, just trying to get something working instead of first thinking about the overall architecture. It is impractical to use this style of programming when writing anything larger than a small script.&#x20;\n",
        ],
        "31.-software-engineering-ii/31.4-teamwork.md": [
            "1. ---\ndescription: '\"There is no I in TEAM\"'\n---",
            "2. # 31.4 Teamwork",
            "3. In this project, CS 61B really pushes you to work together just like in our last project 2B. We are doing teams of 2 this semester so that there is more creativity in the open-endedness of this Software Engineering Project.",
            "4. Ancillary reason: Also reduces programming workload per person, but the project is small enough that a single person can handle it. So please work well together and coordinate how you would like to work together through the small time you have in finishing this project.",
            "5. Typically, we have done another project, Gitlet, which is a solo software engineering project in the year 2015. In 2016, the software engineering project was to make an Editor which was again, solo. In 2017, we had an amazing project called Databases which was a partner-project! And since 2018 to now, 2022, we have now been doing BYOW with a partner. I personallu, actually had Professor Hilfinger in Spring 2022 where I did Gitlet, but we are now back to BYOW. A majority of partnerships have gone well, but some have not unfortunately. In the real world, some tasks are much too large to be handled by a single person. When faced with the same task, some teams succeed, where others may fail. We ask that you do your best to partner up. Remember to take turns in coding and communicate efficiently and effectively. Please treat each other with respect and be open and honest with each other. Make sure to set clear expectations and please start early on. If these expectations are not met, confront this fact head on.&#x20;",
            "6. ### Reflexivity",
            "7. * \u201cA group\u2019s ability to collectively reflect upon team objectives, strategies, and processes, and to adapt to them accordingly.\u201d\n* Recommended that you \u201ccultivate a collaborative environment in which giving and receiving feedback on an ongoing basis is seen as a mechanism for reflection and learning.\u201d\n  * It\u2019s OK and even expected for you and your partner to be a bit unevenly matched in terms of programming ability",
            "8. ### Feedback (especially negative)",
            "9. Sometimes we recieve feedback that makes our day and cheers us up. Sometimes we recieve criticism which feel judgemental or in bad faith. Thus we personally sometimes fear giving feedback in fear that our feedback is misconstrued as an attack or taken offensively. Please do not take negative feedback to heart. Please always remember  feedback is about listening actively and being patient and then responding by improving what can be fixed. All of you are amazing students and I know all of you are capable of making good decisions. So please make sure to be responsible and respectful of your peers and always feel free to talk to a member of course staff if you are not feeling okay about something. We are always here to help.",
            "10. ### Team Reflection",
            "11. We ask that you please reflect on this software engineering project and the teamwork that was present in this project.",
            "12. * This partnership has worked well for me.\n* This partnership has worked well for my partner.\n* Estimate the balance of work between you and your partner, briefly explain.\n  * 50/50 means you estimate you each contributed about equally.\n  * Note, contributing does not mean lines of code. We mean contribution in a more general sense.\n* If applicable, give a particularly great moment from your partnership.\n* What\u2019s something you could do better?\n* What\u2019s something your partner could do better?",
            "13. Note: Except in extreme cases, we will not be penalizing partners who contributed less!&#x20;",
            "14. Everyone on course staff hopes you use this advice to only benefit you in your final and memorable software engineering project and hopefully the software engineering projects that are yet to come in your lives! Happy coding everyone!\\\n",
        ],
        "31.-software-engineering-ii/README.md": [
            "1. ---\ndescription: By Thomas Lee and Mihir Mirchandani\n---",
            "2. # 31. Software Engineering II",
            "3. ",
        ],
        "31.-software-engineering-ii/31.1-complexity-ii.md": [
            "1. ---\ndescription: Complexity comshmexity.\n---",
            "2. # 31.1 Complexity II",
            "3. In the previous Software Engineering lecture, we introduced the concept of complexity.&#x20;",
            "4. We define complexity as: anything related to the structure of a software system that makes it hard to understand and modify the system.",
            "5. Project 3: Build Your own World provides the first opportunity to create complex code. We do not explicitly define what constitutes a room and what constitutes a hallway. There is a great deal of freedom for not just the implementation but the entire architecture of the project. As a result, it is of utmost importance to understand how to manage the complexity.&#x20;",
            "6. In this chapter, we will use examples from Project 3: BYoW to illustrate cases of poorly managed complexity. Consider the following code snippet:",
            '7. <figure><img src="../.gitbook/assets/complex code.png" alt=""><figcaption></figcaption></figure>',
            "8. There are multiple complexity issues with this code:",
            "9. 1. It lacks readability. The code itself is not narrative. It is not immediately clear to someone looking at the code for the first time to figure out what it does.\n2. There are many complex manual computations of moving in each direction. The details of these should be hidden away.\n3. The code is very repetitive. It will be difficult to make changes because the programmer will need to make certain that all of the updates have been fixed.",
            "10. Ultimately, there is no _right_ answer for what the best practice is for coding in this movement functionality. One could argue that this code could benefit from having helper methods to check if a move in a certain direction is valid and helper methods to do the updates. However, it could also be argued that this code is simple enough to not have any helper methods. It is up to the programmer to decide what would be easiest to write and maintain.\n",
        ],
        "18.-red-black-trees/18.4-runtime-analysis.md": [
            "1. ---\ndescription: Short and Sweet.\n---",
            "2. # 18.4 Runtime Analysis",
            "3. Because a left-leaning red-black tree has a 1-1 correspondence with a 2-3 tree and will always remain within 2x the height of its 2-3 tree, the runtimes of the operations will take $$log(N)$$time.",
            '4. {% embed url="https://www.youtube.com/watch?index=8&list=PL8FaHk7qbOD6aKgTz2W-foDiTeBEaBoS3&v=uNrmQ_EwJLU" %}',
            "5. Here's the abstracted code for insertion into a LLRB:",
        ],
        "18.-red-black-trees/18.5-summary.md": [
            "1. # 18.5 Summary",
            "2. * Binary search trees are simple, but they are subject to imbalance which leads to crappy runtime. 2-3 Trees (B Trees) are balanced, but painful to implement.\n* LLRB insertion is simple to implement (deletion is a bit harder to implement, we won't go over the specifics in this course).\n  * Use three basic operations to maintain the balanced structure, namely rotateLeft, rotateRight, and color flip.\n* LLRBs maintain correspondence with 2-3 trees, Standard Red-Black trees maintain correspondence with 2-3-4 trees.\n  * Java\u2019s [TreeMap](https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/999dbd4192d0f819cb5224f26e9e7fa75ca6f289/src/java.base/share/classes/java/util/TreeMap.java) is a red-black tree that corresponds to 2-3-4 trees.\n  * 2-3-4 trees allow glue links on either side (see [Red-Black Tree](http://en.wikipedia.org/wiki/Red%E2%80%93black\\_tree)).\n  * More complex implementation, but faster.\n",
        ],
        "18.-red-black-trees/18.2-creating-llrb-trees.md": [
            "1. ---\ndescription: Software Engineers? More like Tree Engineers\n---",
            "2. # 18.2 Creating LLRB Trees",
            "3. We said in the previous section that we really like 2-3 trees because they always remain balanced, but they are very hard to implement. On the other hand, BSTs can be unbalanced, but we like how simple and intuitive they are. Is there a way to combine the best of two worlds? Why not create a tree that is implemented using a BST, but is structurally identical to a 2-3 tree and thus stays balanced?&#x20;",
            "4. (Note that in this chapter we will be honing in on 2-3 Trees specifically, not 2-3-4 trees)",
            "5. ## Entering Red Black Trees",
            '6. {% embed url="https://www.youtube.com/watch?index=3&list=PL8FaHk7qbOD6aKgTz2W-foDiTeBEaBoS3&v=q7sfCkdrtEs" %}',
            "7. ### From 2-3 Tree to What?",
            "8. We are going to create this tree by looking at a 2-3 tree and asking ourselves what kind of modifications we can make in order to convert it into a BST.",
            "9. For a 2-3 tree that only has 2-nodes (nodes with 2 children), we already have a BST, so we don't need to make any modifications!",
            "10. However, what happens when we get a 3-node?",
            '11. One thing we could do is create a "glue" node that doesn\'t hold any information and only serves to show that its 2 children are actually a part of one node.',
            '12. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 8.28.55 PM.png" alt=""><figcaption><p>Naive Solution: Dummy "glue" node </p></figcaption></figure>',
            "13. However, this is not an elegant solution because we are taking up more space and the code will be ugly. So, instead of using glue nodes we will use **glue links** instead!",
            "14. ### To LLRB Tree",
            "15. To transform dummy glue nodes to glue links, we choose arbitrarily to make the left element a child of the right one. This results in a **left-leaning** tree.&#x20;",
            '16. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 8.31.57 PM.png" alt=""><figcaption><p>Left Leaning Red-Black Tree</p></figcaption></figure>',
            "17. We show that a link is a glue link by making it red. Normal links are black. Because of this, we call these structures **left-leaning red-black trees (LLRB)**. We will be using left-leaning trees in this course.",
            "18. ### One-to-One Correspondence:",
            "19. Left-Leaning Red-Black trees have a **1-1 correspondence with 2-3 trees.** Every 2-3 tree has a **unique** LLRB red-black tree associated with it.&#x20;",
            "20. As for 2-3-4 trees, they maintain correspondence with standard Red-Black trees.",
            "21. ## Properties of LLRBs",
            '22. {% embed url="https://www.youtube.com/watch?index=4&list=PL8FaHk7qbOD6aKgTz2W-foDiTeBEaBoS3&v=4nZFgj7t52E" %}',
            "23. Below is a summary of the properties/invariants of LLRB Trees:&#x20;",
            "24. * 1-1 correspondence with 2-3 trees.\n* No node has 2 red links.\n* There are no red right-links.\n* Every path from root to leaf has the same number of black links (because 2-3 trees have the same number of links to every leaf).\n* Height is no more than 2x height + 1 of the corresponding 2-3 tree.&#x20;\n* The height of a red-black tree is proportional to the log of the number of entries.\n",
        ],
        "18.-red-black-trees/README.md": [
            "1. ---\ndescription: Time for some colors.\n---",
            "2. # 18. Red Black Trees",
            "3. This chapter will continue our discussion on self-balancing trees through a more colorful lens.\n",
        ],
        "18.-red-black-trees/18.1-rotating-trees.md": [
            "1. ---\ndescription: Just like Ferris Wheels.\n---",
            "2. # 18.1 Rotating Trees",
            "3. Wonderfully balanced as they are, B-Trees are really difficult to implement. We need to keep track of the different nodes and the splitting process is pretty complicated. As computer scientists who appreciate clean code and a good challenge, let's find another way to create a balanced tree.",
            "4. ## Multiple Structures of BST",
            '5. {% embed url="https://www.youtube.com/watch?index=3&list=PL8FaHk7qbOD6aKgTz2W-foDiTeBEaBoS3&v=kkd8d0QhiQ0" %}',
            "6. For any BST, there are multiple ways to structure it so that you maintain the BST invariants. Earlier, we talked about how **inserting** elements in different orders will result in a different BST. The sequence in which one inserts into a BST will affect its structure. The BSTs below all consist of the elements 1, 2, and 3, yet all have different structures.&#x20;",
            '7. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 7.32.00 PM.png" alt=""><figcaption></figcaption></figure>',
            "8. <details>",
            "9. <summary>Exercise:  For each tree shown above, provide an order of insertion that yields the structure.</summary>",
            "10. 1. insert(1), insert(2), insert(3)\n2. insert(1), insert(3), insert(2)\n3. insert(2), insert(1), insert(3) or insert(2), insert(3), insert(1)\n4. insert(3), insert(1), insert(2)\n5. insert(3), insert(2), insert(1)",
            "11. </details>",
            "12. ## Tree Rotation",
            "13. The formal definition of rotation is:",
            "14. ```\nrotateLeft(G): Let x be the right child of G. Make G the new left child of x.\n```",
            "15. ```\nrotateRight(G): Let x be the left child of G. Make G the new right child of x.\n```",
            "16. We will slowly demystify this process in the next few paragraphs.",
            "17. Below is a graphical description of what happens in a left rotation on the node G:",
            '18. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 7.38.06 PM.png" alt=""><figcaption><p>rotateLeft(G)</p></figcaption></figure>',
            "19. The written description of what happened above is this:",
            "20. * G's right child, P, merges with G, bringing its children along.\n* P then passes its left child to G and G goes down to the left to become P's left child.",
            "21. You can see that the structure of the tree changes as well as its height. We can also rotate on a non-root node. We just disconnect the node from the parent temporarily, rotate the subtree at the node, then reconnect the new root.",
            "22. ### Implementation",
            "23. Here are the implementations of `rotateRight` and `rotateLeft:`",
            "24. ```\nprivate Node rotateRight(Node h) {\n    // assert (h != null) && isRed(h.left);\n    Node x = h.left;\n    h.left = x.right;\n    x.right = h;\n    return x;\n}\n\n// make a right-leaning link lean to the left\nprivate Node rotateLeft(Node h) {\n    // assert (h != null) && isRed(h.right);\n    Node x = h.right;\n    h.right = x.left;\n    x.left = h;\n    return x;\n}\n```",
            "25. You may be wondering how does the parent node know about which node to point to after we rotate? That's why we are returning x, and other parts of our code will make use of this information to correctly update the parent node's pointer.",
            "26. ## Balancing BSTs with Tree Rotation",
            '27. {% embed url="https://www.youtube.com/watch?index=2&list=PL8FaHk7qbOD6aKgTz2W-foDiTeBEaBoS3&v=b4-2-6R2gzU" %}',
            "28. With rotations, we can actually balance a tree.&#x20;",
            "29. Consider the example below:",
            '30. <figure><img src="../.gitbook/assets/image (90).png" alt=""><figcaption></figcaption></figure>',
            "31. We can do balance the given BST on the left by calling:",
            "32. 1. `rotateRight(3)`\n2. `rotateLeft(1)`",
            "33. The main observation to make for tree rotation is that it is possible to **shorten** or **lengthen** a tree, while maintaining the search tree's property.",
            "34. For more examples, see the demo in [these slides](https://docs.google.com/presentation/d/1pfkQENfIBwiThGGFVO5xvlVp7XAUONI2BwBqYxib0A4/edit#slide=id.g465b5392c\\_00).\n",
        ],
        "18.-red-black-trees/18.3-inserting-llrb-trees.md": [
            "1. ---\ndescription: Some Tree Maintenance.\n---",
            "2. # 18.3 Inserting LLRB Trees",
            "3. We can always insert into an LLRB tree by inserting into a 2-3 tree and converting it as we've done before. However, this would be contrary to our original purpose of creating LLRBs, which was to avoid the complicated implementation of a 2-3 tree!&#x20;",
            "4. Instead, we **insert into the LLRB as we would with a normal BST.** However, this could break its 1-1 mapping to a 2-3 tree, so we will use **rotations** to massage the tree back into a proper structure.",
            '5. {% embed url="https://www.youtube.com/watch?index=5&list=PL8FaHk7qbOD6aKgTz2W-foDiTeBEaBoS3&v=GjTDBrB7QV4" %}',
            "6. ## Maintaining Proper Structure",
            "7. When inserting into an LLRB tree, we always insert the new node with a **red link** to its parent node. This is because in a 2-3 tree, we are always inserting by adding to a leaf node, the color of the link we add should always be red.",
            "8. But sometimes, inserting red links at certain places might lead to cases where we break one of the invariants of LLRBs.&#x20;",
            "9. Below are three such cases where we need to perform certain tasks address in order to maintain the LLRB tree's proper structure:",
            '10. *   Case 1: **Insertion results in a right leaning "3-node -> Left Leaning Violation**',
            '11.     * **Task 1: rotateLeft(node)**\n    * Recall, we are using _left-leaning_ red black trees, which means we can never have a right red link.&#x20;\n    * To address this, we will need to use a **rotation** in order to maintain the LLRB invariant: "There are no right red links".&#x20;\n    * If the left child is _also_ a red link, then we will temporarily allow it for purposes that will become clearer in case 2.',
            "12. ",
            '13.     <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 8.57.00 PM.png" alt=""><figcaption><p>Case 1: Inserting a Right Red Link</p></figcaption></figure>\n*   Case 2: **Double Insertion on the Left -> Incorrect 4 Node Violation**',
            "14.     * **Task 2: rotateRight(node)**\n    * If there are 2 left red links, then we have a 4-node which is illegal.&#x20;\n    * To address this, we will first **rotate** to create the same tree seen in case 1 above.  Then, in both situations, we will address the temporary issue of having an illegal 4-node in case 3.&#x20;",
            "15. ",
            '16.     <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 8.58.30 PM.png" alt=""><figcaption><p>Case 2: Double Insertion on the Left</p></figcaption></figure>\n*   Case 3: **Node has Two Red Children -> Temporary 4 Node**',
            '17.     * **Task 3: Color Flip**\n    * Finally, to address the issue of having illegal 4-node by **flipping the colors** of all edges touching "S" above.&#x20;\n    * This is equivalent to pushing up the middle node in a 2-3 tree.',
            "18. ",
            '19.     <figure><img src="../.gitbook/assets/Screen Shot 2023-02-27 at 9.02.17 PM.png" alt=""><figcaption><p>Case 3: Two Red Links</p></figcaption></figure>',
            "20. Note: You may need to go through a series of rotations in order to complete the transformation. It's possible that one operation will result in additional LLRB rule violations, which we can fix with the corresponding operation.",
            "21. The process is: while the LLRB tree does not satisfy the 1-1 correspondence with a 2-3 tree or breaks the LLRB invariants, perform task 1, 2, or 3 depending on the condition of the tree until you get a legal LLRB.",
            "22. ### Summary of Maintenance Operations",
            "23. * When inserting: Use a **red link**. Insert in the same way as inserting into a BST.\n* If there is a right-leaning \u201c3-node\u201d, we have a **Left Leaning Violation**.\n  * **Rotate left** the appropriate node to fix.\n* If there are two consecutive left links, we have an **Incorrect 4 Node Violation**.\n  * **Rotate right** the appropriate node to fix.\n* If there are any nodes with two red children, we have a **Temporary 4 Node**.\n  * **Color flip** the node to emulate the split operation.\n",
        ],
        "18.-red-black-trees/18.6-exercises.md": [
            "1. # 18.6 Exercises",
            "2. ## Factual",
            "3. 1. Consider the tree below. If we call `rotateLeft(C)`, which operation reverts the tree back to its original form?",
            "4. ![](<../.gitbook/assets/image (123).png>)",
            "5. 2. When you rotate a nodes in a tree, which of the following can happen?\n   * [ ] If the tree was previously a valid search tree, it can become invalid.\n   * [ ] The height can stay the same.\n   * [ ] The height can increase.\n   * [ ] The height can decrease.\n   * [ ] The number of nodes can change.\n   * [ ] The root of the tree can change.",
            "6. <details>",
            "7. <summary>Problem 1</summary>",
            "8. `rotateRight(D)`. The inverse of any `rotateLeft` operation is a `rotateRight` on the node's left child.",
            "9. </details>",
            "10. <details>",
            "11. <summary>Problem 2</summary>",
            "12. * [ ] **If the tree was previously a valid search tree, it can become invalid.** Rotation always preserves search tree properties.\n* [x] **The height can stay the same.** If the rotation does not affect any leaf nodes on the bottommost level, the height will stay the same. &#x20;\n* [x] **The height can increase.** If the rotation affects a leaf on the bottommost level, the height can increase.\n* [x] **The height can decrease.** If the rotation affects a leaf on the bottommost level, the height can increase.\n* [ ] **The number of nodes can change.** Rotation only changes the structure of the tree, not the nodes.\n* [x] **The root of the tree can change.** For example, rotating the root.",
            "13. </details>",
            "14. ## Procedural",
            "15. 1. Consider the following LLRB. What is the height of the corresponding 2-3 tree and how many 3-nodes does it have?",
            "16. ![](<../.gitbook/assets/image (99).png>)",
            "17. 2. Suppose we insert `15` in the LLRB above. What is the first operation that must be applied to maintain the LLRB invariants?\n3. Suppose in the process of insertion, we end up with the following temporary 4-node. What is the corresponding LLRB representation of this node?",
            "18. ![](<../.gitbook/assets/image (73).png>)",
            "19. <details>",
            "20. <summary>Problem 1</summary>",
            "21. ![](<../.gitbook/assets/Check-in 18 Q3 Answer Img.png>)",
            "22. The corresponding 2-3 tree has height 1 and has two 3-nodes (`17 25` and `39 43`).",
            "23. </details>",
            "24. <details>",
            "25. <summary>Problem 2</summary>",
            "26. `15` is inserted to the right of `13`. Since we cannot have a right-leaning red link, we must `rotateLeft(13)`.",
            "27. </details>",
            "28. <details>",
            "29. <summary>Problem 3</summary>",
            "30. ![](<../.gitbook/assets/image (118).png>)",
            "31. </details>",
            "32. ## Metacognitive",
            "33. 1. Give a range of values, when inserted into the LLRB below, results in a `rotateRight` operation as the first balancing operation. Assume that values are distinct, but not necessarily integers.",
            "34. ![](<../.gitbook/assets/image (99).png>)",
            "35. <details>",
            "36. <summary>Problem 1</summary>",
            "37. `rotateRight` occurs when we have two red links in a row. This occurs when we insert to the left of `39`. This value must be larger than `25` (since it is in its right branch) but less than `39` (since it is the left child of 39). So our final range is $$(25, 39)$$.",
            "38. </details>\n",
        ],
        "8.-arraylist.md": [
            "1. # 8. ArrayList",
            "2. In this section, we'll build a new class called `AList` that can be used to store arbitrarily long lists of data, similar to our `DLList`. Unlike the `DLList`, the `AList` will use arrays to store data instead of a linked list.",
            '3. #### Linked List Performance Puzzle <a href="#linked-list-performance-puzzle" id="linked-list-performance-puzzle"></a>',
            "4. Suppose we wanted to write a new method for `DLList` called `int get(int i)`. Why would `get` be slow for long lists compared to `getLast`? For what inputs would it be especially slow?",
            "5. You may find the figure below useful for thinking about your answer.",
            "6. ![dllist\\_circular\\_sentinel\\_size\\_2.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_circular\\_sentinel\\_size\\_2.png)",
            '7. #### Linked List Performance Puzzle Solution <a href="#linked-list-performance-puzzle-solution" id="linked-list-performance-puzzle-solution"></a>',
            "8. It turns out that no matter how clever you are, the `get` method will usually be slower than `getBack` if we're using the doubly linked list structure described in section 2.3.",
            "9. This is because, since we only have references to the first and last items of the list, we'll always need to walk through the list from the front or back to get to the item that we're trying to retrieve. For example, if we want to get item #417 in a list of length 10,000, we'll have to walk across 417 forward links to get to the item we want.",
            "10. In the very worst case, the item is in the very middle and we'll need to walk through a number of items proportional to the length of the list (specifically, the number of items divided by two). In other words, our worst case execution time for `get` is linear in the size of the entire list. This in contrast to the runtime for `getBack`, which is constant, no matter the size of the list. Later in the course, we'll formally define runtimes in terms of big O and big Theta notation. For now, we'll stick to an informal understanding.",
            '11. #### Our First Attempt: The Naive Array Based List <a href="#our-first-attempt-the-naive-array-based-list" id="our-first-attempt-the-naive-array-based-list"></a>',
            "12. Accessing the `i`th element of an array takes constant time on a modern computer. This suggests that an array-based list would be capable of much better performance for `get` than a linked-list based solution, since it can simply use bracket notation to get the item of interest.",
            "13. If you'd like to know **why** arrays have constant time access, check out this [Quora post](https://www.quora.com/Why-does-accessing-an-array-element-take-constant-time).",
            "14. **Optional Exercise 2.5.1:** Try to build an AList class that supports `addLast`, `getLast`, `get`, and `size` operations. Your AList should work for any size array up to 100. For starter code, see [https://github.com/Berkeley-CS61B/lectureCode/tree/master/lists4/DIY](https://github.com/Berkeley-CS61B/lectureCode/tree/master/lists4/DIY).",
            "15. [My solution](https://github.com/Berkeley-CS61B/lectureCode/tree/master/lists4/naive) has the following handy invariants.",
            "16. * The position of the next item to be inserted (using `addLast`) is always `size`.\n* The number of items in the AList is always `size`.\n* The position of the last item in the list is always `size - 1`.",
            "17. Other solutions might be slightly different.",
            '18. #### removeLast <a href="#removelast" id="removelast"></a>',
            "19. The last operation we need to support is `removeLast`. Before we start, we make the following key observation: Any change to our list must be reflected in a change in one or more memory boxes in our implementation.",
            "20. This might seem obvious, but there is some profundity to it. The list is an abstract idea, and the `size`, `items`, and `items[i]` memory boxes are the concrete representation of that idea. Any change the user tries to make to the list using the abstractions we provide (`addLast`, `removeLast`) must be reflected in some changes to these memory boxes in a way that matches the user's expectations. Our invariants provide us with a guide for what those changes should look like.",
            "21. **Optional Exercise 2.5.2:** Try to write `removeLast`. Before starting, decide which of `size`, `items`, and `items[i]` needs to change so that our invariants are preserved after the operation, i.e. so that future calls to our methods provide the user of the list class with the behavior they expect.",
            '22. #### Naive Resizing Arrays <a href="#naive-resizing-arrays" id="naive-resizing-arrays"></a>',
            "23. **Optional Exercise 2.5.3:** Suppose we have an AList in the state shown in the figure below. What will happen if we call `addLast(11)`? What should we do about this problem?",
            "24. ![dllist\\_circular\\_sentinel\\_size\\_2.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig25/full\\_naive\\_alist.png)",
            "25. The answer, in Java, is that we simply build a new array that is big enough to accomodate the new data. For example, we can imagine adding the new item as follows:",
            "26. ```java\nint[] a = new int[size + 1];\nSystem.arraycopy(items, 0, a, 0, size);\na[size] = 11;\nitems = a;\nsize = size + 1;\n```",
            "27. The process of creating a new array and copying items over is often referred to as \"resizing\". It's a bit of a misnomer since the array doesn't actually change size, we are just making a **new** one that has a bigger size.",
            "28. **Exercise 2.5.4:** Try to implement the `addLast(int i)` method to work with resizing arrays.",
            '29. #### Analyzing the Naive Resizing Array <a href="#analyzing-the-naive-resizing-array" id="analyzing-the-naive-resizing-array"></a>',
            "30. The approach that we attempted in the previous section has terrible performance. By running a simple computational experiment where we call `addLast` 100,000 times, we see that the `SLList` completes so fast that we can't even time it. By contrast our array based list takes several seconds.",
            "31. To understand why, consider the following exercise:",
            "32. **Exercise 2.5.5:** Suppose we have an array of size 100. If we call insertBack two times, how many total boxes will we need to create and fill throughout this entire process? How many total boxes will we have at any one time, assuming that garbage collection happens as soon as the last reference to an array is lost?",
            "33. **Exercise 2.5.6:** Starting from an array of size 100, approximately how many memory boxes get created and filled if we call `addLast` 1,000 times?",
            "34. Creating all those memory boxes and recopying their contents takes time. In the graph below, we plot total time vs. number of operations for an SLList on the top, and for a naive array based list on the bottom. The SLList shows a straight line, which means for each `add` operation, the list takes the same additional amount of time. This means each single operation takes constant time! You can also think of it this way: the graph is linear, indicating that each operation takes constant time, since the integral of a constant is a line.",
            "35. By contrast, the naive array list shows a parabola, indicating that each operation takes linear time, since the integral of a line is a parabola. This has significant real world implications. For inserting 100,000 items, we can roughly compute how much longer by computing the ratio of N^2/N. Inserting 100,000 items into our array based list takes (100,000^2)/100,000 or 100,000 times as long. This is obviously unacceptable.",
            "36. ![fig25/insert\\_experiment.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig25/insert\\_experiment.png)",
            '37. #### Geometric Resizing <a href="#geometric-resizing" id="geometric-resizing"></a>',
            "38. We can fix our performance problems by growing the size of our array by a multiplicative amount, rather than an additive amount. That is, rather than **adding** a number of memory boxes equal to some resizing factor `RFACTOR`:",
            "39. ```java\npublic void insertBack(int x) {\n    if (size == items.length) {\n           resize(size + RFACTOR);\n    }\n    items[size] = x;\n    size += 1;\n}\n```",
            "40. We instead resize by **multiplying** the number of boxes by `RFACTOR`.",
            "41. ```java\npublic void insertBack(int x) {\n    if (size == items.length) {\n           resize(size * RFACTOR);\n    }\n    items[size] = x;\n    size += 1;\n}\n```",
            "42. Repeating our computational experiment from before, we see that our new `AList` completes 100,000 inserts in so little time that we don't even notice. We'll defer a full analysis of why this happens until the final chapter of this book.",
            '43. #### Memory Performance <a href="#memory-performance" id="memory-performance"></a>',
            "44. Our `AList` is almost done, but we have one major issue. Suppose we insert 1,000,000,000 items, then later remove 990,000,000 items. In this case, we'll be using only 10,000,000 of our memory boxes, leaving 99% completely unused.",
            '45. To fix this issue, we can also downsize our array when it starts looking empty. Specifically, we define a "usage ratio" R which is equal to the size of the list divided by the length of the `items` array. For example, in the figure below, the usage ratio is 0.04.',
            "46. ![fig25/usage\\_ratio.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig25/usage\\_ratio.png)",
            "47. In a typical implementation, we halve the size of the array when R falls to less than 0.25.",
            '48. #### Generic ALists <a href="#generic-alists" id="generic-alists"></a>',
            "49. Just as we did before, we can modify our `AList` so that it can hold any data type, not just integers. To do this, we again use the special angle braces notation in our class and substitute our arbitrary type parameter for integer wherever appropriate. For example, below, we use `Glorp` as our type parameter.",
            "50. There is one significant syntactical difference: Java does not allow us to create an array of generic objects due to an obscure issue with the way generics are implemented. That is, we cannot do something like:",
            "51. ```java\nGlorp[] items = new Glorp[8];\n```",
            "52. Instead, we have to use the awkward syntax shown below:",
            "53. ```java\nGlorp[] items = (Glorp []) new Object[8];\n```",
            "54. This will yield a compilation warning, but it's just something we'll have to live with. We'll discuss this in more details in a later chapter.",
            '55. The other change we make is that we null out any items that we "delete". Whereas before, we had no reason to zero out elements that were deleted, with generic objects, we do want to null out references to the objects that we\'re storing. This is to avoid "loitering". Recall that Java only destroys objects when the last reference has been lost. If we fail to null out the reference, then Java will not garbage collect the objects that have been added to the list.',
            "56. This is a subtle performance bug that you're unlikely to observe unless you're looking for it, but in certain cases could result in a significant wastage of memory.\n",
        ],
        "6.-arrays.md": [
            "1. # 6. Arrays",
            "2. So far, we've seen how to harness recursive class definitions to create an expandable list class, including the `IntList`, `SLList`, and `DLList`. In the next two sections of this book, we'll discuss how to build a list class using arrays.",
            "3. This section of this book assumes you've already worked with arrays and is not intended to be a comprehensive guide to their syntax.",
            '4. ### Array Basics <a href="#array-basics" id="array-basics"></a>',
            "5. To ultimately build a list that can hold information, we need some way to get memory boxes. Prevously, we saw how we could get memory boxes with variable declarations and class instantiations. For example:",
            "6. * `int x;` gives us a 32 bit memory box that stores ints.\n* `Walrus w1;` gives us a 64 bit memory box that stores Walrus references.\n* `Walrus w2 = new Walrus(30, 5.6);` gets us 3 total memory boxes. One 64 bit box that stores Walrus references, one 32 bit box that stores the int size of the Walrus, and a 64 bit box that stores the double tuskSize of the Walrus.",
            "7. Arrays are a special type of object that consists of a numbered sequence of memory boxes. This is unlike class instances, which have named memory boxes. To get the ith item of an array, we use bracket notation as we saw in HW0 and Project 0, e.g. `A[i]` to get the `i`th element of A.",
            "8. Arrays consist of:",
            "9. * A fixed integer length, N\n* A sequence of N memory boxes (N = length) where all boxes are of the same type, and are numbered 0 through N - 1.",
            "10. Unlike classes, arrays do not have methods.",
            '11. #### Array Creation <a href="#array-creation" id="array-creation"></a>',
            "12. There are three valid notations for array creation. Try running the code below and see what happens. Click [here](http://pythontutor.com/iframe-embed.html#code=public%20class%20ArrayCreationDemo%20%7B%0A%20%20public%20static%20void%20main\\(String%5B%5D%20args%29%20%7B%0A%20%20%20%20int%5B%5D%20x%3B%0A%20%20%20%20int%5B%5D%20y%3B%0A%20%20%20%20x%20%3D%20new%20int%5B3%5D%3B%0A%20%20%20%20y%20%3D%20new%20int%5B%5D%7B1,%202,%203,%204,%205%7D%3B%0A%20%20%20%20int%5B%5D%20z%20%3D%20%7B9,%2010,%2011,%2012,%2013%7D%3B%0A%09%7D%0A%7D\\&codeDivHeight=400\\&codeDivWidth=350\\&cumulative=false\\&curInstr=0\\&heapPrimitives=false\\&origin=opt-frontend.js\\&py=java\\&rawInputLstJSON=%5B%5D\\&textReferences=false) for an interactive visualization.",
            "13. * `x = new int[3];`\n* `y = new int[]{1, 2, 3, 4, 5};`\n* `int[] z = {9, 10, 11, 12, 13};`",
            "14. All three notations create an array. The first notation, used to create `x`, will create an array of the specified length and fill in each memory box with a default value. In this case, it will create an array of length 3, and fill each of the 3 boxes with the default `int` value `0`.",
            "15. The second notation, used to create `y`, creates an array with the exact size needed to accommodate the specified starting values. In this case, it creates an array of length 5, with those five specific elements.",
            "16. The third notation, used to declare **and** create `z`, has the same behavior as the second notation. The only difference is that it omits the usage of `new`, and can only be used when combined with a variable declaration.",
            "17. None of these notations is better than any other.",
            '18. #### Array Access and Modification <a href="#array-access-and-modification" id="array-access-and-modification"></a>',
            "19. The following code showcases all of the key syntax we'll use to work with arrays. Try stepping through the code below and making sure you understand what happens when each line executes. To do so, click [here](https://goo.gl/bertuh) for an interactive visualization. With the exception of the final line of code, we've seen all of this syntax before.",
            '20. ```java\nint[] z = null;\nint[] x, y;\n\nx = new int[]{1, 2, 3, 4, 5};\ny = x;\nx = new int[]{-1, 2, 5, 4, 99};\ny = new int[3];\nz = new int[0];\nint xL = x.length;\n\nString[] s = new String[6];\ns[4] = "ketchup";\ns[x[3] - x[1]] = "muffins";\n\nint[] b = {9, 10, 11};\nSystem.arraycopy(b, 0, x, 3, 2);\n```',
            "21. The final line demonstrates one way to copy information from one array to another. `System.arraycopy` takes five parameters:",
            "22. * The array to use as a source\n* Where to start in the source array\n* The array to use as a destination\n* Where to start in the destination array\n* How many items to copy",
            "23. For Python veterans, `System.arraycopy(b, 0,x, 3, 2)` is the equivalent of `x[3:5] = b[0:2]` in Python.",
            "24. An alternate approach to copying arrays would be to use a loop. `arraycopy` is usually faster than a loop, and results in more compact code. The only downside is that `arraycopy` is (arguably) harder to read. Note that Java arrays only perform bounds checking at runtime. That is, the following code compiles just fine, but will crash at runtime.",
            "25. ```java\nint[] x = {9, 10, 11, 12, 13};\nint[] y = new int[2];\nint i = 0;\nwhile (i < x.length) {\n    y[i] = x[i];\n    i += 1;\n}\n```",
            "26. Try running this code locally in a java file or in the [visualizer](https://goo.gl/YHufJ6). What is the name of the error that you encounter when it crashes? Does the name of the error make sense?",
            '27. #### 2D Arrays in Java <a href="#id-2d-arrays-in-java" id="id-2d-arrays-in-java"></a>',
            "28. What one might call a 2D array in Java is actually just an array of arrays. They follow the same rules for objects that we've already learned, but let's review them to make sure we understand how they work.",
            "29. Syntax for arrays of arrays can be a bit confusing. Consider the code `int[][] bamboozle = new int[4][]`. This creates an array of integer arrays called `bamboozle`. Specifically, this creates exactly four memory boxes, each of which can point to an array of integers (of unspecified length).",
            "30. Try running the code below line-by-lines, and see if the results match your intuition. For an interactive visualization, click [here](http://goo.gl/VS4cOK).",
            "31. ```java\nint[][] pascalsTriangle;\npascalsTriangle = new int[4][];\nint[] rowZero = pascalsTriangle[0];\n\npascalsTriangle[0] = new int[]{1};\npascalsTriangle[1] = new int[]{1, 1};\npascalsTriangle[2] = new int[]{1, 2, 1};\npascalsTriangle[3] = new int[]{1, 3, 3, 1};\nint[] rowTwo = pascalsTriangle[2];\nrowTwo[1] = -5;\n\nint[][] matrix;\nmatrix = new int[4][];\nmatrix = new int[4][4];\n\nint[][] pascalAgain = new int[][]{{1}, {1, 1},\n                                 {1, 2, 1}, {1, 3, 3, 1}};\n```",
            "32. **Exercise 2.4.1:** After running the code below, what will be the values of x\\[0]\\[0] and w\\[0]\\[0]? Check your work by clicking [here](http://goo.gl/fCZ9Dr).",
            "33. ```java\nint[][] x = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};\n\nint[][] z = new int[3][];\nz[0] = x[0];\nz[1] = x[1];\nz[2] = x[2];\nz[0][0] = -z[0][0];\n\nint[][] w = new int[3][3];\nSystem.arraycopy(x[0], 0, w[0], 0, 3);\nSystem.arraycopy(x[1], 0, w[1], 0, 3);\nSystem.arraycopy(x[2], 0, w[2], 0, 3);\nw[0][0] = -w[0][0];\n```",
            '34. #### Arrays vs. Classes <a href="#arrays-vs-classes" id="arrays-vs-classes"></a>',
            "35. Both arrays and classes can be used to organize a bunch of memory boxes. In both cases, the number of memory boxes is fixed, i.e. the length of an array cannot be changed, just as class fields cannot be added or removed.",
            "36. The key differences between memory boxes in arrays and classes:",
            "37. * Array boxes are numbered and accessed using `[]` notation, and class boxes are named and accessed using dot notation.\n* Array boxes must all be the same type. Class boxes can be different types.",
            "38. One particularly notable impact of these difference is that `[]` notation allows us to specify which index we'd like at runtime. For example, consider the code below:",
            "39. ```java\nint indexOfInterest = askUserForInteger();\nint[] x = {100, 101, 102, 103};\nint k = x[indexOfInterest];\nSystem.out.println(k);\n```",
            "40. If we run this code, we might get something like:",
            "41. ```\n$ javac arrayDemo\n$ java arrayDemo\nWhat index do you want? 2\n102\n```",
            "42. By contrast, specifying fields in a class is not something we do at runtime. For example, consider the code below:",
            '43. ```java\nString fieldOfInterest = "mass";\nPlanet p = new Planet(6e24, "earth");\ndouble mass = p[fieldOfInterest];\n```',
            "44. If we tried compiling this, we'd get a syntax error.",
            "45. ```\n$ javac classDemo\nFieldDemo.java:5: error: array required, but Planet found\n        double mass = earth[fieldOfInterest];        \n                               ^\n```",
            "46. The same problem occurs if we try to use dot notation:",
            '47. ```java\nString fieldOfInterest = "mass";\nPlanet p = new Planet(6e24, "earth");\ndouble mass = p.fieldOfInterest;\n```',
            "48. Compiling, we'd get:",
            "49. ```\n$ javac classDemo\nFieldDemo.java:5: error: cannot find symbol\n        double mass = earth.fieldOfInterest;        \n                           ^\n  symbol:   variable fieldOfInterest\n  location: variable earth of type Planet\n```",
            "50. This isn't a limitation you'll face often, but it's worth pointing out, just for the sake of good scholarship. For what it's worth, there is a way to specify desired fields at runtime called _reflection_, but it is considered very bad coding style for typical programs. You can read more about reflection [here](https://docs.oracle.com/javase/tutorial/reflect/member/fieldValues.html). **You should never use reflection in any 61B program**, and we won't discuss it in our course.",
            "51. In general, programming languages are partially designed to limit the choices of programmers to make code simpler to reason about. By restricting these sorts of features to the special Reflections API, we make typical Java programs easier to read and interpret.",
            '52. #### Appendix: Java Arrays vs. Other Languages <a href="#appendix-java-arrays-vs-other-languages" id="appendix-java-arrays-vs-other-languages"></a>',
            "53. Compared to arrays in other languages, Java arrays:",
            '54. * Have no special syntax for "slicing" (such as in Python).\n* Cannot be shrunk or expanded (such as in Ruby).\n* Do not have member methods (such as in Javascript).\n* Must contain values only of the same type (unlike Python).\n',
        ],
        "3.-references-recursion-and-lists.md": [
            "1. # 3. References, Recursion, and Lists",
            '2. ### Lists <a href="#lists" id="lists"></a>',
            "3. In Project 0, we use arrays to track the positions of N objects in space. One thing we would not have been able to easily do is change the number of objects after the simulation had begun. This is because arrays have a fixed size in Java that can never change.",
            "4. An alternate approach would have been to use a list type. You've no doubt used a list data structure at some point in the past. For example, in Python:",
            "5. ```python\nL = [3, 5, 6]\nL.append(7)\n```",
            "6. While Java does have a built-in List type, we're going to eschew using it for now. In this chapter, we'll build our own list from scratch, along the way learning some key features of Java.",
            '7. #### The Mystery of the Walrus <a href="#the-mystery-of-the-walrus" id="the-mystery-of-the-walrus"></a>',
            "8. To begin our journey, we will first ponder the profound Mystery of the Walrus.",
            "9. Try to predict what happens when we run the code below. Does the change to b affect a? Hint: If you're coming from Python, Java has the same behavior.",
            "10. ```java\nWalrus a = new Walrus(1000, 8.3);\nWalrus b;\nb = a;\nb.weight = 5;\nSystem.out.println(a);\nSystem.out.println(b);\n```",
            "11. Now try to predict what happens when we run the code below. Does the change to x affect y?",
            '12. ```java\nint x = 5;\nint y;\ny = x;\nx = 2;\nSystem.out.println("x is: " + x);\nSystem.out.println("y is: " + y);\n```',
            "13. The answer can be found [here](http://cscircles.cemc.uwaterloo.ca/java\\_visualize/#code=public+class+PollQuestions+%7B%0A+++public+static+void+main%28String%5B%5D+args%29+%7B%0A++++++Walrus+a+%3D+new+Walrus%281000,+8.3%29%3B%0A++++++Walrus+b%3B%0A++++++b+%3D+a%3B%0A++++++b.weight+%3D+5%3B%0A++++++System.out.println%28a%29%3B%0A++++++System.out.println%28b%29%3B++++++%0A%0A++++++int+x+%3D+5%3B%0A++++++int+y%3B%0A++++++y+%3D+x%3B%0A++++++x+%3D+2%3B%0A++++++System.out.println%28%22x+is%3A+%22+%2B+x%29%3B%0A++++++System.out.println%28%22y+is%3A+%22+%2B+y%29%3B++++++%0A+++%7D%0A+++%0A+++public+static+class+Walrus+%7B%0A++++++public+int+weight%3B%0A++++++public+double+tuskSize%3B%0A++++++%0A++++++public+Walrus%28int+w,+double+ts%29+%7B%0A+++++++++weight+%3D+w%3B%0A+++++++++tuskSize+%3D+ts%3B%0A++++++%7D%0A%0A++++++public+String+toString%28%29+%7B%0A+++++++++return+String.format%28%22weight%3A+%25d,+tusk+size%3A+%25.2f%22,+weight,+tuskSize%29%3B%0A++++++%7D%0A+++%7D%0A%7D\\&mode=edit).",
            "14. While subtle, the key ideas that underlie the Mystery of the Walrus will be incredibly important to the efficiency of the data structures that we'll implement in this course, and a deep understanding of this problem will also lead to safer, more reliable code.",
            '15. #### Bits <a href="#bits" id="bits"></a>',
            "16. All information in your computer is stored in _memory_ as a sequence of ones and zeros. Some examples:",
            "17. * 72 is often stored as 01001000\n* 205.75 is often stored as 01000011 01001101 11000000 00000000\n* The letter H is often stored as 01001000 (same as 72)\n* The true value is often stored as 00000001",
            "18. In this course, we won't spend much time talking about specific binary representations, e.g. why on earth 205.75 is stored as the seemingly random string of 32 bits above. Understanding specific representations is a topic of [CS61C](http://www-inst.eecs.berkeley.edu/\\~cs61c/), the followup course to 61B.",
            "19. Though we won't learn the language of binary, it's good to know that this is what is going on under the hood.",
            "20. One interesting observation is that both 72 and H are stored as 01001000. This raises the question: how does a piece of Java code know how to interpret 01001000?",
            "21. The answer is through types! For example, consider the code below:",
            "22. ```java\nchar c = 'H';\nint x = c;\nSystem.out.println(c);\nSystem.out.println(x);\n```",
            "23. If we run this code, we get:",
            "24. ```\nH\n72\n```",
            "25. In this case, both the x and c variables contain the same bits (well, almost...), but the Java interpreter treats them differently when printed.",
            "26. In Java, there are 8 primitive types: byte, short, int, long, float, double, boolean, and char. Each has different properties that we'll discuss throughout the course, with the exception of short and float, which you'll likely never use.",
            '27. #### Declaring a Variable (Simplified) <a href="#declaring-a-variable-simplified" id="declaring-a-variable-simplified"></a>',
            "28. You can think of your computer as containing a vast number of memory bits for storing information, each of which has a unique address. Many billions of such bits are available to the modern computer.",
            "29. When you declare a variable of a certain type, Java finds a contiguous block with exactly enough bits to hold a thing of that type. For example, if you declare an int, you get a block of 32 bits. If you declare a byte, you get a block of 8 bits. Each data type in Java holds a different number of bits. The exact number is not terribly important to us in this class.",
            '30. For the sake of having a convenient metaphor, we\'ll call one of these blocks a "box" of bits.',
            "31. In addition to setting aside memory, the Java interpreter also creates an entry in an internal table that maps each variable name to the location of the first bit in the box.",
            "32. For example, if you declared `int x` and `double y`, then Java might decide to use bits 352 through 384 of your computer's memory to store x, and bits 20800 through 20864 to store y. The interpreter will then record that int x starts at bit 352 and y starts at bit 20800. For example, after executing the code:",
            "33. ```java\nint x;\ndouble y;\n```",
            "34. We'd end up with boxes of size 32 and 64 respectively, as shown in the figure below:",
            "35. ![x\\_and\\_y\\_empty\\_bitwise](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/x\\_and\\_y\\_empty\\_bitwise.png)",
            "36. The Java language provides no way for you to know the location of the box, e.g. you can't somehow find out that x is in position 352. In other words, the exact memory address is below the level of abstraction accessible to us in Java. This is unlike languages like C where you can ask the language for the exact address of a piece of data. For this reason, I have omitted the addresses from the figure above.",
            '37. This feature of Java is a tradeoff! Hiding memory locations from the programmer gives you less control, which prevents you from doing certain [types of optimizations](http://www.informit.com/articles/article.aspx?p=2246428\\&seqNum=5). However, it also avoids a [large class of very tricky programming errors](http://www.informit.com/articles/article.aspx?p=2246428\\&seqNum=1). In the modern era of very low cost computing, this tradeoff is usually well worth it. As the wise Donald Knuth once said: "We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil".',
            "38. As an analogy, you do not have direct control over your heartbeat. While this restricts your ability to optimize for certain situations, it also avoids the possibility of making stupid errors like accidentally turning it off.",
            "39. Java does not write anything into the reserved box when a variable is declared. In other words, there are no default values. As a result, the Java compiler prevents you from using a variable until after the box has been filled with bits using the `=` operator. For this reason, I have avoided showing any bits in the boxes in the figure above.",
            "40. When you assign values to a memory box, it is filled with the bits you specify. For example, if we execute the lines:",
            "41. ```java\nx = -1431195969;\ny = 567213.112;\n```",
            "42. Then the memory boxes from above are filled as shown below, in what I call **box notation**.",
            "43. ![x\\_and\\_y\\_empty\\_filled.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/x\\_and\\_y\\_empty\\_filled.png)",
            "44. The top bits represent -1431195969, and the bottom bits represent 567213.112. Why these specific sequences of bits represent these two numbers is not important, and is a topic covered in CS61C. However, if you're curious, see [integer representations](https://en.wikipedia.org/wiki/Two's\\_complement) and [double representations](https://en.wikipedia.org/wiki/IEEE\\_floating\\_point) on wikipedia.",
            "45. Note: Memory allocation is actually somewhat more complicated than described here, and is a topic of CS 61C. However, this model is close enough to reality for our purposes in 61B.",
            "46. **Simplified Box Notation**",
            "47. While the box notation we used in the previous section is great for understanding approximately what's going on under the hood, it's not useful for practical purposes since we don't know how to interpret the binary bits.",
            "48. Thus, instead of writing memory box contents in binary, we'll write them in human readable symbols. We will do this throughout the rest of the course. For example, after executing:",
            "49. ```java\nint x;\ndouble y;\nx = -1431195969;\ny = 567213.112;\n```",
            "50. We can represent the program environment using what I call **simplified box notation**, shown below:",
            "51. ![x\\_and\\_y\\_simplified\\_box\\_notation.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/x\\_and\\_y\\_simplified\\_box\\_notation.png)",
            '52. #### The Golden Rule of Equals (GRoE) <a href="#the-golden-rule-of-equals-groe" id="the-golden-rule-of-equals-groe"></a>',
            "53. Now armed with simplified box notation, we can finally start to resolve the Mystery of the Walrus.",
            "54. It turns out our Mystery has a simple solution: When you write `y = x`, you are telling the Java interpreter to copy the bits from x into y. This Golden Rule of Equals (GRoE) is the root of all truth when it comes to understanding our Walrus Mystery.",
            '55. ```java\nint x = 5;\nint y;\ny = x;\nx = 2;\nSystem.out.println("x is: " + x);\nSystem.out.println("y is: " + y);\n```',
            "56. This simple idea of copying the bits is true for ANY assignment using `=` in Java. To see this in action, click [this link](http://cscircles.cemc.uwaterloo.ca/java\\_visualize/#code=public+class+PollQuestions+%7B%0A+++public+static+void+main\\(String%5B%5D+args%29+%7B%0A++++++int+x+%3D+5%3B%0A++++++int+y%3B%0A++++++y+%3D+x%3B%0A++++++x+%3D+2%3B%0A++++++System.out.println\\(%22x+is%3A+%22+%2B+x%29%3B%0A++++++System.out.println\\(%22y+is%3A+%22+%2B+y%29%3B++++++%0A+++%7D%0A%7D\\&mode=display\\&curInstr=0).",
            '57. #### Reference Types <a href="#reference-types" id="reference-types"></a>',
            "58. Above, we said that there are 8 primitive types: byte, short, int, long, float, double, boolean, char. Everything else, including arrays, is not a primitive type but rather a `reference type`.",
            "59. **Object Instantiation**",
            "60. When we _instantiate_ an Object using `new` (e.g. Dog, Walrus, Planet), Java first allocates a box for each instance variable of the class, and fills them with a default value. The constructor then usually (but not always) fills every box with some other value.",
            "61. For example, if our Walrus class is:",
            "62. ```java\npublic static class Walrus {\n    public int weight;\n    public double tuskSize;\n\n    public Walrus(int w, double ts) {\n          weight = w;\n          tuskSize = ts;\n    }\n}\n```",
            "63. And we create a Walrus using `new Walrus(1000, 8.3);`, then we end up with a Walrus consisting of two boxes of 32 and 64 bits respectively:",
            "64. ![anonymous\\_walrus.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/anonymous\\_walrus.png)",
            "65. In real implementations of the Java programming language, there is actually some additional overhead for any object, so a Walrus takes somewhat more than 96 bits. However, for our purposes, we will ignore such overhead, since we will never interact with it directly.",
            "66. The Walrus we've created is anonymous, in the sense that it has been created, but it is not stored in any variable. Let's now turn to variables that store objects.",
            "67. **Reference Variable Declaration**",
            "68. When we _declare_ a variable of any reference type (Walrus, Dog, Planet, array, etc.), Java allocates a box of 64 bits, no matter what type of object.",
            "69. At first glance, this might seem to lead to a Walrus Paradox. Our Walrus from the previous section required more than 64 bits to store. Furthermore, it may seem bizarre that no matter the type of object, we only get 64 bits to store it.",
            "70. However, this problem is easily resolved with the following piece of information: the 64 bit box contains not the data about the walrus, but instead the address of the Walrus in memory.",
            "71. As an example, suppose we call:",
            "72. ```java\nWalrus someWalrus;\nsomeWalrus = new Walrus(1000, 8.3);\n```",
            "73. The first line creates a box of 64 bits. The second line creates a new Walrus, and the address is returned by the `new` operator. These bits are then copied into the `someWalrus` box according to the GRoE.",
            "74. If we imagine our Walrus weight is stored starting at bit `5051956592385990207` of memory, and tuskSize starts at bit `5051956592385990239`, we might store `5051956592385990207` in the Walrus variable. In binary, `5051956592385990207` is represented by the 64 bits `0100011000011100001001111100000100011101110111000001111000111111`, giving us in box notation:",
            "75. ![someWalrus\\_bit\\_notation.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/someWalrus\\_bit\\_notation.png)",
            "76. We can also assign the special value `null` to a reference variable, corresponding to all zeros.",
            "77. ![someWalrus\\_bit\\_notation\\_null.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/someWalrus\\_bit\\_notation\\_null.png)",
            "78. **Box and Pointer Notation**",
            "79. Just as before, it's hard to interpret a bunch of bits inside a reference variable, so we'll create a simplified box notation for reference variable as follows:",
            "80. * If an address is all zeros, we will represent it with null.\n* A non-zero address will be represented by an **arrow** pointing at an object instantiation.",
            '81. This is also sometimes called "box and pointer" notation.',
            "82. For the examples from the previous section, we'd have:",
            "83. ![someWalrus\\_simplified\\_bit\\_notation.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/someWalrus\\_simplified\\_bit\\_notation.png)",
            "84. ![someWalrus\\_simplified\\_bit\\_notation\\_null.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/someWalrus\\_simplified\\_bit\\_notation\\_null.png)",
            "85. **Resolving the Mystery of the Walrus**",
            "86. We're now finally ready to resolve, fully and completely, the Mystery of the Walrus.",
            "87. ```java\nWalrus a = new Walrus(1000, 8.3);\nWalrus b;\nb = a;\n```",
            "88. After the first line is executed, we have:",
            "89. ![mystery\\_of\\_the\\_walrus\\_resolved\\_step1.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/mystery\\_of\\_the\\_walrus\\_resolved\\_step1.png)",
            "90. After the second line is executed, we have:",
            "91. ![mystery\\_of\\_the\\_walrus\\_resolved\\_step2.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/mystery\\_of\\_the\\_walrus\\_resolved\\_step2.png)",
            "92. Note that above, b is undefined, not null.",
            "93. According to the GRoE, the final line simply copies the bits in the `a` box into the `b` box. Or in terms of our visual metaphor, this means that b will copy exactly the arrow in a and now show an arrow pointing at the same object.",
            "94. ![mystery\\_of\\_the\\_walrus\\_resolved\\_step3.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/mystery\\_of\\_the\\_walrus\\_resolved\\_step3.png)",
            "95. And that's it. There's no more complexity than this.",
            '96. #### Parameter Passing <a href="#parameter-passing" id="parameter-passing"></a>',
            '97. When you pass parameters to a function, you are also simply copying the bits. In other words, the GRoE also applies to parameter passing. Copying the bits is usually called "pass by value". In Java, we **always** pass by value.',
            "98. For example, consider the function below:",
            "99. ```java\npublic static double average(double a, double b) {\n    return (a + b) / 2;\n}\n```",
            "100. Suppose we invoke this function as shown below:",
            "101. ```java\npublic static void main(String[] args) {\n    double x = 5.5;\n    double y = 10.5;\n    double avg = average(x, y);\n}\n```",
            "102. After executing the first two lines of this function, the main method will have two boxes labeled `x` and `y` containing the values shown below:",
            "103. ![main\\_x\\_y.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/main\\_x\\_y.png)",
            '104. When the function is invoked, the `average` function has its **own** scope with two new boxes labeled as `a` and `b`, and the bits are simply _copied_ in. This copying of bits is what we refer to when we say "pass by value".',
            "105. ![average\\_a\\_b.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig21/average\\_a\\_b.png)",
            "106. If the `average` function were to change `a`, then `x` in main would be unchanged, since the GRoE tells us that we'd simply be filling in the box labeled `a` with new bits.",
            "107. **Test Your Understanding**",
            "108. **Exercise 2.1.1**: Suppose we have the code below:",
            "109. ```java\npublic class PassByValueFigure {\n    public static void main(String[] args) {\n           Walrus walrus = new Walrus(3500, 10.5);\n           int x = 9;\n\n           doStuff(walrus, x);\n           System.out.println(walrus);\n           System.out.println(x);\n    }\n\n    public static void doStuff(Walrus W, int x) {\n           W.weight = W.weight - 100;\n           x = x - 5;\n    }\n}\n```",
            "110. Does the call to `doStuff` have an effect on walrus and/or x? Hint: We only need to know the GRoE to solve this problem.",
            '111. #### Instantiation of Arrays <a href="#instantiation-of-arrays" id="instantiation-of-arrays"></a>',
            "112. As mentioned above, variables that store arrays are reference variables just like any other. As an example, consider the declarations below:",
            "113. ```java\nint[] x;\nPlanet[] planets;\n```",
            "114. Both of these declarations create memory boxes of 64 bits. `x` can only hold the address of an `int` array, and `planets` can only hold the address of a `Planet` array.",
            "115. Instantiating an array is very similar to instantiating an object. For example, if we create an integer array of size 5 as shown below:",
            "116. ```\nx = new int[]{0, 1, 2, 95, 4};\n```",
            "117. Then the `new` keyword creates 5 boxes of 32 bits each and returns the address of the overall object for assignment to x.",
            "118. Objects can be lost if you lose the bits corresponding to the address. For example if the only copy of the address of a particular Walrus is stored in `x`, then `x = null` will cause you to permanently lose this Walrus. This isn't necessarily a bad thing, since you'll often decide you're done with an object, and thus it's safe to simply throw away the reference. We'll see this when we build lists later in this chapter.",
            "119. **The Law of the Broken Futon**",
            "120. You might ask yourself why we spent so much time and space covering what seems like a triviality. This is probably especially true if you have prior Java experience. The reason is that it is very easy for a student to have a half-cocked understanding of this issue, allowing them to write code, but without true comprehension of what's going on.",
            "121. While this might be fine in the short term, in the long term, doing problems without full understanding may doom you to failure later down the line. There's a blog post about this so-called [Law of the Broken Futon](https://mathwithbaddrawings.com/2015/04/08/the-math-ceiling-wheres-your-cognitive-breaking-point/) that you might find interesting.",
            '122. #### IntLists <a href="#intlists" id="intlists"></a>',
            "123. Now that we've truly understood the Mystery of the Walrus, we're ready to build our own list class.",
            "124. It turns out that a very basic list is trivial to implement, as shown below:",
            "125. ```java\npublic class IntList {\n    public int first;\n    public IntList rest;        \n\n    public IntList(int f, IntList r) {\n        first = f;\n        rest = r;\n    }\n}\n```",
            '126. You may remember something like this from 61a called a "Linked List".',
            "127. Such a list is ugly to use. For example, if we want to make a list of the numbers 5, 10, and 15, we can either do:",
            "128. ```java\nIntList L = new IntList(5, null);\nL.rest = new IntList(10, null);\nL.rest.rest = new IntList(15, null);\n```",
            "129. Alternately, we could build our list backwards, yielding slightly nicer but harder to understand code:",
            "130. ```java\nIntList L = new IntList(15, null);\nL = new IntList(10, L);\nL = new IntList(5, L);\n```",
            "131. While you could in principle use the IntList to store any list of integers, the resulting code would be rather ugly and prone to errors. We'll adopt the usual object oriented programming strategy of adding helper methods to our class to perform basic tasks.",
            "132. **size and iterativeSize**",
            "133. We'd like to add a method `size` to the `IntList` class so that if you call `L.size()`, you get back the number of items in `L`.",
            "134. Consider writing a `size` and `iterativeSize` method before reading the rest of this chapter. `size` should use recursion, and `iterativeSize` should not. You'll probably learn more by trying on your own before seeing how I do it. The two videos provide a live demonstration of how one might implement these methods.",
            "135. My `size` method is as shown below:",
            "136. ```java\n/** Return the size of the list using... recursion! */\npublic int size() {\n    if (rest == null) {\n        return 1;\n    }\n    return 1 + this.rest.size();\n}\n```",
            "137. The key thing to remember about recursive code is that you need a base case. In this situation, the most reasonable base case is that rest is `null`, which results in a size 1 list.",
            "138. Exercise: You might wonder why we don't do something like `if (this == null) return 0;`. Why wouldn't this work?",
            "139. Answer: Think about what happens when you call size. You are calling it on an object, for example L.size(). If L were null, then you would get a NullPointer error!",
            '140. My `iterativeSize` method is as shown below. I recommend that when you write iterative data structure code that you use the name `p` to remind yourself that the variable is holding a pointer. You need that pointer because you can\'t reassign "this" in Java. The followups in [this Stack Overflow Post](https://stackoverflow.com/questions/23021377/reassign-this-in-java-class) offer a brief explanation as to why.',
            "141. ```java\n/** Return the size of the list using no recursion! */\npublic int iterativeSize() {\n    IntList p = this;\n    int totalSize = 0;\n    while (p != null) {\n        totalSize += 1;\n        p = p.rest;\n    }\n    return totalSize;\n}\n```",
            "142. **get**",
            "143. While the `size` method lets us get the size of a list, we have no easy way of getting the ith element of the list.",
            "144. Exercise: Write a method `get(int i)` that returns the ith item of the list. For example, if `L` is 5 -> 10 -> 15, then `L.get(0)` should return 5, `L.get(1)` should return 10, and `L.get(2)` should return 15. It doesn't matter how your code behaves for invalid `i`, either too big or too small.",
            "145. For a solution, see the lecture video above or the lectureCode repository.",
            "146. Note that the method we've written takes linear time! That is, if you have a list that is 1,000,000 items long, then getting the last item is going to take much longer than it would if we had a small list. We'll see an alternate way to implement a list that will avoid this problem in a future lecture.\n",
        ],
        "33.-software-engineering-iii/33.4-summary.md": [
            "1. # 33.4 Summary",
            "2. In this fun lecture, we steered a bit away from core CS61B exam material and discussed a high-level overview of the impacts that students can have on society and on their own lives using the skills obtained in CS 61B and beyond.",
            "3. We covered specific societal impacts in the realm of economics within the context of industry, and social impacts with how humans in our global civilization interact with one another using the digital products that we can create using our coding skills. Most importantly, we delved into the potential _harms_ to society that our coding skills could have \u2013 from the psychological foundations that could be altered on our youth to the reconfiguration of politics and governmental leaders are elected.&#x20;",
            "4. With great power comes great responsibility, and it is evermore vital that we consider the deep, all-encompassing question of ethics in creating software for humanity.&#x20;",
            "5. With the demand for software engineers and the financial impact that creating software solutions to the worldwide marketplace has, the value added to you as a worker in our society is immense & very fruitful. However, it is important to consider factors beyond just work to create a multifaceted thing called your life. We only work so many hours in the weeks in our lives\u2013 so it is incredibly important to ensure that work (and learning CS to become a software engineer, etc.) is just a subpart of what we consider our entire life. Have fun on the journey!",
            "6. ",
        ],
        "33.-software-engineering-iii/33.3-your-life.md": [
            "1. # 33.3 Your Life",
            "2. ### The Power of Software",
            "3. Unlike other engineering disciplines, software is effectively unconstrained by the laws of physics.",
            "4. * Programming is an act of almost pure creativity!",
            "5. The greatest limitation we face in building systems is being able to understand what we\u2019re building!",
            "6. ### The Limiting Reagent",
            "7. You are a rare commodity.",
            "8. Revenue per Employee:",
            "9. ![](<../.gitbook/assets/image (48).png>)",
            "10. Profits per Employee:",
            "11. ![](<../.gitbook/assets/image (6).png>)",
            "12. Sources: [Link](https://www.wolframalpha.com/input/?i=revenue+per+employee+facebook+vs+alphabet+vs+amazon+vs+microsoft+vs+apple), [Link](https://www.wolframalpha.com/input/?i=profits+per+employee+facebook+vs+alphabet+vs+amazon+vs+microsoft+vs+apple)",
            "13. The skills you are building will be in high demand from companies, non-profits, government agencies, educational institutions, and more.",
            "14. * The choice of how to spend your career is yours.",
            "15. ### Some CS61B Data (Spring 2019 vs Spring 2021)",
            "16. Spring 2021:",
            '17. <figure><img src="https://lh6.googleusercontent.com/F6YHSAwZH0DsakcD41Da2hdSJr3RSpbaanpG5h-xFUOHBU8-7hu7HOAf3xulNAFPjoqAyavr1NbCwgOJ3LSDblur0PMn-ZTFehaFpyKT8kL1e4yMQIZ3cLPVJvREPzDEyOx3I1rx-YG8wMS7LIbDxYSYhw=s2048" alt=""><figcaption></figcaption></figure>',
            "18. Spring 2019:",
            '19. <figure><img src="https://lh4.googleusercontent.com/6S3iPyMdD2mOuqWRBsN97v4iNq4UkRo_nzmUHSZfadjEEk2CVBh1vsdANObMBMIw3JDC7ksCHb7_NFJVFUin6XS1AtNagfUOLDevPBxs-mMKEbY0RtBtQWnbDU8F6f8Usela_tO1FLt9yFqtOM0NG8uc3g=s2048" alt=""><figcaption></figcaption></figure>',
            "20. ### Steering the Course",
            "21. Quite a lot of you will likely end up working at some sort of technology company at some point in your life.",
            "22. There\u2019s nothing (IMO) wrong with working at profit driven tech companies.",
            "23. * Please do realize that even as a rank and file employee, you have the power to effect change, particularly if you are paid in stock (because then you are a partial owner).",
            "24. Let\u2019s see some examples.",
            "25. #### Example 1: Google and the Censored Chinese Search Engine",
            "26. From 2006 to 2010, Google operated a censored version of Google in China.",
            "27. * Withdrew in 2010 in response to cyber attacks by the Chinese government on users of Google\u2019s services: \u201cThese attacks and the surveillance they have uncovered--combined with the attempts over the past year to further limit free speech on the web--have led us to conclude that we should review the feasibility of our business operations in China. We have decided we are no longer willing to continue censoring our results on Google.cn.\u201d",
            "28. In Oct 2018, leaked documents published in [The Intercept](https://theintercept.com/2018/08/01/google-china-search-engine-censorship/) revealed a secret project by Google to re-enter the Chinese market.",
            "29. * This new search engine would be censored, and also allow government officials to hypothetically track Chinese residents making undesired queries.",
            "30. Some employee responses:",
            "31. * After the leaks, access within the company to documents related to the project were curtailed: \u201cEveryone\u2019s access to documents got turned off, and is being turned on \\[on a] document-by-document basis,\u201d said one source. \u201cThere\u2019s been total radio silence from leadership, which is making a lot of people upset and scared. \u2026 Our internal meme site and Google Plus are full of talk, and people are a.n.g.r.y.\u201d \\[[Link](https://theintercept.com/2018/08/03/google-search-engine-china-censorship-backlash/)]\n* Over 1400 employees signed a letter demanding more transparency, and at least five employees resigned in protest \\[[Link](https://theintercept.com/2018/09/13/google-china-search-engine-employee-resigns/)].",
            "32. Jack Poulson (a senior scientist who resigned): ([Link](https://theintercept.com/2018/09/13/google-china-search-engine-employee-resigns/))",
            "33. * \u201cThere are serious worldwide repercussions to this... what are Google\u2019s ethical red lines? We already wrote some down, but now we seem to be crossing those.\u201d\n* Poulson said that he \u201cvery much agree\\[s] with the case Sergey made in 2010. That\u2019s the company I joined, the one that was making that statement.\u201d If the anti-censorship stance is shifting, he said, then he could no longer \u201cbe complicit as a shareholder and citizen of the company.\u201d",
            "34. Ultimately, in response to employee outcry, the project was canceled \\[[Link](https://theintercept.com/2018/12/17/google-china-censored-search-engine-2/)].",
            "35. #### Example 2: Amazon and Climate Change \\[[Link](https://www.nytimes.com/2019/04/10/technology/amazon-climate-change-letter.html)]",
            "36. In 2020, 6,772+ employees at Amazon publicly signed [a letter](https://medium.com/@amazonemployeesclimatejustice/public-letter-to-jeff-bezos-and-the-amazon-board-of-directors-82a8405f5e38) demanding better responses to climate change by Amazon.",
            "37. * Specifically, they want the company to adopt a shareholder resolution ([link](https://www.scribd.com/document/405804472/Amazon-Climate-Plan-Shareholder-Resolution)) to study how the company will adapt to and reduce its contribution to climate change.\n* \u201cAmazon is not a mere victim of climate change\u2014its operations contribute significantly to the problem\u2026 Coal still powers Amazon data centers. Diesel, gasoline, and jet fuel still power package delivery.\u201d\n* \u201cAmazon has the resources and scale to spark the world\u2019s imagination and redefine what is possible and necessary to address the climate crisis.\u201d",
            "38. \u201cLike other shareholders, they can file a resolution urging a particular corporate change that investors vote on at a company\u2019s annual meeting.\u201d",
            "39. ### Time",
            "40. You get some number of decades on the planet.",
            "41. Each week is 168 hours.",
            "42. * \\~40 hours of work.\n* \\~56 hours of sleep. (Anecdotally, most of you will have much more free time after college than you do now)\n* \\~72 hours for everything else.",
            "43. Spend your time wisely, in both your career and personal life.\n",
        ],
        "33.-software-engineering-iii/33.2-the-ledger-of-harms.md": [
            "1. # 33.2 The Ledger of Harms",
            "2. The concerns that we talked about from the previous section are not left unnoticed by leaders of the industry. In fact, the opposite is true: many of them has openly expressed their concerns regarding these not-so-novel technologies that permeate our lives now.",
            "3. * \u201cI think we have created tools that are ripping apart the social fabric of how society works.\u201d - [Chamath Palihapitiya](https://www.theverge.com/2017/12/11/16761016/former-facebook-exec-ripping-apart-society) (early executive at Facebook)\n* \u201cGod only knows what it\\[Facebook]\u2019s doing to our children\u2019s brains.\u201d - [Sean Parker](https://www.theverge.com/2017/11/9/16627724/sean-parker-facebook-childrens-brains-feedback-loop) (the Napster guy)\n* \u201cThe technologies we were most excited about 10 years ago are now implicated in just about every catastrophe of the day.\u201d - [Farhad Manjoo](https://www.nytimes.com/2017/10/11/insider/tech-column-dread.html) (technology journalist)\n* \u201cThese are our lives. These are our precious, finite, mortal little lives. The idea that we are spending them distracted, not accomplishing the thing that we\u2019re trying to do, is just painful. It\u2019s crazy.\u201d - [Justin Rosenstein](https://www.theverge.com/2018/3/28/17172404/justin-rosenstein-asana-social-media-facebook-timeline-gantt), creator of the Like button and founder of Asana\n* \u201cFacebook appeals to your lizard brain \u2014 primarily fear and anger,\u201d he said. \u201cAnd with smartphones, they\u2019ve got you for every waking moment.\u201d\u201d -[Roger McNamee](https://www.nytimes.com/2018/02/04/technology/early-facebook-google-employees-fight-tech.html) (early investor in many tech companies, interesting guy)",
            "4. ## The Ledger of Harms",
            "5. The problems have long been identified. What about the solution?&#x20;",
            "6. The [Center for Humane Technology](https://humanetech.com/) was founded by current and former tech workers to raise awareness and try to combat harmful side effects of internet tech platforms----to develop solutions for these problems.&#x20;",
            "7. In 2018, they created a [Ledger of Harms](https://ledger.humanetech.com/) to \u201ccollect those negative impacts of social media and mobile tech that do not show up on the balance sheets of companies, but on the balance sheet of society.\u201d&#x20;",
            "8. The Ledger encompasses the following categories:&#x20;",
            "9. * The Next Generations: Developmental delays, suicide, physical/mental/social changes.\n* Making Sense of the World: Misinformation, conspiracies, fake news.\n* Attention and Cognition: Loss of ability to focus without distraction.\n* Physical and Mental Health: Stress, loneliness, addiction, risky behavior.\n* Social Relationships: Less empathy, more confusion and misinterpretation.\n* Politics and Elections: Propaganda, distorted dialogue, disrupted democratic processes.\n* Systemic Oppression: Amplification of discrimination.\n* Do Unto Others: Tech employees limit tech usage in their own homes.",
            "10. For each category, they provide research and citations for potential harms.",
            "11. The rest of this section will list quotes from some categories to read through.",
            "12. ### Making Sense of the World",
            "13. * \u201c64% of all extremist group joins are due to our recommendation tools...our recommendation systems grow the problem\u201d, noted an internal Facebook presentation in 2016. Yet repeated attempts to counteract this have been repeatedly ignored, diluted, or deliberately shut down by senior Facebook officers, according to a 2020 Wall Street Journal investigation.\u201d\n* \u201cFake news spreads six times faster than true news. According to researchers, this is because fake news grabs our attention more than authentic information: fake news items usually have a higher emotional content and contain unexpected information which inevitably means that they will be shared and reposted more often.\u201d\n* \u201c45% of tweets about coronavirus are from bots spreading fake information, according to research from Carnegie Mellon University.\u201d\n* \u201c2 minutes of exposure to a conspiracy theory video reduces people\u2019s pro-social attitudes (such as their willingness to help others), as well as reducing their belief in established scientific facts.\u201d",
            "14. ### Attention",
            "15. * \u201cThe presence of a smartphone, even when off, can reduce cognitive capacity by taxing the attentional resources that reside at the core of both working memory capacity and fluid intelligence.\\*\\*\\*\\*\\*\\*\u201d\n* \u201c72% of teens and 48% of parents feel the need to immediately respond to texts, social-networking messages, and other notifications.\u201d\n* \u201c1 hour per day is the amount of time most Americans spend dealing with distractions and then getting focused and back on track each day, which comes to a grand total of 5 full weeks in a year.\u201d",
            "16. ### Mental Health",
            "17. * \u201c30% of 18-44 year olds feel anxious if they haven\u2019t checked Facebook in the last 2 hours \\[in 2020].\u201d&#x20;\n* \u201c1 month away from Facebook leads to a significant improvement in emotional well-being. In an experimental study of over 1,600 American adults (who normally used Facebook for up to an hour each day), deactivating Facebook accounts led to a significant increase in emotional well-being (including a reduction in loneliness and an increase in happiness), as well as a significant reduction in political polarization.\u201d\n* \u201cIn just 3 years, there has been a quadrupling in the number of plastic surgeons with patients undergoing cosmetic surgery for the sake of looking good on social media (from 13% in 2016 to 55% in 2019).\u201c",
            "18. ### Relationships",
            "19. * \u201cEven the mere presence of smartphone can disrupt the connection between two people, having negative effects on closeness, connection, and conversation quality.\u201d\n* \u201cPeople overestimate their ability to correctly interpret sarcasm, humor, or sincerity over text communication, and this means people tend to believe they can communicate over e-mail more effectively than they actually can.\u201d\n* \u201c50% of Americans report that their partner is often or sometimes distracted by their devices when they are trying to talk to them.\n* \u201c89% of cellphone users admit to using their phones during their last social gathering.\u201d",
            "20. ### Politics and Democracy",
            '21. * \u201cMore fake political headlines were shared on Facebook than real ones during the last 3 months of the 2016 US elections.\u201d\n* \u201cExposure to a fake political news story can rewire your memories: in a study, where over 3,000 voters were shown fake stories, many voters later not only \u201cremembered\u201d the fake stories as if they were real events but also "remembered" additional, rich details of how and when the events took place.\u201d\n* \u201cAnalyzing over 2 million recommendations and 72 million comments on YouTube in 2019, researchers demonstrated that viewers consistently moved from watching moderate to extremist videos;\u201d \\[Josh Note: This is much much better now. Good job youtube!]',
            "22. ### Systemic Oppression",
            '23. * \u201cRussia\'s IRA spread false information designed to create outrage about Black Lives Matter and deepen social division in the US. Research indicates that one of the IRA\'s major strategies was to use social media platforms to target conservative groups who supported the police or veterans and specifically feed them misinformation about BLM.\u201d\n* \u201cWith over 800 million users, TikTok promotes itself as a place for self-expression and unrestricted creativity, yet its internal documents reveal a policy of downgrading content from users who do not fit normative ideals of gender, race, class, sexuality, or able-bodiedness, with moderators urged to censor users with "abnormal body shape", "too many wrinkles", or whose environment shows signs of poverty such as "cracks in the wall" or "old decorations".\u201d',
            "24. ### Children",
            "25. * \u201c58 minutes per day is the average amount of time 2-4 years old spend on mobile devices.\u201c\n* \u201cIn a longitudinal study tracking over 200 children from the age of 2 years to 5 years old, children with higher levels of screen time showed greater delays in development across a range of important measures, including language, problem-solving, and social interaction. Analyses indicated that the level of screen time was significantly linked to the specific level of developmental delay 12 -14 months later. \u201c\\\n",
        ],
        "33.-software-engineering-iii/33.1-candy-crush-snapchat-and-friends.md":
            [
                "1. # 33.1 Candy Crush, SnapChat, and Friends",
                "2. Transitioning away the theoretical analysis of Software Engineering and complexity management, this last section of the Software Engineering series will discuss more on how software has and continue to reshape society in our lifetime.",
                "3. ## A Case Study Series",
                "4. ### Case Study 1: Candy Crush",
                "5. The mobile game Candy Crush tracks the number of days you have played in a row. Specifically with the following features:",
                "6. * Every consecutive day gets you a reward.\n* Progress tracking features:&#x20;\n  * Progress indicator\n  * Up to 2 hours worth of a special item for that day that makes the game more fun.",
                "7. ![](<../.gitbook/assets/Screen Shot 2023-04-10 at 10.22.27 PM.png>)\\",
                "8. \nMore importantly: if you miss a day, the counter resets.",
                "9. #### Why does this feature exist in Candy Crush?",
                "10. Specifically, establishing a progress tracker tied with a reward system that punishes you instantly a day is missed? Well, this feature clearly encourages you to engage with the app every day.&#x20;",
                "11. ### Case Study 2: Snapchat",
                '12. Despite being a slightly outdated example (as people have collectively voted it to be "out of style", unlike Taylor Swift), Snapchat incorporates a very similar feature as Candy Crush to keep users constantly engaged, and stay engaged every day.',
                '13. Specifically, its "streak" feature has kept many people hooked for a very long time, as seeing the number "509" just makes you can\'t help but send another random shot of at a random angle that is so blurry with nothing distinguishable but the thick, red "S" sliced across the screen, to a random person from your high school whom you never talk to (True story).',
                "14. ![](<../.gitbook/assets/Screen Shot 2023-04-10 at 10.28.06 PM.png>)",
                "15. Classic.",
                "16. ### Case Study 3: Friends...?",
                "17. What are some other engagement generating features? Big, social media tech companies definitely give solid answers.",
                "18. Based on discussions in class, some of such features include:",
                "19. * LinkedIn: How many searches you appeared in.\n* Notifications in general.\n* Stories induce you to see them all.\n* Infinite scroll. (The most evil)\n* Recommendation algorithm on youtube and especially TikTok.\n* Playstation trophies.",
                "20. Then, a natural question to ask here is, **what are some of the positive and negative impacts of these technologies or features on our lives?**",
                "21. ## Impacts: Net Positive?",
                "22. Every coin has a flip side, and so do the infamous, addictive technologies.&#x20;",
                "23. Going back to the social media case, for example. Some _negative_ impacts of certain social media features that people have discussed include:",
                "24. * Addiction&#x20;\n* Instead of having conversation with people, you fulfill your need to socialize with junk food socialization.\n* Less face to face interaction.\n* Toxic comparisons.\n* Opens you up to manipulation.\n* Fear of missing out (FOMO).\n* Disinformation spreads widely.",
                "25. However, there are also some _positive_ aspects of this unique, modern champagne problem:",
                "26. * Socialize, connection with humans in a way that overcome geographical and sometimes even cultural constraints.&#x20;\n* Stay up to date with what\u2019s going on.\n* Help you stay educated on important topics and issues.&#x20;\n* See a wide range of diverse perspectives AND cute dogs.",
                "27. #### Example: Is TikTok a net positive to the world?",
                "28. According to a free, open discussion in lecture, here are some general thoughts:&#x20;",
                "29. * Yes: Wholesome videos! (some)\n* No: Would have to run some numbers, but the time people spend on TikTok could be spent on something more useful.\n* Yes: People can create information, can spread information more easily that is counter to existing power structures.\n* No: People can spread misinformation / inaccurate information.&#x20;\n* No: Attention spans are getting shorter.\n* No: Company collects massive amounts of information, bad privacy.\n* BIG YES: TikTok creates lot of jobs for people graduating from Berkeley with CS degrees (RIP Meta, FTX, Twitter\u2026).",
                "30. Whether or not these platforms or technology bring a net positive impact on the world is and will continue to be up for debate. The important message here is the importance to reflect on the social impacts of these technology, especially through a dichotomous lens.&#x20;",
                "31. ",
            ],
        "33.-software-engineering-iii/README.md": [
            "1. ---\ndescription: By William Lee and Teresa Luo\n---",
            "2. # 33. Software Engineering III",
            "3. ",
        ],
        "33.-software-engineering-iii/33.5-exercises.md": [
            "1. # 33.5 Exercises",
            "2. Note: there are no exercises for this chapter.",
            "3. For the final exam, there is no need to specifically study this lecture. Any questions relevant to software engineering will be based more on experience with projects than specific terminology and lecture material.\n",
        ],
        "27.-software-engineering-i/27.4-real-world-examples.md": [
            "1. # 27.4 Real World Examples",
            "2. For this section, it's _highly_ recommended to watch the lecture video sections. What follows are summaries of the content; following the visuals are more insightful.",
            "3. ### Retool",
            "4. Building a system to implement many different kinds of things in a repeatable manner lends itself to designing general specifications for information, and then generating pages as needed.",
            "5. This concept extends towards testing - particularly important as the real world has no autograder and no definition of correctness beyond tests that you write.",
            "6. ### Commit History Traversal",
            "7. Using what you already know in 61B - representing graphs, doing a BFS - you can solve real-world industry problems! This was an example of finding what deployments contain a certain change, by adjusting how we might represent a graph and then doing a BFS while choosing specific data structures for runtime and ease of use.\n",
        ],
        "27.-software-engineering-i/27.2-complexity.md": [
            "1. # 27.2 Complexity",
            "2. ### Restrictions of Engineering",
            "3. In other engineering disciplines, we are subject to the laws of nature. Objects have limits on how fast they can move, on how dense they can be, on how much of it there is.",
            "4. * Chemical engineers worry about temperature\n* Material scientists worry about how brittle material is\n* Civil engineers worry about the strength of concrete",
            "5. However, in computer science, we've solved most of these constraints already - the sum power of Apollo missions to get us to the moon is less than the computing power of your phone.",
            "6. ### The Power of Software",
            "7. Computers have evolved over time from being large calculators to fine-tuned machines to being multi-purpose and powerful. Video games, for example, used to be customized for the limitations of operating systems but now can be built in frameworks and abstractions.",
            "8. From this, the limitation is no longer the limit of computing power; it is from the ways that we plan and design what we build. Further:",
            "9. * An individual programmer is no longer able to effectively manage the entire software system for a large project\n  * Spotify, for example, [has over a billion lines of code and 60 million used in production](https://engineering.atspotify.com/2023/04/spotifys-shift-to-a-fleet-first-mindset-part-1/)\n* Any one programmer should only need to understand a fraction of the codebase",
            "10. ### A Definition of Complexity",
            "11. \u201c_Anything related to the structure of a software system that makes it hard to understand and modify it_\u201d - John Ousterhout, \u201cA Philosophy of Software Design\u201d",
            "12. As programs have more features and functionality, their complexity increases exponentially. Consider Spotify adding a queue feature; it has to work, but it also needs to work with everything already implemented such as play/pause, search, skip, etc.&#x20;",
            "13. Complex systems are not a goal; our goal is to keep software simple. Complex systems:",
            "14. * Take longer to understand how code works\n* Are more difficult to fix bugs with confidence\n* Harder to find what needs to change\n  * Unknown unknowns: unclear what needs to be known to make modifications\n  * Very common in large codebases",
            "15. ### Managing Complexity",
            "16. There are two kinds of complexity:",
            "17. * Unavoidable (Essential) Complexity\n  * To implement certain features, that feature carries some level of inherent complexity with it\n* Avoidable Complexity\n  * Complexity that we can address with our choices",
            "18. In response to avoidable complexity, we can:",
            "19. * Make code simpler and more obvious\n  * Using sentinel nodes in Project 1 made life significantly easier to avoid dealing with edge cases\n* Modules as a means of abstraction: the ability to use a piece without understanding how it works based on some specification\n  * Interfaces are an example - HashMap, BSTMap from lab are both Maps and can be used with `get` and `put` for some key-value pairs without understanding the underlying implementation",
            "20. ",
        ],
        "27.-software-engineering-i/27.5-summary-exercises.md": [
            "1. # 27.5 Summary, Exercises",
            "2. ### Summary",
            "3. Good code is more than just working code.",
            "4. Code (and complexity) scales with functionality.",
            "5. Practice good design principles in your classes!",
            "6. The real world is ambiguous; you must define the problem and select the solution given tradeoffs of options.",
            "7. ### Exercises",
            "8. Note that since this chapter is more about design principles than actual content, the exercises merely check factual understanding of lecture material. We encourage you to reflect on the software engineering principles discussed as you work on your own projects.",
            "9. 1. What are two ways to manage complexity?\n2. What is the difference between strategic and tactical programming? Which is better for managing complexity?",
            "10. <details>",
            "11. <summary>Problem 1</summary>",
            "12. **Make code simpler and more obvious**. Eliminate special cases and avoid repetition. Make code as general and parsimonious as possible.",
            "13. **Encapsulate code into modules.** Every module should have a specific purpose. Programmers and users can subsequently use other modules in their design, without having to understand how these modules work.",
            "14. </details>",
            "15. <details>",
            "16. <summary>Problem 2</summary>",
            "17. Tactical programming focuses on getting something to work, instead of focusing on overall design.&#x20;",
            "18. In contrast, strategic programming involves planning ahead and selecting from multiple possible implmentations for the cleanest possible solution. Strategic programming requires thinking about possible future changes and emphasizing code quality.",
            "19. For managing complexity, strategic programming is more effective. Tactical programming introduces complexity with small fixes and patches over time, whereas strategic programming aims to eliminate these complexities by changing the underlying design.",
            "20. </details>\n",
        ],
        "27.-software-engineering-i/README.md": [
            "1. ---\ndescription: By Aniruth Narayanan\n---",
            "2. # 27. Software Engineering I",
            "3. Prior to the Fall 2023 semester, Professor Hug had a lecture about software code complexity.",
            "4. For the Fall 2023 semester, this lecture was adjusted to have a similar form of content with different examples and real-world experiences that I (Aniruth) have been through on my gap year from Berkeley.",
            "5. The content in this GitBook has been adjusted to the newest information. Both video recordings are linked below.",
            '6. {% embed url="https://www.youtube.com/watch?ab_channel=UCBerkeleyCS61B&v=JfATr-BBPY0" %}\nFull Lecture on Software Engineering I by Aniruth Narayanan, Fall 2023\n{% endembed %}',
            '7. {% embed url="https://docs.google.com/presentation/d/1ZQ35zGrbnMCIk2kr37bKyxc4RMvRBFrvZd1me2Av2hQ/edit#slide=id.g28d08577524_0_0" %}\nLecture Slides for Software Engineering I, Fall 2023\n{% endembed %}',
            '8. {% embed url="https://youtu.be/WT-sBbm6rsw" %}\nFull Lecture on Software Engineering I by Professor Hug, Fall 2022\n{% endembed %}\n',
        ],
        "27.-software-engineering-i/27.3-strategic-vs-tactical-programming.md":
            [
                "1. # 27.3 Strategic vs Tactical Programming",
                "2. ### Tactical Programming",
                "3. The goal is to get something working quickly, often using workarounds. Consider code that has many if statements to handle many separate cases to pass autograder tests that is challenging to update and explain.",
                "4. Prototypes, proof-of-concepts often leverage tactical programming, to show that something could theoretically work.",
                "5. However:",
                "6. * There\u2019s no time spent on overall design\n* Code is complicated\n* Refactoring takes time and potentially means restarting\n  * If you didn't plan for Project 2 runtime requirements, you would have to redo the constructor and the entire project\n* Proof of concepts are sometimes deployed in the real world due to lack of time",
                "7. ### Strategic Programming",
                "8. The goal is to write code that works elegantly - at the cost of planning time, to reduce coding time. This emphasizes long term strategy.",
                "9. Code should be:",
                "10. * Maintainable to fix bugs\n* Simple to understand\n* Future-proof to add new functionality\n  * 61B projects have deadlines; afterwards, you can throw it away",
                "11. If the strategy is insufficient, go back to the drawing board before continuing work.",
                "12. Helper method strategy is key to leverage throughout projects, especially when we have written comprehensive tests to ensure that these methods are correct.\n",
            ],
        "27.-software-engineering-i/27.1-introduction-to-software-engineering.md":
            [
                "1. # 27.1 Introduction to Software Engineering",
                "2. ### Scale",
                "3. CS 61A is an introductory course that focuses on the correctness of a program. CS 61B, however, focuses on _engineering_ software projects. These projects are larger but now require decisions between valid options by considering tradeoffs. The choice between using a LinkedList and an ArrayList, for example, is a tradeoff of runtime and implementation decisions.",
                "4. Working on smaller scale projects isn't the same as larger scale projects that place more emphasis on design. These tasks are often ambiguous. In the real world, there are no specs, no hints to guide you through your project; there's also no end date where you can submit your code and be done with it.",
                "5. This lecture features some light theory and then some real-world examples to illustrate these concepts. Project 3 (Build Your Own World) will allow you to embrace the challenge of large scale projects yourself.",
                "6. ### Further Reading",
                "7. Please read **A Philosophy of Software Design Paperback by John Ousterhout** if you are very interested in this topic.\n",
            ],
        "15.-asymptotics-ii/15.2-recursion.md": [
            "1. ---\ndescription: Here we go again...\n---",
            "2. # 15.2 Recursion",
            "3. Now that we've done a couple of nested for loops, let's take a look at our favorite problem: recursion.&#x20;",
            '4. {% embed url="https://youtu.be/Ht6ySSoC0FM" %}',
            "5. Consider the recursive function `f3` below:",
            "6. ```java\npublic static int f3(int n) {\n   if (n <= 1) \n      return 1;\n   return f3(n-1) + f3(n-1);\n}\n```",
            "7. #### What does this function do?",
            "8. Let's think of an example of calling `f3(4)`:",
            "9. * The first call will return  `f3(4-1) + f3(4-1)`&#x20;\n* Each `f3(3-1)` call will branch out to `f3(2-1) + f3(2-1)`\n* Then for each  `f3(2-1)` call, the condition `if (n <= 1)` will be true, which will return 1.\n* What we observe at the end is that 1 will be returned 8 times, meaning we have `f3(2-1)` summed 8 times.\n* Therefore,`f3(4)`will return 8.",
            "10. We can visualize this as a tree, where each level represents a recursive call and each node value represents the argument to the function :",
            '11. <figure><img src="../.gitbook/assets/image (76).png" alt=""><figcaption><p>Visualization of f3\'s recursive calls</p></figcaption></figure>',
            "12. You can do a couple more examples, and see that this function returns $$2^N-1$$\u200b . Visualizing the recursive calls is extremely useful for getting a sense of what the function is doing, and we will discuss a few methods of determining runtime in recursive functions.",
            "13. ### Method 1: Intuition",
            "14. Based on the visualization below, we can notice that every time we add one to `n` we double the amount of work that has to be done:",
            '15. <figure><img src="../.gitbook/assets/image (32).png" alt=""><figcaption></figcaption></figure>',
            "16. Then, adding one to `n` N times means doubling the amount of work N times, which results in the intuitive answer for runtime to be $$2^N$$.",
            "17. ### Method 2: Algebra",
            "18. Another way to approach this problem is to count the number of calls to `f3` involved. Utilizing the same tree visualization above, we can see that the number of calls to f3 at each recursive level is equivalent to _the number of nodes at each recursive level_ of the tree. For instance, the number of calls we made to `f3` at the top level is 1, at the second level is 2, at the third level is 4, etc.&#x20;",
            "19. The total number of calls to f3 then is the **sum** of the number of nodes at each recursive level, which can be expressed as the equation below:&#x20;",
            "20. $$\nC(N)=1+2+4+...+2^{N-1}\n$$",
            "21. Applying the formula we saw earlier for the sum of the first powers of 2:&#x20;",
            "22. $$\n1+2+4+8+...+Q=2Q\u22121\n$$",
            "23. Substituting $$Q$$ with $$2^{N-1}$$, we get:",
            "24. $$\nC(N)=2Q\u22121=2(2^{N-1})-1=2^N-1\n\u200b\n\u200b\u200b\n$$",
            "25. The work during _each call_ is constant , so the overall runtime for `f3` is $$\\theta(2^N)$$.",
            "26. ### Method 3: Recurrence Relation (Out of Scope)",
            "27. This method is not required reading and is outside of the course scope, but worth mentioning for interest's sake.",
            '28. We can use a "recurrence relation" to count the number of calls, instead of an algebraic approach. This looks like:',
            "29. $$\nC(1)=1 C(N) = 2C(N-1) + 1\n$$",
            "30. Expanding this out with a method we will not go over but you can read about in the slides or online, we reach a similar sum to the one above. We can then again reduce it to $$2^N - 1$$ , reaching the same result of $$\\theta(2^N)$$.\n",
        ],
        "15.-asymptotics-ii/15.4-mergesort.md": [
            "1. # 15.4 Mergesort",
            "2. In our last example, we'll analyze merge sort, another cool sorting algorithm.",
            '3. {% embed url="https://youtu.be/3aRCQJxGwCQ" %}\nMergesort basics: merging two sorted lists.\n{% endembed %}',
            "4. First, let's remind ourselves of selection sort, which we will initially use as a building block for merge sort.",
            "5. Selection sort works off two basic steps:",
            "6. * Find the smallest item among the unsorted items, move it to the front, and \u2018fix\u2019 it in place.\n* Sort the remaining unsorted/unfixed items using selection sort.",
            "7. If we analyze selection sort, we see that its runtime is $$\\Theta(N^2)$$.",
            "8. **Exercise:** To convince yourself that selection sort has $$\\Theta(N^2)$$ runtime, work through the geometric approach (try drawing out the state of the list at every sort call), or count the operations.",
            "9. Let's introduce one other idea here: **arbitrary units of time**. While the exact time something will take will depend on the machine, on the particular operations, etc., we can get a general sense of time through our arbitrary units (AU).",
            "10. If we run an $$N=6$$ selection sort, and the runtime is of order $$N^2$$\u200b\u200b, it will take \\~36 AU to run. If $$N=64$$, it'll take \\~2048 AU to run. Now we don't know if that's 2048 nanoseconds, or seconds, or years, but we can get a relative sense of the time needed for each size of $$N$$.",
            "11. Hold onto this thought for later analysis.",
            "12. Now that we have selection sort, let's talk about **merging.**",
            "13. Say we have two **sorted** arrays that we want to combine into a single big sorted array. We could append one to the other, and then re-sort it, but that doesn't make use of the fact that each individual array is already sorted. How can we use this to our advantage?",
            "14. It turns out, we can merge them more quickly using the sorted property. The smallest element must be at the start of one of the two lists. So let's compare those, and put the smallest element at the start of our new list.",
            "15. Now, the next smallest element has to be at the new start of one of the two lists. We can continue comparing the first two elements and moving the smallest into place until one list is empty, then copy the rest of the other list over into the end of the new list.",
            "16. To see an animation of this idea, [go here](https://docs.google.com/presentation/d/1mdCppuWQfKG5JUBHAMHPgbSv326JtCi5mvjH1-6XcMw/edit#slide=id.g463de7561\\_042).",
            '17. What is the runtime of the merge operation? We can use the number of "write" operations to the new list as our cost model, and count the operations. Since we have to write each element of each list only once, the runtime is $$\\Theta(N)$$.',
            "18. Selection sort is slow, and merging is fast. How do we combine these to make sorting faster?",
            '19. {% embed url="https://youtu.be/AlwAZkqzHqI" %}\nA closer look at Mergesort\n{% endembed %}',
            "20. We noticed earlier that doing selection sort on an $$N=64$$ list will take \\~2048 AU. But if we sort a list half that big, $$N=32$$, it only takes \\~512 AU. That's more than twice as fast! So making the arrays we sort smaller has big time savings.",
            "21. Having two sorted arrays is a good step, but we need to put them together. Luckily, we have merge. Merge, being of linear runtime, only takes \\~64 AU. So in total, splitting it in half, sorting, then merging, only takes 512 + 512 + 64 = 1088 AU. Faster than selection sorting the whole array. But how much faster?",
            "22. Now, AUs aren't real units, but they're sometimes easier and more intuitive than looking at the runtime. The runtime for our split-in-half-then-merge-them sort is $$N+2(\\frac{N}{2})^2$$\u200b\u200b, which is about half of $$N^2$$ for selection sort. However, they are still both $$\\Theta(N^2)$$.",
            "23. What if we halved the arrays again? Will it get better? Yes! If we do two layers of merges, starting with lists of size $$\\frac{N}{4}$$, the total time will be \\~640 AU.",
            "24. **Exercise:** Show why the time is \\~640AU by calculating the time to sort each sub-list and then merge them into one array.",
            "25. What if we halved it again? And again? And again?",
            "26. Eventually we'll reach lists of size 1. At that point, we don't even have to use selection sort, because a list with one element is already sorted.",
            "27. This is the essence of **merge sort:**",
            "28. * If the list is size 1, return. Otherwise:\n* Mergesort the left half\n* Mergesort the right half\n* Merge the results",
            "29. So what's the running time of **merge sort**?",
            "30. We know merge itself is order $$N$$, so we can start by looking at each layer of merging:",
            "31. * To get the top layer: merge \\~64 elements = 64 AU\n* Second layer: merge \\~32 elements, twice = 64 AU\n* Third layer: \\~16\\*4 = 64 AU\n* ...",
            "32. Overall runtime in AU is \\~64\\*k, where $$k$$ is the number of layers. Here,  $$k=log_{2}(64)=6$$, so the overall cost of mergesort is \\~384 AU.",
            "33. Now, we saw earlier that splitting up more layers was faster, but still order $$N^2$$\u200b\u200b. Is merge sort faster than $$N^2$$\u200b\u200b?",
            "34. Yes!\\\nMergesort has worst case runtime $$\\Theta(N*log(N))$$.",
            "35. * The top level takes \\~N AU.\n* Next level takes \\~N/2 + \\~N/2 = \\~N.\n* One more level down: \\~N/4 + \\~N/4 + \\~N/4 + \\~N/4 = \\~N.",
            "36. Thus, total runtime is \\~Nk, where $$k$$ is the number of levels.",
            "37. How many levels are there? We split the array until it is length 1, so $$k=log_{2}(N)$$. Thus the overall runtime is $$\\Theta(N*log(N))$$.",
            "38. ",
            "39. **Exercise:** Use exact counts to argue for $$\\Theta(N*log(N))$$. Account for cases where we cannot divide the list perfectly in half.",
            "40. ",
            "41. So is $$\\Theta(N*log(N))$$ actually better than $$\\Theta(N^2)$$? Yes! It turns out $$\\Theta(N*log(N))$$ is not much slower than linear time.",
            "42. ![timing\\_table\\_for\\_runtimes](https://joshhug.gitbooks.io/hug61b/content/assets/timetable.png)",
            "43. ",
        ],
        "15.-asymptotics-ii/15.5-summary.md": [
            "1. ---\ndescription: Wrapping up our asymptotics adventures.\n---",
            "2. # 15.5 Summary",
            '3. {% embed url="https://youtu.be/keUNAiiGVy8" %}',
            "4. ### **Takeaways**",
            "5. * There are no magic shortcuts for analyzing code runtime.\n* In our course, it\u2019s OK to do exact counting or intuitive analysis.\n* Know how to sum 1 + 2 + 3 + ... + N and 1 + 2 + 4 + ... + N.\n* We won\u2019t be writing mathematical proofs in this class.\n* Many runtime problems you\u2019ll do in this class resemble one of the five problems from today.&#x20;\n* This topic has one of the highest skill ceilings of all topics in the course. All the tools are here, but **practice** is your friend!\n* Different solutions to the same problem, e.g. sorting, may have different runtimes (with big enough differences for the runtime to go from impractical to practical!).\n* $$N^2$$\u200b\u200b vs. $$Nlog(N)$$ is an enormous difference.\n* Going from $$Nlog(N)$$ to $$N$$ is nice, but not a radical change.",
            "6. Hopefully, this set of examples has provided some good practice with the techniques and patterns of runtime analysis. You can also find extra practice problems in the next section. Remember, there are no magic shortcuts, but you have to tools to approach the problems. Go forth and analyze!!\n",
        ],
        "15.-asymptotics-ii/README.md": [
            "1. ---\ndescription: There's no magic shortcut.\n---",
            "2. # 15. Asymptotics II",
            "3. This chapter covers various asymptotic analysis examples, which provide useful insights on how to analyze the efficiency of algorithms.\n",
        ],
        "15.-asymptotics-ii/15.6-exercises.md": [
            "1. ---\ndescription: >-\n  Doing more practices is the best way to gain intuition when it comes to\n  asymptotics!\n---",
            "2. # 15.6 Exercises",
            "3. ## Factual",
            "4. 1. Prove that $$\\Theta(\\log _{2} n) = \\Theta(\\log _{3} n)$$.\n2. What is the runtime of the following function?",
            "5. ```java\npublic static int f(int n) {\n    if (n <= 1) {\n        return 0;\n    }\n    return f(n - 1) + f(n - 1) + f(n - 1);\n}\n```",
            "6. <details>",
            "7. <summary>Problem 1</summary>",
            "8. Using the properties of logs, we see that $$\\log_2 n = \\frac{\\log n}{\\log 2} = \\Theta(\\log n)$$. Similarly, $$\\log_3 n = \\frac{\\log n}{\\log 3} = \\Theta(\\log n)$$. So when consiering asymptotics relating to logarithms, the base does not matter (as long as it is some constant).",
            "9. </details>",
            "10. <details>",
            "11. <summary>Problem 2</summary>",
            "12. This is essentially `fib` with 3 recursive calls instead of 2. The runtime is $$\\Theta(3^n)$$.",
            "13. </details>",
            "14. ## Procedural",
            "15. 1. Find the runtime of running `print_fib` with for arbitrarily large n.",
            "16. ```java\npublic void print_fib(int n) {\n   for (int i = 0; i < n; i++) {\n       System.out.println(fib(i));\n   }\n}\n\npublic int fib(int n){\n   if (n <= 0) {\n     return 0;\n   } else if (n == 1) {\n     return 1;\n   } else {\n     return fib(n-1) + fib(n-2);\n   }\n}\n```",
            "17. 2. Do the above problem again, but change the body of the for loop in `print_fib` to be:",
            "18. ```java\n System.out.println(fib(n));\n```",
            "19. 3. Find the runtime of the function `f` on an input of size `n`, given the `createArray` function as described below:",
            "20. ```java\npublic static void f(int n) {\n    if (n == 1) {\n        return;\n    }\n    f(n / 2);\n    f(n / 2);\n    createArray(n);\n}\n\npublic int[] createArray(int Q) {\n    int[] x = new int[Q];\n    for (int i = 0; i < x.length; i++) {\n        x[i] = i;\n    }\n    return x;\n}\n```",
            "21. 4. [Problem 8](https://drive.google.com/file/d/1Vo8p4vbOGt7eY5TtalvAEnk4ignpTVvm/view?usp=sharing) from the Spring 2018 Midterm 2\n5. [Problem 4](https://drive.google.com/file/d/1yWyRp7QTizspTp9dsKz5yxE6bSf9YUIi/view?usp=sharing) from the Spring 2017 Midterm 2",
            "22. <details>",
            "23. <summary>Problem 1</summary>",
            "24. From lecture, we know that `fib(i)` runs in roughly $$2^i$$ time. If we run `fib` for each `i` from `1` to `n`, we get a total work of $$2^0 + 2^1 + ... 2^n$$. Note that this is a geometric sum with last term $$2^n$$, so the overall runtime is actually still $$\\Theta(2^n)$$.",
            "25. </details>",
            "26. <details>",
            "27. <summary>Problem 2</summary>",
            "28. Again, we know that `fib(i)` takes about $$2^i$$ time. However, this time we call `fib(n)` each time in the loop `n` total times. This gives a runtime of $$\\Theta(n2^n)$$.",
            "29. </details>",
            "30. <details>",
            "31. <summary>Problem 3</summary>",
            "32. This function creates two recursive branches of half the size per call, with each node taking linear work. As such, each level has $$n$$ total work, and since we halve the input each time, there are $$\\log n$$ total levels, for a runtime of $$\\Theta(n \\log n)$$.",
            "33. </details>",
            "34. <details>",
            "35. <summary>Problem 4</summary>",
            "36. [Solutions](https://drive.google.com/file/d/1LIyFXwHYCWXNqIgKTsTyKiOYnB79\\_ykk/view?usp=sharing) and [walkthrough](https://www.youtube.com/watch?v=nMZn4EV0gGw) are linked here and on the course website.",
            "37. </details>",
            "38. <details>",
            "39. <summary>Problem 5</summary>",
            "40. [Solutions](https://drive.google.com/file/d/1b99XARlZxg3NMfeSAVt2yzDzwSEcASq3/view?usp=sharing) are linked here and on the course website.",
            "41. </details>",
            "42. ## Metacognitive",
            "43. 1. What would the runtime of `modified_fib` be? Assume that values is an array of size n. If a value in an int array is not initialized to a number, it is automatically set to 0.",
            "44. ```java\npublic void modified_fib(int n, int[] values) {\n   if (n <= 1) {\n     values[n] = n;\n     return n;\n   } else {\n     int val = values[n];\n     if (val == 0) {\n       val = modified_fib(n-1, values) + modified_fib(n-2, values);\n       values[n] = val;\n     }\n     return val;\n   }\n}  \n```",
            "45. <details>",
            "46. <summary>Problem 1</summary>",
            "47. This is an example of dynamic programming, a topic covered in more depth in CS170. Note that since `values` is saved across calls, we only recompute each value of `n` once. Computing a single value of `n` only takes constant time, since we just add two already-computed values or do an array access. As such, the overall runtime is linear: $$\\Theta(n)$$.",
            "48. </details>\n",
        ],
        "15.-asymptotics-ii/15.1-for-loops.md": [
            "1. ---\ndescription: Count, count, count...\n---",
            "2. # 15.1 For Loops",
            "3. Now that we've seen some runtime analysis, let's work through some more difficult examples. Our goal is to get some practice with the patterns and methods involved in runtime analysis. This can be a tricky idea to get a handle on, so the more practice the better.",
            "4. ## Example One:&#x20;",
            '5. {% embed url="https://www.youtube.com/watch?v=SlBSvazddmk" %}',
            "6. Last time, we saw the function dup1, that checks for the first time any entry is duplicated in a list:",
            "7. ```java\nint N = A.length;\nfor (int i = 0; i < N; i += 1)\n   for (int j = i + 1; j < N; j += 1)\n      if (A[i] == A[j])\n         return true;\nreturn false;\n```",
            "8. We have two ways of approaching our runtime analysis:",
            "9. 1. Counting number of operations\n2. Geometric visualization",
            "10. ### Method 1: Count Number of Operations",
            '11. Since the main repeating operation is the comparator, we will count the number of **"=="** operations that must occur.&#x20;',
            "12. The first time through the outer loop, the inner loop will run $$N-1$$times. The second time, it will run $$N-2$$ times. Then $$N-3$$, $$N-4$$, .... all the way till running the inner loop exactly $$1$$ time when i = $$N - 1$$. In the worst case, we have to go through every entry, and the outer loop runs $$N$$ times.&#x20;",
            '13. Then, let $$C$$ =  total number of "==" operations that have occurred. The number of comparisons is:&#x20;',
            "14. $$\nC = 1 + 2 + 3 + ... + (N - 3) + (N - 2) + (N - 1) = N(N-1)/2\n$$",
            "15. where $$N(N-1)/2$$ is part of the $$N^2$$ family.&#x20;",
            '16. Since "==" is a constant time operation, the overall runtime in the worst case is $$\\theta(N^2)$$.',
            "17. ### Method 2: Geometric Visualization",
            "18. We can also approach this from a geometric view.&#x20;",
            "19. Let's draw out when we use == operations in the grid of $$i,j$$combinations:",
            "20. ![](<../.gitbook/assets/image (122).png>)",
            "21. We see that the number of == operations is the same as the _area_ of a right triangle with a side length of $$N - 1$$. Since area is in the $$N^2$$\u200b\u200b family, we see again that the overall runtime is $$\\theta(N^2)$$.",
            "22. ## Example 2:&#x20;",
            '23. {% embed url="https://www.youtube.com/watch?v=sFUkCiswzXc" %}',
            "24. Let's look at a more involved example next. Consider the following function, with similar nested for loops:",
            '25. ```java\npublic static void printParty(int N) {\n   for (int i = 1; i <= N; i = i * 2) {\n      for (int j = 0; j < i; j += 1) {\n         System.out.println("hello");   \n         int ZUG = 1 + 1;\n      }\n   }\n}\n```',
            '26. The outer loop advances by _multiplying_ `i` by 2 each time. The inner loop runs from 0 to the current value of `i`. The two operations inside the loop are both constant time, so let\'s approach this by asking **"how many times does this print out "hello" for a given value of N?"**',
            "27. Our visualization tool from above helped us see dup1's runtime, so let's use a similar approach here. We'll lay out the grid for the nested for loops, and then track the total number of print statements needed for a given N below.",
            "28. If N is 1, then `i` only reaches 1, and `j` is only 0, since 0 < 1. So there is only one print statement:",
            '29. <figure><img src="../.gitbook/assets/image (98).png" alt=""><figcaption><p>Visualizations when N = 1</p></figcaption></figure>',
            "30. If N is 2, the next time through the loop `i` will be $$1*2 = 2,$$ and `j` can reach 1. The total number of print statements will be 3 = 1 (from first loop) + 2 (from second loop).",
            '31. <figure><img src="../.gitbook/assets/image (20).png" alt=""><figcaption><p>Visualizations when N = 2</p></figcaption></figure>',
            "32. <details>",
            "33. <summary>Conceptual Check: What happens when N = 3? </summary>",
            "34. After the second loop, $$i = 2 * 2 = 4$$, which is greater than $$N$$, so the outer loop does not continue, and ends after `i = 2`, just like N = 2 did.&#x20;",
            "35. N = 3 will have the same number of print statements as N = 2.",
            "36. The next change is at N=4, where there will be 4 prints when i = 4, 3 prints when i = 2, and 1 print when i = 1 (remember `i` never equals 3). So a total of 7.",
            '37. <img src="../.gitbook/assets/image (13).png" alt="" data-size="original">',
            "38. ",
            "39. </details>",
            "40. &#x20;We can keep filling out our diagram to get a fuller picture. Here it is up to N = 18:",
            '41. <figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption><p>Visualizations for N = 18</p></figcaption></figure>',
            "42. What we see, if we add up all the counts at each stage of the loops, is that the number of print statements is:",
            "43. $$\nC(N) = 1 + 2 + 4 + ... + N\n$$",
            "44. (if N is a power of 2).",
            "45. Again, we can think of this in two ways. Since we're already on a graphical roll, let's start there.&#x20;",
            "46. ### Method 1: Finding the Bound Visually&#x20;",
            "47. If we graph the trajectory of 0.5 N (lower dashed line), and 4N (upper dashed line), and $$C(N)$$ itself (the red staircase line), we see that C(N) is fully bounded between those two dashed lines.",
            '48. <figure><img src="../.gitbook/assets/image (53).png" alt=""><figcaption></figcaption></figure>',
            "49. Therefore, the runtime (by definition) must also be linear: $$\\theta(N)$$.",
            "50. ### Method 2: Finding the Bound Mathematically",
            "51. We can obtain the same result by solving our equation from above with the power of mathematics:",
            "52. $$\nC(N) = 1 + 2 + 4 + ... + N = 2N - 1\n$$",
            "53. &#x20;Again if N is a power of 2.&#x20;",
            "54. For example, if $$N = 8$$ :&#x20;",
            "55. $$\nC(N) = 1 + 2 + 4 + 8 = 15 = 2*8 - 1\n$$",
            "56. And by removing lesser terms and multiplicative constants, we know that $$2N - 1$$ is in the linear family, so the runtime is $$\\theta(N)$$.",
            "57. We now get a more exact value to the red-staircase line plotted below, which is $$2N$$.",
            '58. <figure><img src="../.gitbook/assets/image (12).png" alt=""><figcaption><p>Graph of 2N, bounded by 0.5N and 4N</p></figcaption></figure>',
            "59. ## Techniques: No Magic Shortcuts",
            '60. {% embed url="https://www.youtube.com/watch?themeRefresh=1&v=zWQwIHqlyuc" %}',
            "61. It would be really nice if there were some magic way to look at an algorithm and just _know_ its runtime. And it would be even nicer if all nested for loops have a runtime of $$N^2$$ .",
            "62. Unfortunately, they're not. And we know this because we just did two nested for loop examples above, each with _different_ runtimes.",
            "63. In the end, there is **no shortcut** to doing runtime analysis. It requires careful thought. But there are a few useful techniques and things to know:",
            "64. * **Find exact sum**\n* **Write out examples**\n* **Draw pictures**",
            "65. We used each of these techniques above.",
            "66. Also used in the examples above are two important sums you'll see very often:&#x20;",
            "67. * **Sum of First Natural Numbers**: $$1+2+3+...+Q=Q(Q+1)/2=\u0398(Q \u200b2 \u200b\u200b )$$\n* **Sum of First Powers of 2**: $$1+2+4+8+...+Q=2Q\u22121=\u0398(Q)$$",
            "68. You saw both of these above, and they'll return again and again in runtime analysis.",
            "69. ",
        ],
        "15.-asymptotics-ii/15.3-binary-search.md": [
            "1. ---\ndescription: hi-lo!\n---",
            "2. # 15.3 Binary Search",
            "3. Binary Search Introduction",
            '4. {% embed url="https://youtu.be/RfoP3xULk70" %}\nGetting Familiar with Binary Search\n{% endembed %}',
            "5. Binary search is a nice way of searching a list for a particular item. It requires the list to be in sorted order and uses that fact to find an element quickly.",
            "6. To do a binary search, we start in the middle of the list, and check if that's our desired element. If not, we ask: is this element bigger or smaller than our element?",
            "7. If it's bigger, then we know we only have to look at the half of the list with smaller elements. If it's too small, then we only look at the half with bigger elements. In this way, we can cut in half the number of options we have left at each step, until we find our target element.",
            "8. What's the worst possible case? When the element we want isn't in the list at all. Then we will make comparisons until we've eliminated all regions of the list, and there are no more bigger or smaller halves left.",
            "9. For an animation of binary search, see [these slides.](https://docs.google.com/presentation/d/1P4HKmsO3Aaugv7\\_U16jJN0UbfTEJi1uZUdi\\_WbIIGe0/edit#slide=id.g463de7561\\_042)",
            "10. What's the intuitive runtime of binary search? Take a minute and use the tools you know to consider this.",
            "11. We start with $$N$$ options, then $$N/2$$, then $$N/4$$ ... until we have just 1. Each time, we cut the array in half, so in the end we must perform a total of $$log_{2}(N)$$operations. Each of the $$log_{2}(N)$$ operations, eg. finding the middle element and comparing with it, takes constant time. So the overall runtime then is order $$log_{2}(N)$$.",
            "12. It's important to note, however that each step doesn't cut it _exactly_ in half. If the array is of even length, and there is no 'middle', we have to take either a smaller or a larger portion. But this is a good intuitive approach.",
            "13. We'll do a precise way next.",
            '14. {% embed url="https://youtu.be/SPX408bkhgU" %}\nA more precise analysis of Binary Search runtime\n{% endembed %}',
            "15. To precisely calculate the runtime of binary search, we'll count the number of operations, just as we've done previously.",
            "16. First, we define our cost model: let's use the number of recursive binary search calls. Since the number of operations inside each call is constant, the number of calls will be the only thing varying based on the size of the input, so it's a good cost model.",
            "17. Like we've seen before, let's do some example counts for specific $$N$$. As an exercise, try to fill this table in before continuing:",
            '18. <table><thead><tr><th width="97">N</th><th width="100">1</th><th width="100">2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr><td><strong>Count</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>',
            "19. ",
            "20. Alright, here's the result:",
            '21. <table><thead><tr><th>N</th><th width="100">1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr><td><strong>Count</strong></td><td>1</td><td>2</td><td>2</td><td>3</td><td>3</td><td>3</td><td>3</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td></tr></tbody></table>',
            "22. These seems to support our intuition above of $$log_{2}(N)$$. We can see that the count seems to increase by one only when $$N$$ hits a power of 2.",
            '23. ...but we can be even more precise: $$C(N)=\\lfloor log_{2}(N)\\rfloor +1$$ (These L-shaped bars are the "floor" function, which is the result of the expression rounded down to the nearest integer.)',
            "24. A couple properties worth knowing (see below for proofs):&#x20;",
            "25. * $$\\lfloor f(N) \\rfloor=\\Theta(f(N))$$\n* $$\\lceil f(N) \\rceil=\\Theta(f(N))$$\n* $$log_{p}(N) = \\Theta(log_{q}(N))$$",
            "26. The last one essentially states that for logarithmic runtimes, the base of the logarithm doesn't matter at all, because they are all equivalent in terms of Big-O (this can be seen by applying the logarithm change of base). Applying these simplifications, we see that $$\\Theta(\\lfloor log_{2}(N)\\rfloor=\\Theta(log(N))$$ just as we expected from our intuition.",
            "27. ",
            "28. **Example Proof:** Prove $$\\lfloor f(N) \\rfloor=\\Theta(f(N))$$",
            "29. **Solution:**&#x20;",
            "30. We start with the following inequality:",
            "31. $$\nf(N) - \\frac{1}{2}<f(N)\\le \\lfloor f(N) + \\frac{1}{2} \\rfloor \\le f(N) + \\frac{1}{2}\n$$",
            "32. Simplifying $$f(N)-\\frac{1}{2}$$ and $$f(N)+\\frac{1}{2}$$ according to our big theta rules by dropping the constants, we see that they are of order $$f(N)$$. Therefore $$\\lfloor f(N) + \\frac{1}{2} \\rfloor$$ is bounded by two expressions of order $$f(N)$$, and is therefore also $$\\Theta(f(N))$$.",
            "33. **Exercise:**&#x20;",
            "34. * Prove $$\\lceil f(N) \\rceil=\\Theta(f(N))$$\n* Prove $$log_{p}(N) = \\Theta(log_{q}(N))$$",
            "35. **One cool fact to wrap up with:** Log time is super good! It's almost as fast as constant time, and way better than linear time. This is why we like binary search, rather than stepping one by one through our list and looking for the right thing.",
            "36. To show this concretely:",
            "37. | $$N$$               | $$log_{2}(N)$$ | Typical runtime (nanoseconds) |\n| ------------------- | -------------- | ----------------------------- |\n| 100                 | 6.6            | 1                             |\n| 100,000             | 16.6           | 2.5                           |\n| 100,000,000         | 26.5           | 4                             |\n| 100,000,000,000     | 36.5           | 5.5                           |\n| 100,000,000,000,000 | 46.5           | 7                             |",
            "38. The input size increase by a factor of 1 trillion but the runtime only inceased by a factor of about 7! This shows how remarkable $$log(N)$$ runtime is especially for large size inputs.",
            "39. \\\n",
        ],
        "14.-disjoint-sets/14.1-introduction.md": [
            "1. ---\ndescription: '\ud83d\udea8 New Data Structure Alert \ud83d\udea8: Disjoint Sets'\n---",
            "2. # 14.1 Introduction",
            "3. People like you and I reside in our countries and live here. We can think of each country as a set and all of the people within it as elements within that set. The same person cannot live in two different countries simultaneously. What we have just modeled is a **disjoint set**.&#x20;",
            "4. > Two sets are named _disjoint sets_ if they have no elements in common. A Disjoint-Sets (or Union-Find) data structure keeps track of a fixed number of elements partitioned into a number of _disjoint sets_. The data structure has two operations:",
            "5. 1. `connect(x, y)`: connect `x` and `y`. Also known as `union`\n2. `isConnected(x, y)`: returns true if `x` and `y` are connected (i.e. part of the same set).",
            '6. {% embed url="https://www.youtube.com/watch?v=JNa8BRRs8L4&ab_channel=JoshHug" %}\nProfessor Hug\'s Explanation of an Introduction to Disjoin Sets\n{% endembed %}',
            "7. A Disjoint Sets data structure has a fixed number of elements that each start out in their own subset. By calling `connect(x, y)` for some elements `x` and `y`, we merge subsets together.",
            "8. For example, say we have four elements which we'll call A, B, C, D. To start off, each element is in its own set:",
            '9. <figure><img src="../.gitbook/assets/image (23).png" alt=""><figcaption><p>{A} {B} {C} {D}</p></figcaption></figure>',
            "10. After calling `connect(A, B)`:",
            "11. \\\n\\",
            '12. \n<figure><img src="../.gitbook/assets/image (126).png" alt=""><figcaption><p>{A, B} {C} {D}</p></figcaption></figure>',
            "13. Note that the subsets A and B were merged. Let's check the output some `isConnected` calls:",
            "14. \\\n`isConnected(A, B) -> true`",
            "15. `isConnected(A, C) -> false`",
            "16. After calling `connect(A, D)`:",
            "17. \\\n\\",
            '18. \n<figure><img src="../.gitbook/assets/image (104).png" alt=""><figcaption><p>{A, B, D} {C}</p></figcaption></figure>',
            "19. \\\nWe find the set A is part of and merge it with the set D is part of, creating one big A, B, D set. C is left alone.",
            "20. `isConnected(A, D) -> true`\\\n`isConnected(A, C) -> false`",
            "21. \\",
            "22. \nWith this intuition in mind, let's formally define what our DisjointSets interface looks like. As a reminder, an **interface** determines _what_ behaviors a data structure should have (but not _how_ to accomplish it). In this way, any class that implements the `DisjointSets` interface knows to always include functions: `connect(int p, int q)` and `isConnected(int p, int q)` as seen below. For now, we'll only deal with sets of non-negative integers. This is not a limitation because in production we can assign integer values to anything we would like to represent.",
            "23. ```java\npublic interface DisjointSets {\n    /** connects two items P and Q */\n    void connect(int p, int q);\n\n    /** checks to see if two items are connected */\n    boolean isConnected(int p, int q); \n}\n```",
            "24. \\\nBut how are we going to save data for these Disjoint sets to see which member belongs to it's corresponding set? What data structures are we going to use to represent this awesome data structure? In addition to learning about how to implement a fascinating data structure, this chapter will be a chance to see how an implementation of a data structure evolves. We will discuss four iterations of a Disjoint Sets design before being satisfied: [_Quick Find_](14.2-quick-find.md) _\u2192_ [_Quick Union_](14.3-quick-union.md) _\u2192_ [_Weighted Quick Union (WQU)_](14.4-weighted-quick-union-wqu.md) _\u2192_ [_WQU with Path Compression_](14.5-weighted-quick-union-with-path-compression.md). **We will see how design decisions greatly affect asymptotic runtime and code complexity.**\n",
        ],
        "14.-disjoint-sets/14.5-weighted-quick-union-with-path-compression.md":
            [
                "1. ---\ndescription: Weighted Quick Union is pretty good, but we can do even better!\n---",
                "2. # 14.5 Weighted Quick Union with Path Compression",
                '3. {% embed url="https://www.youtube.com/watch?v=DZKzDebT4gU" %}\nProfessor Hug\'s explanation on Weighted Quick Union with Path Compression\n{% endembed %}',
                "4. The clever insight is realizing that whenever we call `find(x)` we have to traverse the path from `x` to root. So, along the way we can connect all the items we visit to their root at no extra asymptotic cost.",
                "5. Connecting all the items along the way to the root will help make our tree shorter with each call to `find`.",
                "6. Recall that **both `connect(x, y)` and `isConnected(x, y)` always call `find(x)` and `find(y)`.** Thus, after calling `connect` or `isConnected` enough, essentially all elements will point directly to their root.",
                "7. By extension, the average runtime of `connect` and `isConnected` becomes **almost constant** in the long term! This is called the _amortized runtime_.",
                "8. More specifically, for M operations on N elements, WQU with Path Compression is in $$O(N + M (lg* N))$$. lg\\* is the [iterated logarithm](https://en.wikipedia.org/wiki/Iterated\\_logarithm) which is less than 5 for any real-world input.",
                '9. ### Summary <a href="#summary" id="summary"></a>',
                "10. N: number of elements in Disjoint Set",
                "11. | Implementation             | `isConnected` | `connect` |\n| -------------------------- | ------------- | --------- |\n| Quick Find                 | \u0398(1)          | \u0398(N)      |\n| Quick Union                | O(N)          | O(N)      |\n| Weighted Quick Union (WQU) | O(log N)      | O(log N)  |\n| WQU with Path Compression  | O(\u03b1(N))\\*     | O(\u03b1(N))\\* |",
                "12. \\*behaves as constant in long term.\n",
            ],
        "14.-disjoint-sets/README.md": [
            "1. ---\ndescription: By Dhruti Pandya and Mihir Mirchandani\n---",
            "2. # 14. Disjoint Sets",
            "3. ",
        ],
        "14.-disjoint-sets/14.6-exercises.md": [
            "1. # 14.6 Exercises",
            "2. ## Factual",
            "3. 1. [Problem 2](https://drive.google.com/file/d/1GuTG5O-2SSudWgm47rIi4orkmmpU4Apa/view?usp=share\\_link) from the Spring 2016 Midterm 2.\n2. [Problem 1d ](https://drive.google.com/file/d/1uE1QlF4YguWVp8m8UJ97R2xPC4b1NnQ5/view?usp=sharing)from the Spring 2015 Midterm 2.\n3. Suppose we have the following WQU with path compression. What is the height of the tree after we call `isConnected(8, 9)`?",
            '4. <figure><img src="../.gitbook/assets/ch14-exercises-pathcompression.png" alt=""><figcaption></figcaption></figure>',
            "5. <details>",
            "6. <summary>Problem 1</summary>",
            "7. [Solutions](https://drive.google.com/file/d/1KXLkjx1e8QPiu-waBJFwmcT5IToMHQK5/view?usp=sharing) are linked here and on the course website.",
            "8. </details>",
            "9. <details>",
            "10. <summary>Problem 2</summary>",
            "11. [Solutions](https://drive.google.com/file/d/1IYt4VbzdX4dTekh6cYAC8tigpJ\\_LgljV/view?usp=sharing) are linked here and on the course website.",
            "12. </details>",
            "13. <details>",
            "14. <summary>Problem 3</summary>",
            "15. The resulting tree will have height 1. Every node along the path from `0` to `9` will now have parent `0`, and similarly every node along the path from `0` to `8` will also have parent `0`.",
            "16. </details>",
            "17. ## Conceptual",
            "18. 1. Which of the following arrays could represent a valid weighted quick union structure?\n   * [ ] `[8, 0, 4, 0, 0, 4, 0, 4, 2, 0]`\n   * [ ] `[4, -8, 8, 2, 1, -2, 1, 1, 4, 5]`\n   * [ ] `[3, 3, 5, 9, 3, 6, 3, 4, 1, -10]`\n   * [ ] `[2, -10, 1, 1, 1, 1, 1, 2, 1, 7]`",
            "19. <details>",
            "20. <summary>Problem 1</summary>",
            "21. * [ ] `[8, 0, 4, 0, 0, 4, 0, 4, 2, 0]`: invalid. There is a cycle 8 --> 2 --> 4 --> 0 --> 8.\n* [ ] `[4, -8, 8, 2, 1, -2, 1, 1, 4, 5]`: invalid. The maximum height is $$4 > \\log_2 (10)$$.\n* [ ] `[3, 3, 5, 9, 3, 6, 3, 4, 1, -10]`: invalid. The size of the tree rooted at 9 (excluding the subtree rooted at 3) is less than the size of the tree rooted at 3. As such, when we connected the two trees, 3 should have been the parent of 9 instead of the other way around.\n* [ ] `[2, -10, 1, 1, 1, 1, 1, 2, 1, 7]`: valid.",
            "22. </details>",
            "23. ## Procedural",
            "24. 1. Define a _fully connected_ WQU as one where all elements are in the same set. What is the maximum and minimum height of a fully connected WQU with 6 elements?\n2. Suppose we have a WQU of height H. What is the minimum number of elements that must be in the WQU?",
            "25. <details>",
            "26. <summary>Problem 1</summary>",
            "27. The minimum height is always 1 (all elements are connected to the root). The maximum height is 2 (we take the floor of $$\\log_2 6$$).",
            "28. </details>",
            "29. <details>",
            "30. <summary>Problem 2</summary>",
            "31. We know that $$H \\leq \\log_2 N$$, where $$N$$ is the number of elements in the WQU. As such, $$N \\geq 2^H$$.",
            "32. </details>",
            "33. ## Metacognitive",
            "34. 1. [Problem 3](https://drive.google.com/file/d/1yWyRp7QTizspTp9dsKz5yxE6bSf9YUIi/view?usp=sharing) from the Spring 2017 Midterm 2.\n2. Suppose we create a WQU with $$N$$items, then we perform $$M_C$$ union operations and $$M_U$$ union operations. Using big O notation, what is the runtime of this sequence of operations?\n3. Using the same variables as problem 2, describe a sequence of operations that would result in a runtime of $$O(N + M_U + M_C)$$.\n4. Write a `int find(int p)` method for the WQU with path compression. It should perform path compression as described in lecture: any node on the path from root to our target node should have its parent reset to the root. It takes in the target node `p` and returns the root of the tree `p` is in.",
            "35. <details>",
            "36. <summary>Problem 1</summary>",
            "37. [Solutions](https://drive.google.com/file/d/1b99XARlZxg3NMfeSAVt2yzDzwSEcASq3/view?usp=sharing) are linked here and on the course website.",
            "38. </details>",
            "39. <details>",
            "40. <summary>Problem 2</summary>",
            "41. $$O(N + (M_U + M_C)\\log N)$$. Each operation takes $$\\log N$$ time, and we need $$N$$ time to initialize an empty array of size $$N$$.",
            "42. </details>",
            "43. <details>",
            "44. <summary>Problem 3</summary>",
            "45. Initialize the array as usual. Then, connect the items in sequence: `connect(0, 1)`, `connect(0, 2)`, `connect(0, 3)`, up to `connect(0, N)`. This results in a height-1 tree, on which `connect` and `isConnected` run in constant time.",
            "46. </details>",
            "47. <details>",
            "48. <summary>Problem 4</summary>",
            "49. ```java\npublic int find(int p) {\n    int root = p;\n    while (root != parent[root]) {\n        root = parent[root];\n    }\n    \n    while (p != root) {\n        int newp = parent[p];\n        parent[p] = root;\n        p = newp;\n    }\n    \n    return root;\n}\n```",
            "50. </details>\n",
        ],
        "14.-disjoint-sets/14.3-quick-union.md": [
            "1. # 14.3 Quick Union",
            '2. {% embed url="https://www.youtube.com/watch?v=RY7UCusguGg" %}\nProfessor Hug\'s explanation on Quick Union\n{% endembed %}',
            "3. Suppose we prioritize making the `connect` operation fast. We will still represent our sets with an array. Instead of an id, we assign each item the index of its parent. If an item has no parent, then it is a 'root' and we assign it a negative value.",
            "4. This approach allows us to imagine each of our sets as a tree. For example, we represent `{0, 1, 2, 4}, {3, 5}, {6}` as:",
            "5. &#x20;Note that we represent the sets using **only an array**. We visualize it ourselves as trees.",
            '6. <figure><img src="https://joshhug.gitbooks.io/hug61b/content/chap9/9.3.1.png" alt=""><figcaption></figcaption></figure>',
            "7. For QuickUnion we define a helper function `find(int item)` which returns the root of the tree `item` is in. For example, for the sets above, `find(4) == 0`, `find(1) == 0`, `find(5) == 3`, etc. Each element has a unique root.",
            "8. **`connect(x, y)`**",
            "9. To connect two items, we find the set that each item belongs to (the roots of their respective trees), and make one the child of the other. Example:",
            "10. `connect(5, 2)`:",
            "11. 1. `find(5)` -> 3\n2. `find(2)` -> 0\n3. Set `find(5)`'s value to `find(2)` aka `parent[3] = 0`",
            "12. &#x20;Note how element 3 now points to element 0, combining the two trees/sets into one.",
            '13. <figure><img src="https://joshhug.gitbooks.io/hug61b/content/chap9/9.3.2.png" alt=""><figcaption></figcaption></figure>',
            "14. In the best case, if `x` and `y` are both roots of their trees, then `connect(x, y)` just makes `x` point to `y`, a \u0398(1) operation! (Hence the name QuickUnion)",
            "15. **`isConnected(x, y)`**",
            "16. If two elements are part of the same set, then they will be in the same tree. Thus, they will have the same root. So for `isConnected(x, y)` we simply check if `find(x) == find(y)`.",
            '17. ### Performance <a href="#performance" id="performance"></a>',
            "18. There is a potential performance issue with QuickUnion: the tree can become very long. In this case, finding the root of an item (`find(item)`) becomes very expensive. Consider the tree below:",
            "19. ![](https://joshhug.gitbooks.io/hug61b/content/chap9/9.3.3.png)",
            "20. In the worst case, we have to traverse all the items to get to the root, which is a \u0398(N) runtime. Since we have to call `find` for both `connect` and `isConnected`, the runtime for both is upper bounded by O(N).",
            '21. ### Summary and Code <a href="#summary-and-code" id="summary-and-code"></a>',
            "22. N = number of elements in our DisjointSets data structure",
            "23. | Implementation | Constructor | `connect` | `isConnected` |\n| -------------- | ----------- | --------- | ------------- |\n| QuickUnion     | \u0398(N)        | O(N)      | O(N)          |\n| QuickFind      | \u0398(N)        | \u0398(N)      | \u0398(1)          |\n| QuickUnion     | \u0398(N)        | O(N)      | O(N)          |",
            "24. From the runtime chart, QuickUnion seems worse than QuickFind! Note however that O(N) as an **upper bound**. When our trees are balanced, both `connect` and `isConnected` perform reasonably well. In the next section we'll see how to _guarantee_ they perform well.",
        ],
        "14.-disjoint-sets/14.4-weighted-quick-union-wqu.md": [
            "1. # 14.4 Weighted Quick Union (WQU)",
            "2. Improving on Quick Union relies on a key insight: whenever we call `find`, we have to climb to the root of a tree. Thus, the shorter the tree the faster it takes!",
            "3. **New rule:** whenever we call `connect`, we always link the root of the smaller tree to the larger tree.",
            "4. Following this rule will give your trees a maximum height of $$log N$$, where N is the number of elements in our Disjoint Sets. How does this affect the runtime of `connect` and `isConnected`?",
            '5. {% embed url="https://www.youtube.com/watch?v=xc9s9wdaSdU" %}\nProfessor Hug\'s explanation on Weighted Quick Union\n{% endembed %}',
            "6. Let's illustrate the benefit of this with an example. Consider connecting the two sets T1 and T2 below:",
            "7. ![](https://joshhug.gitbooks.io/hug61b/content/chap9/9.4.1.png)",
            "8. We have two options for connecting them:",
            "9. &#x20;The first option we link T1 to T2. In the second, we link T2 to T1.",
            '10. <figure><img src="https://joshhug.gitbooks.io/hug61b/content/chap9/9.4.2.png" alt=""><figcaption></figcaption></figure>',
            "11. The **second option is preferable** as it only has a height of 2, rather than 3. By our new rule, we would choose the second option as well because T2 is smaller than T1 (size of 3 compared to 6).",
            "12. We determine smaller / larger by the number of items in a tree. Thus, when connecting two trees we need to know their size (or weight). We can store this information in the root of the tree by replacing the `-1`'s with `-(size of tree)`.&#x20;",
            "13. **Maximum height: Log N**",
            "14. Following the above rule ensures that the _maximum_ height of any tree is \u0398(log N). N is the number of elements in our Disjoint Sets. **By extension, the runtimes of `connect` and `isConnected` are bounded by O(log N).**&#x20;",
            "15. Why $$logN$$? The video above presents a more visual explanation. Here's an optional mathematical explanation why the maximum height is $$log_{2}N$$. Imagine any element $$x$$ in tree $$T1$$. The depth of $$x$$ increases by 1 only when $$T1$$ is placed below another tree $$T2$$. When that happens, the size of the resulting tree will be at least double the size of $$T1$$ because $$size(T2)\\geq size(T1)$$. The tree with $$x$$ can double at most $$log_{2}N$$ times until we've reached a total of N items ($$2^{log_{2}N} = N$$). So we can double up to $$log_{2}N$$\u200b\u200b times and each time, our tree adds a level \u2192 maximum $$log_{2}N$$ levels.&#x20;",
            "16. You may be wondering why we don't link trees based off of height instead of weight. It turns out this is more complicated to implement and gives us the same \u0398(log N) height limit.",
            '17. ### Summary <a href="#summary-and-code" id="summary-and-code"></a>',
            "18. | Implementation       | Constructor | `connect` | `isConnected` |\n| -------------------- | ----------- | --------- | ------------- |\n| QuickUnion           | \u0398(N)        | O(N)      | O(N)          |\n| QuickFind            | \u0398(N)        | \u0398(N)      | \u0398(1)          |\n| QuickUnion           | \u0398(N)        | O(N)      | O(N)          |\n| Weighted Quick Union | \u0398(N)        | O(log N)  | O(log N)      |",
            "19. N = number of elements in our DisjointSets data structure\n",
        ],
        "14.-disjoint-sets/14.2-quick-find.md": [
            "1. ---\ndescription: Keeping track of set membership...\n---",
            "2. # 14.2 Quick Find",
            '3. {% embed url="https://www.youtube.com/watch?v=W6Dckcv8PIo&ab_channel=JoshHug" %}\nProfessor Hug\'s explanation on Quick Find\n{% endembed %}',
            "4. ### List of Sets",
            "5. Intuitively, we might first consider representing Disjoint Sets as a list of sets, e.g, `List<Set<Integer>>`.",
            "6. For instance, if we have N=6 elements and nothing has been connected yet, our list of sets looks like: `[{0}, {1}, {2}, {3}, {4}, {5}, {6}]`. Looks good. However, consider how to complete an operation like `connect(5, 6)`. We'd have to iterate through up to `N` sets to find 5 and `N` sets to find 6. Our runtime becomes `O(N)`. And, if you were to try and implement this, the code would be quite complex.",
            "7. > The lesson to take away is that **initial design decisions determine our code complexity and runtime.**",
            "8. ### Quick Find",
            "9. Let's consider another approach using a _single array of integers_.",
            "10. * The **indices of the array** represent the elements of our set.\n* The **value at an index** is the set number it belongs to.",
            "11. For example, we represent `{0, 1, 2, 4}, {3, 5}, {6}` as:\\",
            '12. \n<figure><img src="../.gitbook/assets/image (72).png" alt=""><figcaption><p>Set 4: {0, 1, 2, 4} | Set 5: {3, 5} | Set 6: {6}</p></figcaption></figure>',
            "13. The array indices (0...6) are the elements. The value at `id[i]` is the set it belongs to. _The specific set number doesn't matter as long as all elements in the same set share the same id._",
            "14. ### **`connect(x, y)`**",
            "15. Let's see how the connect operation would work. Right now, `id[2] = 4` and `id[3] = 5`. After calling `connect(2, 3)`, all the elements with id 4 and 5 should have the same id. Let's assign them all the value 5 for now:\\",
            '16. \n<figure><img src="../.gitbook/assets/image (102).png" alt=""><figcaption><p>Set 5: {0, 1, 2, 3, 4, 5} | Set 6: {6}</p></figcaption></figure>',
            "17. **`isConnected(x, y)`**",
            "18. To check `isConnected(x, y)`, we simply check if `id[x] == id[y]`. Note this is a constant time operation!",
            '19. \\\nWe call this implementation "Quick Find" because finding if elements are connected takes constant time.',
            "20. ### Code & Runtimes",
            "21. ```java\npublic class QuickFindDS implements DisjointSets {\n\n    private int[] id;\n\n    /* \u0398(N) */\n    public QuickFindDS(int N){\n        id = new int[N];\n        for (int i = 0; i < N; i++){\n            id[i] = i;\n        }\n    }\n\n    /* need to iterate through the array => \u0398(N) */\n    public void connect(int p, int q){\n        int pid = id[p];\n        int qid = id[q];\n        for (int i = 0; i < id.length; i++){\n            if (id[i] == pid){\n                id[i] = qid;\n            }\n        }\n    }\n\n    /* \u0398(1) */\n    public boolean isConnected(int p, int q){\n        return (id[p] == id[q]);\n    }\n}\n```",
            "22. ",
            "23. N = number of elements in our DisjointSets data structure\\",
            "24. \n| Implementation | Constructor                      | `connect` | `isConnected` |\n| -------------- | -------------------------------- | --------- | ------------- |\n| ListOfSets     | \u0398(N)[\u00b9](14.2-quick-find.md#note) | O(N)      | O(N)          |\n| QuickFind      | \u0398(N)                             | \u0398(N)      | \u0398(1)          |",
            "25. ### Note",
            "26. 1\\. We didn't discuss this but you can reason that having to create N distinct sets initially is \u0398(N)[ \u21a9](https://joshhug.gitbooks.io/hug61b/content/chap9/chap92.html#reffn\\_1)\n",
        ],
        "28.-reductions-and-decomposition/28.2-shortest-paths-on-dags.md": [
            "1. # 28.2 Shortest Paths on DAGs",
            "2. Recall from the previous section that **DAGs** are **directed, acyclic graphs**. If we wanted to find the shortest path on DAGs we could use [Dijkstra's](../24.-shortest-paths/24.2-dijkstras-algorithm.md). However, with DAGs there's a simple shortest path algorithm which also handles negative edge weights!",
            "3. ### Dijkstra's Negative Edge Weight Failure",
            "4. Recall that Dijkstra's can fail if negative edges exist because it relies on the assumption that once we visit an edge, we've found the shortest path to that edge. But if negative edge weights can exist ahead of where we can see, then this assumption fails. Consider the following example:",
            "5. \\",
            '6. \n<figure><img src="../.gitbook/assets/image (37).png" alt=""><figcaption></figcaption></figure>',
            "7. Starting from A, Dijkstra's will visit C first, then B (never even considering the edge _**B\u2192C**_\u00b9",
            "8. Of course, negative edge weights do not mean Dijkstra's is guaranteed to fail. Dijkstra's succeeds with the following example:",
            '9. <figure><img src="../.gitbook/assets/image (40).png" alt=""><figcaption></figcaption></figure>',
            "10. > \u00b9. This technically depends on your implementation of Dijkstra's. If we ensure that the relaxation step only considers neighbors that are still in the queue (haven't been visited yet), then it is true that _**B\u2192C**_ will never be considered. If you don't have that check, then technically when we pop the last node (B) from the queue, we'd consider B's neighbors and update C which gives us the right answer for this specific example. However, in that case one could argue that the graph breaks the Dijkstra invariant and thus Dijkstra has 'failed'. Note, the Dijkstra invariant: _once a node is deleted from the queue (visited) then you've found the shortest path to that node._[ \u21a9](https://joshhug.gitbooks.io/hug61b/content/chap21/chap212.html#reffn\\_1)",
            "11. ",
            "12. ### Shortest Path Algorithm for DAGs",
            "13. Visit vertices in topological order:",
            "14. * On each visit, relax all outgoing edges",
            "15. Recall the definition for relaxing an edge _**u\u2192v**_ with weight w:",
            "16. ```\nif distTo[u] + w < distTo[v]:\n    distTo[v] = distTo[u] + w\n    edgeTo[v] = u\n```",
            "17. Since we visit vertices in topological order, a vertex is visited only when all possible info about it has been considered. This means that if negative edge weights exist along a path to v, then those have been taken into account by the time we get to \ufffdv!",
            "18. Finding a topological sort takes O(V+E) time while relaxation from each vertex also takes O(V+E) time in total. Thus, the overall runtime is O(V+E). Recall that Dijkstra's takes O((V+E)logV) time because of our min-heap operations.",
            "19. What if we want to solve the shortest path problem on graphs that aren't DAGs and also may have negative edges? An extension of Dijkstra's called [Bellman Ford](https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford\\_algorithm) can suit your needs, though it is out of scope for this course.",
            "20. ",
        ],
        "28.-reductions-and-decomposition/28.3-longest-path.md": [
            "1. # 28.3 Longest Path",
            '2. ## In General <a href="#in-general" id="in-general"></a>',
            "3. Consider the problem of finding the longest path from a start vertex to every other vertex. The path must be simple (contain no cycles).",
            "4. It turns out that best known algorithm is exponential (impractically inefficient).",
            "5. Negating all the edge weights and finding the shortest path leaves us in a tricky situation because then we could have negative cycles and we could go around and around them indefinitely.",
            '6. ## Longest Paths on DAGs <a href="#longest-paths-on-dags" id="longest-paths-on-dags"></a>',
            "7. But what if we are dealing with DAGs? In that case, we have no cycles so we can do as suggested above:",
            "8. 1. Form a new copy of the graph, called G', with all edge weights negated (signs flipped).\n2. Run DAG shortest paths on G' yielding result X\n3. Flip the signs of all values in X.distTo. X.edgeTo is already correct.",
            '9. Alternatively, we could modify the DAG shortest path algorithm from the previous section to choose the larger distTo when relaxing an edge. While this would make more sense in practice, the benefit of thinking of the first approach is that wree a able to use an existing algorithm as a "[black box](https://en.wikipedia.org/wiki/Black\\_box)" to solve a new problem. We\'ll learn more about this kind of problem solving in the next section: [Reductions](https://joshhug.gitbooks.io/hug61b/content/chap214.md).\n',
        ],
        "28.-reductions-and-decomposition/README.md": [
            "1. ---\ndescription: By Mihir Mirchandani\n---",
            "2. # 28. Reductions and Decomposition",
            "3. ",
        ],
        "28.-reductions-and-decomposition/28.1-topological-sorts-and-dags.md": [
            "1. # 28.1 Topological Sorts and DAGs",
            "2. We have covered a tremendous amount of material so far. Programming practices, using an IDE, designing data structures, asymptotic analysis, implementing a ton of different abstract data types (e.g. using a BST, Trie, or HashTable to implement a map, heaps to implement a Priority Queue), and finally algorithms on graphs.",
            "3. > Why is this knowledge useful?",
            "4. You may have heard people say that CS 61B teaches much of what you need to solve standard interview questions at tech companies - but why do companies seek candidates with this specific knowledge?",
            "5. One major reason is that many real world problems can be formulated in such a way that they're solvable with the data structures and algorithms we've learned. This chapter is about working through some tricky problems using the tools we have already learned.",
            "6. ",
            '7. ## Topological Sorting <a href="#topological-sorting" id="topological-sorting"></a>',
            "8. Suppose we have a collection of different tasks or activities, some of which must happen before another. How do we find sort the tasks such that for each task _**v**_, the tasks that happen before _**v**_ come earlier in our sort?",
            "9. We can first view our collection of tasks as a graph in which each node represents a task. An edge _**v**_**\u2192**_**w**_ indicates that _**v**_ must happen before _**w**_. Now our original problem is reduced to finding a **topological sort**.",
            "10. > **Topological Sort:** an ordering of a graph's vertices such that for every directed edge _**u**_\u2192_**v**_, _**u**_ comes before _**v**_ in the ordering.",
            "11. ",
            '12. <figure><img src="../.gitbook/assets/image (115).png" alt=""><figcaption><p>Question 1</p></figcaption></figure>',
            "13. <details>",
            "14. <summary><strong>Question 1.1:</strong> What are some valid topological orderings of the above graph?</summary>",
            "15. **Answer:** Valid orderings include: \\[D,B,A,E,C,F] and \\[E,D,C,B,A,F].",
            "16. </details>",
            "17. An important note is that it only makes sense to topological sort certain types of graphs. To see this, consider the following graph:",
            "18. \\",
            '19. \n<figure><img src="../.gitbook/assets/image (38).png" alt=""><figcaption></figcaption></figure>',
            "20. <details>",
            "21. <summary>What is a valid topological sorting of this?</summary>",
            "22. There isn't one! D comes before B but B comes before C, E, D. Since we have a cycle, topological sort is not defined. We also can't topologically sort an undirected graph since each edge in an undirected graph creates a cycle.",
            "23. </details>",
            "24. \\",
            '25. \n<figure><img src="../.gitbook/assets/image (113).png" alt=""><figcaption></figcaption></figure>',
            "26. So topological sorts only apply to **directed, acyclic (no cycles) graphs** - or **DAG**s.",
            "27. > **Topological Sort:** an ordering of a **DAG**'s vertices such that for every directed edge _**u**_\u2192_**v**_, _**u**_ comes before _**v**_ in the ordering.",
            "28. For any topological ordering, you can redraw the graph so that the vertices are all in one line. Thus, topological sort is sometimes called a **linearization** of the graph. For example, here's the earlier example linearized for one of the topological orderings.",
            "29. \\",
            '30. \n<figure><img src="../.gitbook/assets/image (17).png" alt=""><figcaption></figcaption></figure>',
            "31. Notice that the topological sort for the above DAG has to start with either D or E and must end with F or C. For this reason, D and E are called _sources_, and F and C are called _sinks_.",
            '32. \\\n[#topological-sorting](28.1-topological-sorts-and-dags.md#topological-sorting "mention")',
            "33. How can we find a topological sort? Take a moment to think of existing graph algorithms you already know could be helpful in solving this problem.",
            "34. Topological Sort Algorithm:",
            "35. * Perform a DFS traversal from every vertex in the graph, **not** clearing markings in between traversals.\n* Record DFS postorder along the way.\n* Topological ordering is the reverse of the postorder.",
            "36. **Why it works:** Each vertex _**v**_ gets added to the end of the postorder list only after considering **all** descendants of _**v**_. Thus, when any _**v**_ is added to the postorder list, all its descendants are already on the list. Thus reversing this list gives a topological ordering.",
            "37. Since we're simply using DFS, the runtime of this is **O(V+E)** where **V** and **E** are the number of nodes and edges in the graph respectively.",
            '38. #### Pseudocode <a href="#pseudocode" id="pseudocode"></a>',
            "39. ```\ntopological(DAG):\n    initialize marked array\n    initialize postOrder list\n    for all vertices in DAG:\n        if vertex is not marked:\n            dfs(vertex, marked, postOrder)\n    return postOrder reversed\n\ndfs(vertex, marked, postOrder):\n    marked[vertex] = true\n    for neighbor of vertex:\n        dfs(neighbor, marked, postOrder)\n    postOrder.add(vertex)\n```",
            "40. ",
            "41. <details>",
            "42. <summary><strong>(Out of scope) Extra question:</strong> How could we implement topological sort using BFS? <em>Hint 1: We'd definitely need to store some extra information.</em> <em>Hint 2: Think about keeping track of the in-degrees of each vertex.</em></summary>",
            "43. **Solution:**",
            "44. 1. Calculate in-degree of all vertices.\n2. Pick any vertex \ufffdv which has in-degree of 0.\n3. Add \ufffdv to our topological sort list. Remove the vertex \ufffdv and all edges coming out of it. Decrement in-degrees of all neighbors of vertex \ufffdv by 1.\n4. Repeat steps 2 and 3 until all vertices are removed.",
            "45. How can we accomplish Step 2 efficiently? We can use a min Priority Queue of vertices with priority equal to the in-degrees.",
            "46. </details>",
            "47. ",
            '48. ### Review <a href="#review" id="review"></a>',
            "49. * Topological sorts are a way of linearizing **Directed, Acyclic Graphs (DAGs)**.\n* We can find a topological sort of any DAG in **O(V+E)** time using **DFS** (or **BFS**).",
            "50. ",
            "51. ",
        ],
        "28.-reductions-and-decomposition/28.5-exercises.md": [
            "1. # 28.5 Exercises",
            "2. ## Factual",
            "3. 1. Which of the following statements about reductions are true?\n   * [ ] DAG-LPT reduces to DAG-SPT.\n   * [ ] 3SAT reduces to Independent Set.\n   * [ ] Percolation on an NxN grid reduces to Disjoint Sets on an NxN grid.\n   * [ ] Disjoint Sets on an NxN grid reduces to Percolation on an NxN grid.",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. * [x] DAG-LPT reduces to DAG-SPT.\n* [x] 3SAT reduces to Independent Set.\n* [x] Percolation on an NxN grid reduces to Disjoint Sets on an NxN grid.\n* [ ] Disjoint Sets on an NxN grid reduces to Percolation on an NxN grid.",
            "7. </details>",
            "8. ## Procedural",
            "9. 1. What ordering of vertices do we get if we use the topological sort algorithm from lecture on the graph below? Assume that all ties are broken in numerical order. For example, if there are multiple vertices with in-degree zero, use the smallest first. For example, if a node has multiple outgoing edges, use the smallest first.",
            "10. ![](<../.gitbook/assets/image (116).png>)",
            "11. 2. Now suppose we add weights to the edges. If we use Dijkstra's algorithm to find the SPT from 0, what is the second vertex we will visit? That is, after visiting vertex 0, which one comes next?\n3. The graph from part 2 is a DAG (i.e. contains no cycles). If we use the DAG Shortest Paths algorithm from lecture, what vertex will be visited after vertex 0?\n4. According to the DAG Shortest Paths algorithm from lecture, the first vertex we should visit is vertex 6. After relaxing the edge from 6 to 1, what is distTo\\[1]? Assume 0 is the start vertex.\n5. How many edges are in the longest paths tree for the graph above if we start from vertex 0?",
            "12. <details>",
            "13. <summary>Problem 1</summary>",
            "14. The DFS Post Order is `3 2 4 5 1 0 6`. The reverse of this is `6 0 1 5 4 2 3`. Thus, the topological sort we get is `6 0 1 5 4 2 3`.",
            "15. </details>",
            "16. <details>",
            "17. <summary>Problem 2</summary>",
            "18. 2, it has the shortest distance from the start vertex.&#x20;",
            "19. </details>",
            "20. <details>",
            "21. <summary>Problem 3</summary>",
            "22. 1, because it is the next in the topological order that we got in question 1.",
            "23. </details>",
            "24. <details>",
            "25. <summary>Problem 4</summary>",
            "26. Infinity. Recall that when relaxing the edge from `6` to `1` in a shortest paths algorithm, the new distance to be considered is `distTo[6] + w(6, 1)` where `w(6, 1)` is the weight of the edge from `6` to `1`. Since `distTo[6]` is infinity, the sum is `infinity + 9`. Since this is no better than the original value of `distTo[1]`, which is also infinity, `distTo[1]` does not change.",
            "27. </details>",
            "28. <details>",
            "29. <summary>Problem 5</summary>",
            "30. 5. There are 6 reachable vertex from 0, so the LPT will have V - 1 = 5 edges.",
            "31. </details>\n",
        ],
        "28.-reductions-and-decomposition/28.4-reductions-and-decomposition.md":
            [
                "1. # 28.4 Reductions and Decomposition",
                "2. Recall in previous section that to solve one problem (longest paths), we created a new graph G' and fed it into a different algorithm and then interpreted the result.",
                '3. <figure><img src="../.gitbook/assets/image (103).png" alt=""><figcaption></figcaption></figure>',
                '4. This process is known as **reduction**. Since DAG-SPT can be used to solve DAG-LPT, we say that "DAG-LPT reduces to DAG-SPT."',
                "5. In other words, the problem of DAG-LPT can be reduced to the problem of DAG-SPT.",
                '6. A problem like DAG-LPT can potentially be reduced to multiple other problems. As a real-world analogy, consider climbing a hill. There are many ways we can solve the problem of "climbing a hill."',
                '7. * "Climbing a hill" reduces to "riding a ski lift"\n* "Climbing a hill" reduces to "being shot out of a cannon"\n* "Climbing a hill" reduces to "riding a bike up the hill"',
                "8. Formally, **if any subroutine for task Q can be used to solve P, we say P reduces to Q.**",
                "9. This definition is visualized below:",
                '10. <figure><img src="../.gitbook/assets/image (119).png" alt=""><figcaption></figcaption></figure>',
                "11. Note that this is simply a generalization of the first graphic on this page. P reduces to Q since Q is used to solve P. This works by preprocessing the input \ufffdx into \ufffdy, running the algorithm Q on \ufffdy, and postprocessing the output into a solution for P. This is what we did for reducing DAG-LPT to DAG-SPT.",
                '12. #### Example <a href="#example" id="example"></a>',
                "13. Here we'll show how one problem can reduce to a seemingly unrelated different problem. First, the two problems:",
                "14. **Independent Set Problem**",
                "15. An independent set is a set of vertices in which no two vertices are adjacent.",
                "16. The Independent Set Problem: Does there exist an independent set of size k? In other words, can we color k vertices red, such that none touch?",
                '17. <figure><img src="../.gitbook/assets/image (16).png" alt=""><figcaption><p>Example of independent sets solutions for k=2 and k=4</p></figcaption></figure>',
                "18. **3SAT Problem**",
                "19. What values of `x1`, `x2`, `x3`, `x4` satisfy the following boolean formula: `(x1 || x2 || !x3) && (x1 || !x1 || x1) && (x2 || x3 || x4)`?",
                "20. The 3SAT Problem: Given a boolean formula, does there exist a truth value for boolean variables that obeys a set of 3-variable disjunctive constraints?",
                "21. Terminology clarification:",
                '22. * Constraints are True/False values.\n* **Disjunctive** means separated by OR. 3SAT has a set of "clauses," each made up of 3 literals with each literal separated by an OR. For example, the first clause above is `(x1 || x2 || !x3)`.\n* In the 3SAT problem we must satisfy the entire set of clauses (combine each clause with AND).',
                "23. **e.g.:** `(x1 || x2 || !x3) && (x1 || !x1 || x1) && (x2 || x3 || x4)`       Yes, a solution for x1, x2, x3, x4 exists       Solution: x1 = true, x2 = true, x3 = true, x4 = false",
                "24. **Reduction**",
                "25. **CLAIM**: 3SAT reduces to Independent Set",
                "26. * Recall this means we claim we can solve 3SAT by using the Independent Set algorithm!",
                "27. **PROOF**: To prove the reduction, we need to argue that we can:",
                "28. 1. Preprocess a given 3SAT problem\n2. Solve it with Independent Set\n3. Postprocess the output of part 2 into a solution to the original 3SAT problem.",
                "29. Let's do it!",
                "30. _**Preprocess a given 3SAT problem**_ Given an instance X of 3SAT, preprocess it into a graph G:",
                "31. 1. For each clause in X, create 3 vertices in a triangle\n2. Add an edge between each literal and its negation\\",
                '32. \n<figure><img src="../.gitbook/assets/image (27).png" alt=""><figcaption></figcaption></figure>',
                "33. _**Solve with Independent Sets**_ On graph G, find an independent set of _size = number of clauses in 3SAT_.",
                '34. <figure><img src="../.gitbook/assets/image (114).png" alt=""><figcaption></figcaption></figure>',
                '35. _**Postprocess the output**_ Elements in the independent set are considered "True", while elements outside are considered "False." If you can find an independent set of size = number of clauses in 3SAT, then you\'ve successfully solved 3SAT (using independent sets whoo!).',
                "36. In the above example, since `x3`, `!x2`, `x3`, `x4` were picked for the independent set, we consider each of those literals to be True and values for the rest don't matter. Therefore, `x3 = True, x2 = False, x4 = True, x1 = doesn't matter.`",
                "37. **Why this works:** We'll reference the below example when going through the proof.",
                "38. `(x1 || x2 || !x3) && (x1 || !x1 || x1) && (x2 || x3 || x4)`",
                "39. The above 3SAT problem has 3 clauses. To form a satisfying truth assignment we must pick one literal from each clause and give it the value True. Of course, we must be consistent. If we choose `x1` to be True in the first clause, we can't choose `!x1` to be True in the third clause (x1 can't both be True and False!).",
                "40. Representing a clause by a triangle forces us to pick only literal in a clause for the independent set. Repeat this for every clause and and finding an independent set of _size = number of clauses_ means exactly one literal will be picked from each clause (we'll consider a picked node to be True).",
                "41. We also make sure to add an edge from each literal to its negation to prevent us from choosing opposite literals (e.g. both `x1` and `!x1`) in different clauses. This may also have the effect of finding an independent set impossible - in this case, 3SAT is also not solvable.",
                "42. Here's a visualization of the above reduction:",
                '43. <figure><img src="../.gitbook/assets/image (124).png" alt=""><figcaption></figcaption></figure>',
                "44. Note that reductions are a general concept and apply to many different types of problems (they don't always involve creating graphs!)",
                '45. ### Reflection <a href="#reflection" id="reflection"></a>',
                "46. One can argue that we have been doing reduction all throughout the course.",
                "47. * Abstract Lists reduce to arrays or linked lists\n* Percolation reduces to Disjoint Sets\n* Maze generation reduces to \\[your solution here ;)]",
                "48. However these aren't exactly reductions because you aren't using a single other algorithm to solve your problem. Notably, in the earlier reduction example we used the Independent Sets algorithm as a '[black box](https://en.wikipedia.org/wiki/Black\\_box)' to solve 3SAT.",
                "49. Perhaps a better term for what we've been accomplishing earlier in the course is _decomposition_ - breaking a complex task into smaller parts. Using abstraction to make problem solving easier. This is the heart of computer science.\n",
            ],
        "17.-b-trees/17.2-big-o-vs.-worst-case.md": [
            "1. ---\ndescription: A short digression on asymptotics\n---",
            "2. # 17.2 Big O vs. Worst Case",
            "3. Consider the following statements about BSTs. Which of the following are true?",
            "4. 1. The worst-case height of a BST is $$\\Theta(N)$$.\n2. BST height is $$O(N)$$.\n3. BST height is $$O(N^2)$$.",
            '5. The answer is that all three statements are true. BSTs always have a height that is linear or better, and a linear height is obviously "less than" the quadratic upper bound in the last point.',
            "6. However, a more tricky question is which of the three statements is _the most informative_.&#x20;",
            "7. The answer here is the first statement: it gives an _exact_ upper and lower bound unlike the other statements. $$O(N)$$ could mean linear, logarithmic, square-root, or constant, but $$\\Theta(N)$$ can only mean linear.",
            "8. For an analogy, consider the following statements about the worst-case cost of a hotel room:",
            "9. 1. The most expensive room is $639/night.\n2. The most expensive room is less than or equal to $2000/night.&#x20;",
            "10. Here, we see that the first statement gives us exact information, whereas the second statement does not. In the second statement, the most expensive room could be $2000, $10, or anywhere in between.&#x20;",
            "11. However, _both are statements about the worst case_. Applying this to asymptotic notation,  this means that we can refer to the worst case with $$\\Theta$$, $$O$$, or even $$\\Omega$$. **Big O is not the same as the worst case!**",
            "12. ## Using Big O",
            "13. If $$\\Theta$$ is always more informative than $$O$$, then why do we bother using Big O notation at all? There are several reasons:",
            '14. * We can make broader statements. For example, saying "binary search is $$O(\\log N)$$ is correct, but saying "binary search tree is $$\\Theta(log N)$$" would not be correct, since it can be constant in certain scenarios.\n* Sometimes, it is not possible or extremely difficult to determine the exact runtime. In such cases, we would still like to provide a generalized upper bound.\n',
        ],
        "17.-b-trees/17.1-bst-performance.md": [
            "1. # 17.1 BST Performance",
            "2. ## Tree Height",
            '3. One unforunate feature of BSTs is that they range from a best-case "bushy" tree to a worst-case "spindly" tree.&#x20;',
            '4. In the best case, our tree will have height $$\\Theta(log N)$$, whereas in the worst case our tree has a height of $$\\Theta(N)$$, at which point it basically becomes a linked list. For example, `contains` on a "spindly" BST would take linear time.',
            "5. Both trees below have a height H = 3, yet the left tree is able to hold many more items than the left.",
            '6. <figure><img src="../.gitbook/assets/image (65).png" alt=""><figcaption></figcaption></figure>\n',
        ],
        "17.-b-trees/17.4-b-tree-invariants.md": [
            "1. # 17.4 B-Tree Invariants",
            "2. ## B-Tree Invariants",
            "3. Because of the way B-Trees are constructed, they have two invariants:",
            "4. 1. All leaves are the same distance from the root.\n2. A non-leaf node with k items must have exactly k + 1 children.",
            '5. These two invariants guarantee a "bushy" tree with $$\\log N$$ height.\n',
        ],
        "17.-b-trees/17.6-summary.md": [
            "1. # 17.6 Summary",
            "2. * BSTs have best-case height $$\\Theta(\\log N)$$, and worst-case height $$\\Theta(N)$$.\n* Big O is _not_ the same as worst-case!\n* B-Trees are a modification of the BST that maintain $$\\Theta(\\log N)$$ runtime for `add` and `contains` in the worst case. They maintain perfect balance during insertion.\n* A B-Tree has a limit $$L$$ on the number of values a node can hold, instead of having one item per node like a BST.\n* Upon `add` in a B-Tree, we simply append the value to an existing leaf node in the correct location instead of creating a new leaf node. If the node is too full, it splits and pushes a value up.\n",
        ],
        "17.-b-trees/README.md": [
            "1. # 17. B-Trees",
            "2. In this section, we build off our knowledge of binary search trees to understand a new self-balancing search tree structure: B-Trees.\n",
        ],
        "17.-b-trees/17.7-exercises.md": [
            "1. # 17.7 Exercises",
            "2. ## Factual",
            "3. 1. Which of the following are valid 2-3 trees?",
            "4. &#x20;",
            '5. <figure><img src="../.gitbook/assets/image (54).png" alt=""><figcaption></figcaption></figure>',
            "6. <details>",
            "7. <summary>Problem 1</summary>",
            "8. Only (a).",
            "9. A 2-3 tree only allows up to 3 children, which means a node can only have at most 2 values. (b) has an extra value in the rightmost node.",
            "10. A 2-3 tree must also be perfectly balanced. (c) is imbalanced.",
            "11. A B-Tree node always has one more child than the number of values. So in a 2-3 tree, a node with 1 value has 2 children, and a node with 2 values has 3 children. (d) violates this with the node `21`, which has one value but 3 children.",
            "12. </details>",
            "13. ## Procedural",
            "14. 1. Draw the 2-3 tree that results from inserting `1, 2, 3, 4, 5, 6, 7` in order.\n2. [Problem 1](https://drive.google.com/file/d/1Vo8p4vbOGt7eY5TtalvAEnk4ignpTVvm/view?usp=sharing) of the Spring 2018 Midterm 2",
            "15. <details>",
            "16. <summary>Problem 1</summary>",
            "17. Check your answers using this [interactive visualizer.](https://www.cs.usfca.edu/\\~galles/visualization/BTree.html)",
            "18. </details>",
            "19. <details>",
            "20. <summary>Problem 2</summary>",
            "21. [Solutions](https://drive.google.com/file/d/1LIyFXwHYCWXNqIgKTsTyKiOYnB79\\_ykk/view?usp=sharing) and [walkthrough](https://www.youtube.com/watch?v=nMZn4EV0gGw) are linked here and on the course website.",
            "22. </details>\n",
        ],
        "17.-b-trees/17.3-b-tree-operations.md": [
            "1. # 17.3 B-Tree Operations",
            "2. ## Height and Depth",
            "3. The average height and depth of a BST are important properties in determining performance. **Height** refers to the depth of the deepest leaf and is a tree-wide property, whereas **depth** refers to the distance from the root of a particular node and is node-specific.",
            "4. The **average depth** of a tree is the mean of the depth of every node.",
            '5. <figure><img src="../.gitbook/assets/image (105).png" alt=""><figcaption><p>Heights and depths of a binary tree</p></figcaption></figure>',
            "6. ### BSTs in Practice",
            "7. Height and average depth determine the runtime of BST operations. The height determines the worst-case runtime to fine a node, while the average depth determines the average-case runtime of search operations.",
            "8. The order in which we insert nodes has a major impact on the height and average depth of a BST. For example, consider inserting nodes `1, 2, 3, 4, 5, 6, 7`. This results in a spindly BST with height 6 and average depth 3. If we insert the same nodes in the order  `4, 2, 1, 3, 6, 5, 7`, we get a much better height of 2 and an average depth of 1.43.",
            '9. <figure><img src="../.gitbook/assets/image (78).png" alt=""><figcaption><p>Height and depth variations based on insertion order</p></figcaption></figure>',
            "10. ### Real-World BSTs",
            "11. In considering how BSTs operate in real-life applications, we may want to start by considering randomized BSTs.&#x20;",
            "12. Luckily, on average, randomly generated insertion orders have $$\\log N$$ height and average depth. In fact, you can prove that $$E[d] = 2 \\ln N$$ and $$E[h] = 4.311 \\ln N$$ (such a proof is beyond the scope of this course). Such properties hold even when considering both insertion and deletion.",
            "13. However, it is not always possible to randomize the order of insertions. For example, if we have real-time data that comes in sequentially, there is no way to shuffle the data since we do not have all the points at once.&#x20;",
            '14. As such, we need a different way to maintain "bushiness" in our search trees.',
            "15. ## B-Trees",
            "16. ### Avoiding Imbalance",
            "17. If we could simply avoid adding new leaves in our BST, the height would never increase. Such an idea, however, would be infeasible, since we do need to insert values at some point.",
            '18. One idea that we might approach is that of "overstuffing" the leaf nodes. Instead of adding a new node upon insertion, we simply stack the new value into an existing leaf node at the appropriate location.&#x20;',
            '19. <figure><img src="../.gitbook/assets/image (66).png" alt=""><figcaption><p>Overstuffing a node upon <code>insert(17)</code></p></figcaption></figure>',
            "20. However, a clear problem with this approach is that it results in large leaf nodes that basically become a list of values, going back to the problem of linear search.",
            "21. ### Moving Items Up",
            '22. To ameliorate the issue of overly stuffed leaf nodes, we may consider "moving up" a value when a leaf node reaches a certain number of values.&#x20;',
            '23. <figure><img src="../.gitbook/assets/image (159).png" alt=""><figcaption><p>Moving <code>17</code> from a leaf node to its parent</p></figcaption></figure>',
            "24. However, this runs into the issue that our binary search property is no longer preserved--`16` is to the right of `17`. As such, we need a second fix:&#x20;",
            '25. <figure><img src="../.gitbook/assets/image (109).png" alt=""><figcaption><p>Splitting the children of an overstuffed node</p></figcaption></figure>',
            "26. Above, we split the children of an overstuffed node into ranges: $$(-\\infty, 15)$$, $$[15, 17]$$, and $$(18, \\infty)$$. A search on this structure would operate exactly the same as a BST, except for a value between 15 and 17, we go to the middle child instead of the left or right.",
            "27. If we set some constant $$L$$ as a limit on our node size, our search time only increases by a constant factor (in other words, there is no asymptotic change).",
            "28. Adding to a node may cause a cascading chain reaction, as shown in the image below where we add `25` and `26`.&#x20;",
            '29. <figure><img src="../.gitbook/assets/image (56).png" alt=""><figcaption><p>Adding <code>25</code> and <code>26</code> causes multiple node splittings</p></figcaption></figure>',
            "30. In the case when our root is above the limit, we are forced to increase the tree height.",
            '31. <figure><img src="../.gitbook/assets/image (61).png" alt=""><figcaption><p>Root has four items</p></figcaption></figure>',
            '32. <figure><img src="../.gitbook/assets/image (84).png" alt=""><figcaption><p>Splitting the root</p></figcaption></figure>',
            "33. ### Perfect Balance",
            "34. Observe that our new splitting-tree data structure has perfect balance.&#x20;",
            "35. If we split the root, every node is pushed down by one level. If we split a leaf or internal node, the height does not change. There is never a change that results in imbalance.",
            "36. The real name for this data structure is a **B-Tree**. B-Trees with a limit of 3 items per node are also called **2-3-4 trees** or **2-4** trees (a node can have 2, 3, or 4 children). Setting a limit of 2 items per node results in a **2-3 tree.**",
            "37. ### B-Tree Usage",
            "38. B-Trees are used mostly in two specific contexts: first, with a small L for conceptually balancing search trees, or secondly, with L in the thousands for databases and filesystems with large records.\n",
        ],
        "17.-b-trees/17.5-b-tree-performance.md": [
            "1. # 17.5 B-Tree Performance",
            "2. ## B-Tree Runtime Analysis",
            "3. To consider the runtime of B-Trees, let $$L$$ be the maximum items per node. Based on our invariants, the maximum height must be somewhere between $$\\log_{L + 1} N$$ (best case, when all nodes have $$L$$ items) and $$\\log_2 N$$ (worst case, when each node has 1 item).&#x20;",
            "4. The overall height, then, is always on the order of $$\\Theta(\\log N)$$",
            '5. <figure><img src="../.gitbook/assets/image (45).png" alt=""><figcaption><p>Worst-case B-Tree height</p></figcaption></figure>',
            '6. <figure><img src="../.gitbook/assets/image (26).png" alt=""><figcaption><p>Best-case B-Tree height</p></figcaption></figure>',
            "7. ### Runtime for `contains`",
            "8. In the worst case, we have to examine up to $$L$$ items per node. We know that height is logarithmic, so the runtime of `contains` is bounded by $$O(L \\log N)$$. Since $$L$$ is a constant, we can drop the multiplicative factor, resulting in a runtime of $$O(\\log N)$$.",
            "9. ### Runtime for `add`",
            "10. A similar analysis can be done for `add`, except we have to consider the case in which we must split a leaf node. Since the height of the tree is $$O(\\log N)$$, at worst, we do $$\\log N$$ split operations (cascading from the leaf to the root). This simply adds an additive factor of $$\\log N$$ to our runtime, which still results in an overall runtime of $$O(\\log N)$$.\n",
        ],
        "13.-asymptotics-i/13.7-big-theta.md": [
            "1. ---\ndescription: Not to be confused with Big-O.\n---",
            "2. # 13.7 Big-Theta",
            '3. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=CGdubALgQw4" %}',
            "4. ### Formalizing Order of Growth",
            "5. Given some function $$Q(N)$$, we can apply our last two simplifications to get the order of growth of $$Q(N)$$.&#x20;",
            "6. For example, if $$Q(N)=3N^3+N^2$$, the order of growth is $$N^3$$.",
            '7. From now onward, we will refer to order of growth as $$\\Theta$$ (pronounced "big theta").&#x20;',
            "8. ### Order of Growth Examples",
            "9. The following functions have these corresponding order of growths:",
            "10. | Function          | Order of Growth |\n| ----------------- | --------------- |\n| $$N^3+3N^4$$      | $$N^4$$         |\n| $$1/N + N^3$$     | $$N^3$$         |\n| $$1/N + 5$$       | $$1$$           |\n| $$Ne^N+N$$        | $$Ne^N$$        |\n| $$40sin(N)+4N^2$$ | $$N^2$$         |",
            "11. Instead of saying a function has _order of growth_ \\_\\_\\_, we say that the function _belongs to_ $$\\Theta (\\text{__})$$. In other words, it belongs to the family of functions that have that same order of growth.",
            "12. ### Formal Definition",
            "13. For some function $$R(N)$$ with order of growth $$f(N)$$, we write that:",
            "14. $$R(N) \\in \\Theta(f(N))$$ and there exists some positive constants $$k_1$$, $$k_2$$ such that...",
            "15. $$k_1 \\cdot f(N) \\leq R(N) \\leq k_2\\cdot f(N)$$ for all values $$N$$ greater than some $$N_0$$ (a very large $$N$$).\n",
        ],
        "13.-asymptotics-i/13.1-an-introduction-to-asymptotic-analysis.md": [
            "1. ---\ndescription: We always have to start somewhere.\n---",
            "2. # 13.1 An Introduction to Asymptotic Analysis",
            '3. {% embed url="https://www.youtube.com/watch?v=DF1ThvyLwnk&ab_channel=JoshHug" %}',
            "4. Previously, we have focused on how to save time _writing_ the program. Now, we will learn how to make the best use of our computer's time and memory.&#x20;",
            "5. We can consider the process of writing efficient programs from two different perspectives:",
            "6. 1. Programming Cost _(everything in the course up to this date)_\n   1. How long does it take for you to develop your programs?\n   2. How easy is it to read or modify your code?\n   3. How maintainable is your code? (very important \u2014 much of the cost comes from maintenance and scalability, not development!)\n2. Execution Cost _(everything in the course from this point on)_\n   1. **Time complexity**: How much time does it take for your program to execute?\n   2. **Space complexity**: How much memory does your program require?",
            "7. To give a sense of what is coming up, consider a **sorted** array. Our goal is to determine if there is a duplicate element in the list.",
            "8. ```java\nList<Integer> example = [-3, -1, 2, 4, 4, 8, 10, 12];\n```",
            "9. A **na\u00efve algorithm** would be to compare every pair of elements. In the above example, we would compare -3 with every element in the list, then -1, then 2, etc.&#x20;",
            "10. A **better algorithm** would be to take advantage of the sorted nature of the list! Instead of comparing every pair of elements, we can compare each element with just the element next to it.",
            "11. We can see that the **na\u00efve algorithm** seems like it\u2019s doing a lot more unnecessary, redundant work than the **better algorithm**. But how much more work is it doing? How do we quantify how efficient a program is? This chapter will provide you the formal techniques and tools to compare the efficiency of various algorithms!\n",
        ],
        "13.-asymptotics-i/13.6-simplified-analysis-process.md": [
            "1. ---\ndescription: It's not that simple.\n---",
            "2. # 13.6 Simplified Analysis Process",
            "3. **Summary of our (Painful) Analysis Process**",
            "4. * Construct a table of exact counts of all possible operations (takes lots of effort!)\n* Convert table into worst case order of growth using 4 simplifications.",
            "5. We will now propose an alternative method that avoids building a table altogether!",
            '6. {% embed url="https://www.youtube.com/watch?v=lJ1A8Jyeba0&ab_channel=JoshHug" %}',
            "7. Our simplified analysis process will consist of:",
            "8. * Choosing our cost model, which is the representative operation we want to count.\n* Figuring out the order of growth for the count of our representative operation by either:\n  * Making an exact count and discarding unnecessary pieces or...\n  * Using intuition/inspection to determine orders of growth. This is something that comes with practice.",
            "9. #### Example: Analysis of Nested For Loops - Exact Counts",
            "10. Find the order of growth of the worst case runtime of `dup1`.",
            "11. ```java\nint N = A.length;\nfor (int i = 0; i < N; i += 1)\n   for (int j = i + 1; j < N; j += 1)\n      if (A[i] == A[j])\n         return true;\nreturn false;\n```",
            "12. We will choose our cost model to be the _number of == operations_.&#x20;",
            "13. Looking at the structure of the loops, the inner loop first gets run j=N-1 times. At the second iteration, i=1, so the inner loop runs an additional j=N-2 times. At the third iteration, i=2, so the inner loop runs an additional j=N-3 times. The total number of times the loop is run is thus:",
            "14. $$\\text{cost} = 1 + 2 + 3 + \\ldots + (N-2) + (N-1)$$",
            "15. This cost can be simplified to $$\\frac{N(N-1)}{2}$$ ([how?](https://en.wikipedia.org/wiki/1\\_%2B\\_2\\_%2B\\_3\\_%2B\\_4\\_%2B\\_%E2%8B%AF)). We can use simplification to throw away all lower order terms and constants to get the worst case order of growth $$N^2$$.",
            "16. #### Example: Analysis of Nested For Loops - Geometric Argument",
            "17. * We can see that the number of equals can be given by the area of a right triangle, which has a side length of $$N- 1$$.\n* Therefore, the order of growth of area is $$N^2$$.\u200b\u200b\n* This is definitely not something that is immediately obvious. It takes time and practice to see these patterns!\n",
        ],
        "13.-asymptotics-i/13.4-asymptotic-behavior.md": [
            "1. ---\ndescription: Be on your best behavior!\n---",
            "2. # 13.4 Asymptotic Behavior",
            "3. In most applications, we are most concerned about what happens for very large values of $$N$$.  This is known as the _asymptotic behavior_. We want to learn what types of algorithms are able to handle large amounts of data. Some examples of applications that require highly efficient algorithms are:",
            "4. * Simulating the interactions of billions of particles\n* Maintaining a social network with billions of users\n* Encoding billions of bytes of video data",
            "5. Algorithms that scale well have better _asymptotic_ runtime behavior than algorithms that scale poorly.",
            '6. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=vxYADFsa3HU" %}',
            "7. Let us return to the original problem of characterizing the runtimes of our `dup` functions. Recall that we desire characterizations that have the following features:",
            "8. * Simple and mathematically rigorous.\n* Clearly demonstrate the superiority of dup2 over dup1.",
            "9. We\u2019ve accomplished the second task! We are able to clearly see that `dup2` performed better than `dup1`. However, we didn\u2019t do it in a very simple or mathematically rigorous way. Luckily, we can apply a series of simplifications to solve these issues.",
            "10. ### Simplification 1: Consider Only the Worst Case",
            "11. When comparing algorithms, we often only care about the worst case. The worst case is often where we see the most interesting effects, so we can usually ignore all other cases but the worst case.&#x20;",
            "12. #### Example:",
            "13. Consider the operation counts of some algorithm below. What do you expect will be the order of growth of the runtime for the algorithm?",
            "14. * $$N$$ (linear)\n* $$N^2$$ (quadratic)\n* $$N^3$$ (cubic)\n* $$N^6$$ (sextic)",
            "15. | Operation        | Count           |\n| ---------------- | --------------- |\n| less than (<)    | $$100N^2 + 3N$$ |\n| greater than (>) | $$N^3+1$$       |\n| and (&&)         | $$5000$$        |",
            "16. **Answer:** $$N^3$$ (cubic)",
            '17. Intuitively, $$N^3$$ grows faster than $$N^2$$, so it would "dominate." To help further convince you that this is the case, consider the following argument:',
            "18. * Suppose the < operator takes $$\\alpha$$ nanoseconds, the > operator takes $$\\beta$$ nanoseconds, and the && takes $$\\gamma$$ nanoseconds.\n* The total time is $$\\alpha (100N^2 + 3)+\\beta (2N^3+1)+5000\\gamma$$ nanoseconds.\n* For very large $$N$$, the $$2\\beta N^3$$ term is much larger than others.\n* It can help to think of it in terms of calculus. What happens as $$N$$ approaches infinity? Which term ends up dominating?",
            "19. ### Simplification 2: Restrict Attention to One Operation",
            "20. Pick some representative operation to act as a proxy for overall runtime. From our `dup` example:",
            "21. * Good choice: `increment`, or **less than** or **equals** or **array accesses.**\n* Bad choice: **assignment of** `j = i + 1`, or `i = 0.`",
            "22. The operation we choose is called the \u201c**cost model**.\u201d",
            "23. ### Simplification 3: Eliminate Low Order Terms",
            "24. Ignore lower order terms.",
            "25. **Sanity check**: Why does this make sense? (Related to the checkpoint above!)",
            "26. ### **Simplification 4: Eliminate Multiplicative Constants**",
            "27. Ignore multiplicative constants.",
            "28. * Why? No real meaning!\n* By choosing a single representative operation, we already \u201cthrew away\u201d some information.\n* Some operations had counts of $$3N^2$$, \u200b\u200b$$N^2/2$$, etc. In general, they are all in the family/shape of $$N^2$$.",
            "29. ### Checkpoint Exercise:",
            "30. Apply our four simplification rules to the `dup2` table.&#x20;",
            "31. | Operation      | Symbolic Count |\n| -------------- | -------------- |\n| i = 0          | 1              |\n| j = i+1        | 0 to $$N$$     |\n| <              | 0 to $$N-1$$   |\n| ==             | 1 to $$N-1$$   |\n| array accesses | 2 to $$2N-2$$  |",
            "32. **Example answer**: array accesses with order of growth $$N$$.",
            "33. <, ==, and j=i+1 would be fine answers as well.",
            "34. ### Simplification Summary",
            "35. * Only consider the worst case.\n* Pick a representative operation (aka: cost model)\n* Ignore lower order terms\n* Ignore multiplicative constants.\n",
        ],
        "13.-asymptotics-i/README.md": [
            "1. ---\ndescription: 'By: Thomas Lee'\n---",
            "2. # 13. Asymptotics I",
            "3. ",
        ],
        "13.-asymptotics-i/13.8-big-o.md": [
            "1. ---\ndescription: Not to be confused with Big-Theta.\n---",
            "2. # 13.8 Big-O",
            '3. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=yG5mYNR3aIU" %}',
            '4. O (pronounced "Big-Oh") is similar to $$\\Theta$$. Instead of being an "equality" on the order of growth, it can be though of as "less than or equal."',
            "5. For example, the following statements are all true:",
            "6. * $$N^3  + 3N^4 \\in \\Theta (N^4)$$\n* $$N^3 + 3N^4 \\in \\text{O}(N^4)$$\n* $$N^3 + 3N^4 \\in \\text{O}(N^6)$$\n* $$N^3 + 3N^4 \\in \\text{O}(N^{N!})$$",
            "7. ### **Formal Definition**",
            "8. $$R(N) \\in \\text{O}(f(N))$$ means that there exists positive constant $$k_2$$ such that:\\\n$$R(N) \\leq k_2 \\cdot f(N)$$ for all values of $$N$$ greater than some $$N_0$$ (a very large $$N$$).",
            "9. Observe that this is a looser condition than $$\\Theta$$ since O does not care about the lower bound.&#x20;\n",
        ],
        "13.-asymptotics-i/13.2-runtime-characterization.md": [
            "1. ---\ndescription: Techniques for Measuring Computational Cost.\n---",
            "2. # 13.2 Runtime Characterization",
            "3. We want to be able to _characterize_ the runtime of the two algorithms we previously saw. In other words, we want to come up with some proxy that communicates the overall performance of each algorithm.&#x20;",
            "4. When characterizing runtimes, we have two goals in mind:",
            "5. * They should be simple but mathematically rigorous.\n* They should clearly demonstrate the superiority of one algorithm over another if one algorithm is better than the other.&#x20;",
            '6. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=mRn8Z46psX8" %}',
            "7. We have converted both the na\u00efve algorithm and the better algorithm into Java code. The function dup1 corresponds to the na\u00efve algorithm and dup2 corresponds to the better algorithm.",
            "8. ```java\n// Na\u00efve algorithm: compare everything\npublic static boolean dup1(int[] A) {  \n  for (int i = 0; i < A.length; i += 1) {\n    for (int j = i + 1; j < A.length; j += 1) {\n      if (A[i] == A[j]) {\n         return true;\n      }\n    }\n  }\n  return false;\n}\n```",
            "9. ```java\n// Better algorithm: compare only neighbors\npublic static boolean dup2(int[] A) {\n  for (int i = 0; i < A.length - 1; i += 1) {\n    if (A[i] == A[i + 1]) { \n      return true; \n    }\n  }\n  return false;\n}\n```",
            "10. ### Technique 1",
            "11. The first technique we will consider using for runtime characterization is directly measuring execution time in seconds using a client program.&#x20;",
            "12. There are a few ways we could do this:",
            "13. * Use a physical stopwatch (not recommended).\n* Use Unix's built in `time` command.\n* Use the Princeton Standard library which has a `stopwatch` class.",
            "14. Using any of these methods will show (with varying levels of accuracy, depending on whether the physical stopwatch path was chosen) that as input size increases, `dup1` takes a longer time to complete, whereas `dup2` completes at relatively around the same rate.",
            "15. It seems like technique 1 works perfectly fine for characterizing our runtimes! It is very easy to understand the results of the experiment, and it is easy to implement. However, there are some serious cons associated it that dissuades us from using it for everything:",
            "16. * It could take a _long_ time to finish running.&#x20;\n* Running times can vary by machine, compiler, input data, etc.&#x20;",
            "17. For these reasons, technique 1 does not meet our goals in characterizing runtimes. It's simple, but it's not mathematically rigorous. Moreover, the differences based on machine, compiler, input, etc. mean that the results may not clearly demonstrate the relationship between `dup1` and `dup2`.",
            '18. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=fxDIy6w09fw" %}',
            "19. ### Technique 2A",
            "20. Rather than physically timing the amount of time it takes for the algorithm to run, we can instead count the total number of operations completed by each algorithm! Intuitively, the algorithm that runs the fewer number of operations would be superior to the other. We can fix the input size `N` to be the same for both algorithms and compare the number of operations run.&#x20;",
            "21. Let us apply this to the `dup1` algorithm with an input size of `N=10000`:",
            "22. ```java\nfor (int i = 0; i < A.length; i += 1) {\n  for (int j = i+1; j < A.length; j += 1) {\n    if (A[i] == A[j]) {\n       return true;\n    }\n  }\n}\nreturn false;\n```",
            "23. | Operation      | Count (for N=10000)                               |\n| -------------- | ------------------------------------------------- |\n| i = 0          | 1                                                 |\n| j = i+1        | 1 (in the best case) to 10000 (in the worst case) |\n| <              | 2 to 50,015,001                                   |\n| += 1           | 0 to 50,005,000                                   |\n| ==             | 1 to 49,995,000                                   |\n| array accesses | 2 to 99,990,000                                   |",
            "24. The operation `i = 0` is only run a single time at the very start of the function call. `j = i+1` is more complicated--in the best case when `A[0] == A[1]`, it only runs a single time (convince yourself of this fact!). In the worst case, `j` is initialized once for each value that `i` takes on in the outer loop, so it is initialized a total of 10000 times.&#x20;",
            "25. As we dive further into the loop, it gets progressively less feasible to calculate the exact counts. We will show later on that the exact numbers do not matter that much.&#x20;",
            "26. To summarize technique 2A, we have solved the issue of _machine independence_. Differences in machines do not affect the total of operations run (usually). However, it is tedious to compute all the counts for each operation. In addition, our chosen input size is arbitrary, and we do not know how much time it actually takes to run.&#x20;",
            "27. ### Technique 2B",
            "28. We can resolve the issue of choosing input size by calculating the _symbolic count_ instead, which means that we calculate our counts in terms of the input `N`. Using this technique, we can update our table to include the symbolic counts:",
            "29. | Operation      | Symbolic Count       | Count (for N=10000)                               |\n| -------------- | -------------------- | ------------------------------------------------- |\n| i = 0          | 1                    | 1                                                 |\n| j = i+1        | 1 to N               | 1 (in the best case) to 10000 (in the worst case) |\n| <              | 2 to (N\u00b2 + 3N + 2)/2 | 2 to 50,015,001                                   |\n| += 1           | 0 to (N\u00b2 + N)/2      | 0 to 50,005,000                                   |\n| ==             | 1 to (N\u00b2 - N)/2      | 1 to 49,995,000                                   |\n| array accesses | 2 to N\u00b2-N            | 2 to 99,990,000                                   |",
            "30. Using symbolic counts allows us to see how the algorithms _scales_ with input size. However, it is now even more tedious to calculate! It also still does not tell the actual time.\n",
        ],
        "13.-asymptotics-i/13.9-summary.md": [
            "1. # 13.9 Summary",
            "2. To summarize this chapter:",
            "3. * Given a piece of code, we can express its runtime as a function $$R(N)$$\n  * $$N$$ is a **property** of the input of the function often representing the **size** of the input\n* Rather than finding the exact value of $$R(N)$$, we only worry about finding the **order of growth** of $$R(N)$$.\n* One approach (not universal):\n  * Choose a representative operation\n  * Let $$C(N)$$ be the count of how many times that operation occurs as a function of $$N$$.\n  * Determine order of growth $$f(N)$$ for $$C(N)$$, i.e. $$C(N)\\in \\Theta(f(N))$$\n  * Often (but not always) we consider the worst case count.\n  * If operation takes constant time, then $$R(N)\\in \\Theta(f(N))$$.\n",
        ],
        "13.-asymptotics-i/13.10-exercises.md": [
            "1. # 13.10 Exercises",
            "2. ## Factual",
            "3. 1. Analyze the runtime of the following code in terms of `N`:",
            "4. ```java\nfor (int i = 0; i < N; i++) {\n    int j = 0;\n    while (j < N) {\n        j = N;\n    }\n}\n```",
            "5. 2. Let $$f(N) = 2N$$. Which of the following statements is true?\n   * [ ] $$f(N) \\in \\Theta(1)$$\n   * [ ] $$f(N) \\in \\Theta(N)$$\n   * [ ] $$f(N) \\in \\Theta(N^2)$$\n   * [ ] $$f(N) \\in O(1)$$\n   * [ ] $$f(N) \\in O(N)$$\n   * [ ] $$f(N) \\in O(N^2)$$",
            "6. <details>",
            "7. <summary>Problem 1</summary>",
            "8. Note that the inner loop only runs once, since it immediately sets `j = N` in the first iteration. As such, the runtime is just the runtime of the outer loop, which iterates `N` times. The overall runtime, then, is linear.",
            "9. </details>",
            "10. <details>",
            "11. <summary>Problem 2</summary>",
            '12. Remember that $$\\Theta$$ means the same order of growth (linear), while $$O$$ can be roughly thought of as "less than or equal to" some order of growth.',
            "13. * [ ] $$f(N) \\in \\Theta(1)$$\n* [x] $$f(N) \\in \\Theta(N)$$\n* [ ] $$f(N) \\in \\Theta(N^2)$$\n* [ ] $$f(N) \\in O(1)$$\n* [x] $$f(N) \\in O(N)$$\n* [x] $$f(N) \\in O(N^2)$$",
            "14. </details>",
            "15. ## Procedural",
            "16. 1. **True or false.** Suppose we have a function $$f$$, and we are told $$f(N) \\in \\Theta(N^2)$$. If we run $$f$$on an input of size $$N$$, then an input of size $$2N$$, it will take roughly 4 times as long.\n2. **True or false.** Suppose we have a function $$f$$, and we are told $$f(N) \\in \\Theta(N^2)$$. If we run $$f$$on an input of size 100, then an input of size 200, it will take roughly 4 times as long.",
            "17. <details>",
            "18. <summary>Problem 1</summary>",
            "19. **True**. This is the definition of asymptotics.",
            "20. </details>",
            "21. <details>",
            "22. <summary>Problem 2</summary>",
            "23. **False**. 100 may be too small of an input for asymptotic behavior to start displaying. Remember that asymptotics only apply to very large inputs!",
            "24. </details>",
            "25. ## Metacognitive",
            "26. 1. Why do use asymptotics instead of empirical timing (for example, like the `Stopwatch` class from Lab 3)?",
            "27. <details>",
            "28. <summary>Problem 1</summary>",
            "29. There are several advantages to using asymptotics over empirical timing. See if you can come up with more beyond the list below!",
            "30. * Different computers run at different speeds. Depending on architecture, hardware components, even room temperature, the same code can execute with vastly different empirical times.\n* It may not be feasible to test code on extremely large inputs.&#x20;\n* Asymptotics are language-agnostic. The same algorithm may have different empirical runtime depending on which language it's written in (for example, C is usually around 10-400x faster than Python), but will have the same asymptotic runtime.\n* The worst-case runtime may only occur in certain cases that are hard to measure empirically.&#x20;",
            "31. ",
            "32. </details>\n",
        ],
        "13.-asymptotics-i/13.3-checkpoint-an-exercise.md": [
            "1. ---\ndescription: Some much needed practice.\n---",
            "2. # 13.3 Checkpoint: An Exercise",
            "3. Exercise: Apply techniques 2A and 2B to `dup2`.",
            "4. * Calculate the counts of each operation for the following code with respect to N.\n* Predict the _**rough**_ magnitudes of each one.",
            "5. ```java\nfor (int i = 0; i < A.length - 1; i += 1){\n  if (A[i] == A[i + 1]) { \n    return true; \n  }\n}\nreturn false;\n```",
            '6. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=IxQh6SXRBRw" %}',
            "7. #### Solution:",
            "8. Note: It's okay if you were slightly off\u2014as mentioned earlier, you want _**rough**_ estimates.",
            "9. | Operation      | Symbolic Count | Count (for N=10000) |\n| -------------- | -------------- | ------------------- |\n| i = 0          | 1              | 1                   |\n| j = i+1        | 0 to $$N$$     | 0 to 10,000         |\n| <              | 0 to $$N-1$$   | 0 to 9,999          |\n| ==             | 1 to $$N-1$$   | 1 to 9,999          |\n| array accesses | 2 to $$2N-2$$  | 2 to 19998          |",
            '10. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=jms0P6p6aQc" %}\n"I have another problem for you to solve ( \u0361\u00b0 \u035c\u0296 \u0361\u00b0)..." - Josh Hug\n{% endembed %}',
            "11. Let us compare the `dup1` table with the `dup2` table:",
            "12. `dup1` table:",
            "13. | Operation      | Symbolic Count           | Count (for N=10000)                               |\n| -------------- | ------------------------ | ------------------------------------------------- |\n| i = 0          | 1                        | 1                                                 |\n| j = i+1        | 1 to $$N$$               | 1 (in the best case) to 10000 (in the worst case) |\n| <              | 2 to $$(N\u00b2 + 3N + 2)/2$$ | 2 to 50,015,001                                   |\n| += 1           | 0 to $$(N\u00b2 + N)/2$$      | 0 to 50,005,000                                   |\n| ==             | 1 to $$(N\u00b2 - N)/2$$      | 1 to 49,995,000                                   |\n| array accesses | 2 to $$N\u00b2-N$$            | 2 to 99,990,000                                   |",
            "14. `dup2` table:",
            "15. | Operation      | Symbolic Count | Count (for N=10000) |\n| -------------- | -------------- | ------------------- |\n| i = 0          | 1              | 1                   |\n| j = i+1        | 0 to $$N$$     | 0 to 10,000         |\n| <              | 0 to $$N-1$$   | 0 to 9,999          |\n| ==             | 1 to $$N-1$$   | 1 to 9,999          |\n| array accesses | 2 to $$2N-2$$  | 2 to 19998          |",
            "16. We can see that `dup2` performs significantly better than `dup1` in the worst case!&#x20;",
            "17. One way to rationalize this is that it takes fewer operations for `dup2` to accomplish the same goal as `dup1`.&#x20;",
            "18. A better realization is that the algorithm for `dup2` scales much better in the worst case (e.g. ($$N\u00b2 + 3N + 2)/2$$ vs $$N$$)",
            "19. An even **better** realization is that parabolas ($$N\u00b2$$) always grow faster than lines ($$N$$).",
            "20. ",
        ],
        "2.-defining-and-using-classes.md": [
            "1. # 2. Defining and Using Classes",
            "2. If you do not have prior Java experience, we recommend that you work through the exercises in [HW0](http://sp19.datastructur.es/materials/hw/hw0/hw0.html) before reading this chapter. It will cover various syntax issues that we will not discuss in the book.",
            '3. #### Static vs. Non-Static Methods <a href="#static-vs-non-static-methods" id="static-vs-non-static-methods"></a>',
            "4. **Static Methods**",
            "5. All code in Java must be part of a class (or something similar to a class, which we'll learn about later). Most code is written inside of methods. Let's consider an example:",
            '6. ```java\npublic class Dog {\n    public static void makeNoise() {\n        System.out.println("Bark!");\n    }\n}\n```',
            "7. If we try running the `Dog` class, we'll simply get an error message:",
            "8. ```\n$ java Dog\nError: Main method not found in class Dog, please define the main method as:\n       public static void main(String[] args)\n```",
            "9. The `Dog` class we've defined doesn't do anything. We've simply defined something that `Dog` **can** do, namely make noise. To actually run the class, we'd either need to add a main method to the `Dog` class, as we saw in chapter 1.1. Or we could create a separate [`DogLauncher`](https://www.youtube.com/watch?v=Q-LE-jJQLTM) class that runs methods from the `Dog` class. For example, consider the program below:",
            "10. ```java\npublic class DogLauncher {\n    public static void main(String[] args) {\n        Dog.makeNoise();\n    }\n}\n```",
            "11. ```\n$ java DogLauncher\nBark!\n```",
            '12. A class that uses another class is sometimes called a "client" of that class, i.e. `DogLauncher` is a client of `Dog`. Neither of the two techniques is better: Adding a main method to `Dog` may be better in some situations, and creating a client class like `DogLauncher` may be better in others. The relative advantages of each approach will become clear as we gain additional practice throughout the course.',
            "13. **Instance Variables and Object Instantiation**",
            "14. Not all dogs are alike. Some dogs like to yap incessantly, while others bellow sonorously, bringing joy to all who hear their glorious call. Often, we write programs to mimic features of the universe we inhabit, and Java's syntax was crafted to easily allow such mimicry.",
            "15. One approach to allowing us to represent the spectrum of Dogdom would be to create separate classes for each type of Dog.",
            '16. ```java\npublic class TinyDog {\n    public static void makeNoise() {\n        System.out.println("yip yip yip yip");\n    }\n}\n\npublic class MalamuteDog {\n    public static void makeNoise() {\n        System.out.println("arooooooooooooooo!");\n    }\n}\n```',
            "17. As you should have seen in the past, classes can be instantiated, and instances can hold data. This leads to a more natural approach, where we create instances of the `Dog` class and make the behavior of the `Dog` methods contingent upon the properties of the specific `Dog`. To make this more concrete, consider the class below:",
            '18. ```java\npublic class Dog {\n    public int weightInPounds;\n\n    public void makeNoise() {\n        if (weightInPounds < 10) {\n            System.out.println("yipyipyip!");\n        } else if (weightInPounds < 30) {\n            System.out.println("bark. bark.");\n        } else {\n            System.out.println("woof!");\n        }\n    }    \n}\n```',
            "19. As an example of using such a Dog, consider:",
            "20. ```java\npublic class DogLauncher {\n    public static void main(String[] args) {\n        Dog d;\n        d = new Dog();\n        d.weightInPounds = 20;\n        d.makeNoise();\n    }\n}\n```",
            '21. When run, this program will create a `Dog` with weight 20, and that `Dog` will soon let out a nice "bark. bark.".',
            "22. Some key observations and terminology:",
            "23. * An `Object` in Java is an instance of any class.\n* The `Dog` class has its own variables, also known as _instance variables_ or _non-static variables_. These must be declared inside the class, unlike languages like Python or Matlab, where new variables can be added at runtime.\n* The method that we created in the `Dog` class did not have the `static` keyword. We call such methods _instance methods_ or _non-static methods_.\n* To call the `makeNoise` method, we had to first _instantiate_ a `Dog` using the `new` keyword, and then make a specific `Dog` bark. In other words, we called `d.makeNoise()` instead of `Dog.makeNoise()`.\n* Once an object has been instantiated, it can be _assigned_ to a _declared_ variable of the appropriate type, e.g. `d = new Dog();`\n* Variables and methods of a class are also called _members_ of a class.\n* Members of a class are accessed using _dot notation_.",
            "24. **Constructors in Java**",
            "25. As you've hopefully seen before, we usually construct objects in object oriented languages using a _constructor_:",
            "26. ```java\npublic class DogLauncher {\n    public static void main(String[] args) {\n        Dog d = new Dog(20);\n        d.makeNoise();\n    }\n}\n```",
            '27. Here, the instantiation is parameterized, saving us the time and messiness of manually typing out potentially many instance variable assignments. To enable such syntax, we need only add a "constructor" to our Dog class, as shown below:',
            '28. ```java\npublic class Dog {\n    public int weightInPounds;\n\n    public Dog(int w) {\n        weightInPounds = w;\n    }\n\n    public void makeNoise() {\n        if (weightInPounds < 10) {\n            System.out.println("yipyipyip!");\n        } else if (weightInPounds < 30) {\n            System.out.println("bark. bark.");\n        } else {\n            System.out.println("woof!");\n        }    \n    }\n}\n```',
            "29. The constructor with signature `public Dog(int w)` will be invoked anytime that we try to create a `Dog` using the `new` keyword and a single integer parameter. For those of you coming from Python, the constructor is very similar to the `__init__` method.",
            "30. **Terminology Summary**",
            "31. **Array Instantiation, Arrays of Objects**",
            "32. As we saw in HW0, arrays are also instantiated in Java using the new keyword. For example:",
            "33. ```java\npublic class ArrayDemo {\n    public static void main(String[] args) {\n        /* Create an array of five integers. */\n        int[] someArray = new int[5];\n        someArray[0] = 3;\n        someArray[1] = 4;\n    }\n}\n```",
            "34. Similarly, we can create arrays of instantiated objects in Java, e.g.",
            "35. ```java\npublic class DogArrayDemo {\n    public static void main(String[] args) {\n        /* Create an array of two dogs. */\n        Dog[] dogs = new Dog[2];\n        dogs[0] = new Dog(8);\n        dogs[1] = new Dog(20);\n\n        /* Yipping will result, since dogs[0] has weight 8. */\n        dogs[0].makeNoise();\n    }\n}\n```",
            "36. Observe that new is used in two different ways: Once to create an array that can hold two `Dog` objects, and twice to create each actual `Dog`.",
            '37. #### Class Methods vs. Instance Methods <a href="#class-methods-vs-instance-methods" id="class-methods-vs-instance-methods"></a>',
            "38. Java allows us to define two types of methods:",
            "39. * Class methods, a.k.a. static methods.\n* Instance methods, a.k.a. non-static methods.",
            "40. Instance methods are actions that can be taken only by a specific instance of a class. Static methods are actions that are taken by the class itself. Both are useful in different circumstances. As an example of a static method, the `Math` class provides a `sqrt` method. Because it is static, we can call it as follows:",
            "41. ```java\nx = Math.sqrt(100);\n```",
            "42. If `sqrt` had been an instance method, we would have instead the awkward syntax below. Luckily `sqrt` is a static method so we don't have to do this in real programs.",
            "43. ```java\nMath m = new Math();\nx = m.sqrt(100);\n```",
            "44. Sometimes, it makes sense to have a class with both instance and static methods. For example, suppose want the ability to compare two dogs. One way to do this is to add a static method for comparing Dogs.",
            "45. ```java\npublic static Dog maxDog(Dog d1, Dog d2) {\n    if (d1.weightInPounds > d2.weightInPounds) {\n        return d1;\n    }\n    return d2;\n}\n```",
            "46. This method could be invoked by, for example:",
            "47. ```java\nDog d = new Dog(15);\nDog d2 = new Dog(100);\nDog.maxDog(d, d2);\n```",
            "48. Observe that we've invoked using the class name, since this method is a static method.",
            "49. We could also have implemented `maxDog` as a non-static method, e.g.",
            "50. ```java\npublic Dog maxDog(Dog d2) {\n    if (this.weightInPounds > d2.weightInPounds) {\n        return this;\n    }\n    return d2;\n}\n```",
            "51. Above, we use the keyword `this` to refer to the current object. This method could be invoked, for example, with:",
            "52. ```java\nDog d = new Dog(15);\nDog d2 = new Dog(100);\nd.maxDog(d2);\n```",
            "53. Here, we invoke the method using a specific instance variable.",
            "54. **Exercise 1.2.1**: What would the following method do? If you're not sure, try it out.",
            "55. ```java\npublic static Dog maxDog(Dog d1, Dog d2) {\n    if (weightInPounds > d2.weightInPounds) {\n        return this;\n    }\n    return d2;\n}\n```",
            "56. **Static Variables**",
            '57. It is occasionally useful for classes to have static variables. These are properties inherent to the class itself, rather than the instance. For example, we might record that the scientific name (or binomen) for Dogs is "Canis familiaris":',
            '58. ```java\npublic class Dog {\n    public int weightInPounds;\n    public static String binomen = "Canis familiaris";\n    ...\n}\n```',
            "59. Static variables should be accessed using the name of the class rather than a specific instance, e.g. you should use `Dog.binomen`, not `d.binomen`.",
            "60. While Java technically allows you to access a static variable using an instance name, it is bad style, confusing, and in my opinion an error by the Java designers.",
            "61. **Exercise 1.2.2**: Complete this exercise:",
            "62. * Video: [link](https://youtu.be/8Gq-8mVbyFU)\n* Slide: [link](https://docs.google.com/presentation/d/10BFLHH8VaoYy7XaazwjaoTtLw3zvasX4HCssDruqw84/edit#slide=id.g6caa9a6fe\\_057)\n* Solution Video: [link](https://youtu.be/Osuy8UEH03M)",
            '63. #### public static void main(String\\[] args) <a href="#public-static-void-mainstring-args" id="public-static-void-mainstring-args"></a>',
            "64. With what we've learned so far, it's time to demystify the declaration we've been using for the main method. Breaking it into pieces, we have:",
            "65. * `public`: So far, all of our methods start with this keyword.\n* `static`: It is a static method, not associated with any particular instance.\n* `void`: It has no return type.\n* `main`: This is the name of the method.\n* `String[] args`: This is a parameter that is passed to the main method.",
            "66. **Command Line Arguments**",
            "67. Since main is called by the Java interpreter itself rather than another Java class, it is the interpreter's job to supply these arguments. They refer usually to the command line arguments. For example, consider the program `ArgsDemo` below:",
            "68. ```java\npublic class ArgsDemo {\n    public static void main(String[] args) {\n        System.out.println(args[0]);\n    }\n}\n```",
            "69. This program prints out the 0th command line argument, e.g.",
            "70. ```\n$ java ArgsDemo these are command line arguments\nthese\n```",
            '71. In the example above, `args` will be an array of Strings, where the entries are {"these", "are", "command", "line", "arguments"}.',
            "72. **Summing Command Line Arguments**",
            "73. **Exercise 1.2.3**: Try to write a program that sums up the command line arguments, assuming they are numbers. For a solution, see the webcast or the code provided on GitHub.",
            '74. #### Using Libraries <a href="#using-libraries" id="using-libraries"></a>',
            "75. One of the most important skills as a programmer is knowing how to find and use existing libraries. In the glorious modern era, it is often possible to save yourself tons of work and debugging by turning to the web for help.",
            "76. In this course, you're welcome to do this, with the following caveats:",
            "77. * Do not use libraries that we do not provide.\n* Cite your sources.\n* Do not search for solutions for specific homework or project problems.",
            '78. For example, it\'s fine to search for "convert String integer Java". However, it is not OK to search for "Project 2048 Berkeley".',
            "79. For more on collaboration and academic honesty policy, see the course syllabus.\n",
        ],
        "37.-software-engineering-iv/README.md": [
            "1. ---\ndescription: By Mihir Mirchandani\n---",
            "2. # 37. Software Engineering IV",
            "3. ",
        ],
        "37.-software-engineering-iv/37.1-the-end-is-near.md": [
            "1. # 37.1 The end is near",
            "2. Another Software Engineering lecture!? Yes!! We're actually now almost done with 61B. Best of luck on finals! Today, we'll actually just be spending time reflecting on 61B.",
            "3. 61B has been taught since \\~Spring 1994. Before that, it was CS60C, which goes back to at least 1988.&#x20;",
            "4. In modern times there have been 4 varieties of 61B:",
            "5. * Hilfinger: 4 extremely long real world projects that are somewhat based on data structures material.\n* Hug: 1 (or 2) long real world project that is somewhat based on data structures material. Remaining material ties in tightly to lectures.\n* 61BL: Lab based class that focuses heavily on data structures, but with one large real world project (Gitlet).\n* Shewchuk / Yelick (extinct): Focus on implementing data structures. No large real world project",
            "6. ### 61B 1.0",
            "7. Gitlet was first offered in the Spring 2015 offering of 61B.",
            "8. * My first solo offering of the class.\n* Projects had significant authorship from students.&#x20;\n  * Project 0 - Bomb Checkers (me, but implemented by Jimmy Lee).\n  * Project 1 - ngordnet (me).\n  * Project 2 - Gitlet (Joey Moghadam).\n  * Project 3 -  Fun with Tries (me, adapted from my old Princeton HWs).\n* Joey also used the project as one of his assignments in Summer 2015.",
            "9. ### 61B 2.0",
            "10. CS61B Version 2 (Spring 2016, 2017)",
            "11. * Fall 2014/Spring 2015 observation: Hated that students had to split time between the core data structures content and a huge project that wasn\u2019t related to that content.\n* Decided to have the messy real world project due right before data structures:\n  * 2016: Build a text editor.\n  * 2017: Create software for manipulating text databases. (This is a very cool project! I really hope you can try this for yourselves and then take CS 186!!)",
            "12. ### Feedback helps...",
            '13. <figure><img src="https://lh3.googleusercontent.com/EYU0gslQ-pFXdqjr9voSyTbvcHUY7Ztp1DP5FmIlRDXW1T7_f2OyJObaQTxmHPs8CZLOnMm-UX5SpRpp_uzbCQvd161nJZh7jYifdMJM9Qo35KrbaHChB0wYBKbiLlt3qRjRJZCEsmb9O5EFrSJiMZlvDw=s2048" alt=""><figcaption><p>Gitlet Feedback</p></figcaption></figure>',
            "14. CS61B Current Version (3.6) (Fall 2022 and Spring 2023)",
            "15. * Replaced Gitlet with Ngordnet.\n* Ngordnet is much more data structures focused.\n  * 2A: Build a TimeSeries and build an NgramMap.\n  * 2B: Build whatever you need to support additional functionality, including implementing a graph somehow.",
            "16. GSIs voted \\~3 to 1 to keep Ngordnet over Gitlet.&#x20;",
            "17. ",
            "18. ### Summary",
            "19. Overall this course was meant to help you with Software Engineering. And all of you have done an amazing job making data structures such as HashMaps and ArrayLists and using Java to solve real-world problems that I hope was interesting! Many people even come out of this class signing up for Linguistics 100 after learning about hyponyms and doing Ngordnet. Wherever you now go equipped with this amazing knowledge of 61B, I wish you the best of luck! Here's to solving more problems with CS! Cheers and good luck on Finals!",
            "20. Spring/summer 2015 Gitlet was way too hard.",
            "21. * No testing provided.\n* No tips on persistence.\n",
        ],
        "22.-tree-traversals-and-graphs/22.3-graphs.md": [
            "1. # 22.3 Graphs",
            "2. Trees are great, aren't they? But as we saw, we could draw some things using nodes and edges that weren't trees. Specifically, our restriction that there can only be one path between any two nodes didn't fit every situation. Let's see what happens when we get rid of that restriction.",
            '3. {% embed url="https://www.youtube.com/watch?v=anRVomtGXFc" %}\nGraph Definition\n{% endembed %}',
            '4. ### What is a graph? <a href="#what-is-a-graph" id="what-is-a-graph"></a>',
            "5. A graph consists of:",
            "6. * A set of nodes (or vertices)\n* A set of zero of more edges, each of which connects two nodes.",
            "7. That's it! No other restrictions.",
            "8. All of the structures below in green? Everything is a valid graph! The second one is also a tree, but none of the others are.",
            '9. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-26 at 2.02.16 PM.png" alt=""><figcaption><p>Graphs</p></figcaption></figure>',
            "10. In general, note that **all trees are also graphs, but not all graphs are trees.**",
            '11. ### Simple Graphs only <a href="#simple-graphs-only" id="simple-graphs-only"></a>',
            '12. Graphs can be divided into two categories: _simple_ graphs and _multigraphs_ (or complicated graphs, a term I invented, because that\'s how I like to think of them.) Fortunately, in this course (and almost all applications and research) focuses only on simple graphs. So when we say "graph" in this course, you should always think of a "simple graph" (unless we say otherwise.)',
            "13. Well, it's time to address the elephant in the room. What's a simple graph?",
            '14. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-26 at 2.03.21 PM.png" alt=""><figcaption><p>Simple Graphs and Multigraphs</p></figcaption></figure>',
            "15. Look at the graphs in red. The graph in the middle has 2 distinct edges going from/to bottom-next to/from bottom-left node. In other words, there are multiple edges between two nodes. This is **not** a simple graph, and we ignore their existence unless specified otherwise. Graphs like these are called multigraphs.",
            "16. Look at the third graph. It has a loop! An edge from a node to itself! We don't allow this either. Graphs like these are sometimes categorized as multigraphs, and sometimes, even multigraphs explicitly ban self-loops.",
            '17. ### More categorizations. <a href="#more-categorizations" id="more-categorizations"></a>',
            "18. Graphs are simple in the following text, and in this course, unless specified otherwise. But there are more categorizations.",
            '19. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-26 at 2.04.26 PM.png" alt=""><figcaption></figcaption></figure>',
            "20. There are undirected graphs, where an edge `(u, v)` can mean that the edge goes from the nodes u to v and from the nodes v to u too. There are directed graphs, where the edge `(u, v)` means that the edge starts at u, and goes to v (and the vice versa is not true, unless the edge `(v, u)` also exists.) Edges in directed graphs have an arrow.",
            "21. There are also acyclic graphs. These are graphs that don't have any cycles. And then there are cyclic graphs, i.e., there exists a way to start at a node, follow some **unique** edges, and return back to the same node you started from.",
            "22. In the above picture, we can clearly see the difference between how we draw directed and undirected edges.",
            "23. Take a look at the cyclic graphs. If you start at `a`, you can run back around using only distinct edges and get back to `a`. Thus, the graph is cyclic.",
            "24. Take a look at the top-left graph. Is there any node `n`, such that if you start at `n`, you can follow some distinct edges, and get back to `n`? Nope! (Remember than for directed edges, you must follow the directions. You can go from `a` to `b` but not `b` to `a`.)\n",
        ],
        "22.-tree-traversals-and-graphs/README.md": [
            "1. ---\ndescription: By Mihir Mirchandani and Dhruti Pandya\n---",
            "2. # 22. Tree Traversals and Graphs",
            "3. ",
        ],
        "22.-tree-traversals-and-graphs/22.1-tree-recap.md": [
            "1. # 22.1 Tree Recap",
            '2. ## Trees and Traversals <a href="#trees-and-traversals" id="trees-and-traversals"></a>',
            '3. {% embed url="https://www.youtube.com/watch?v=wkkCVWn7au4" %}\nProfessor Hug\'s review of 17.1 and 17.2\n{% endembed %}',
            "4. ",
            "5. ### What is a Tree?",
            "6. Recall that a tree consists of:",
            "7. * A set of connected nodes (or vertices). We use both terms interchangeably.\n* A set of edges that connect those nodes. No edges can form a cycle.\n  * **Constraint:** There is exactly one path between any two nodes.",
            "8. ",
            '9. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-26 at 6.04.39 AM.png" alt=""><figcaption><p>Graph Examples (Some Trees, some not...)</p></figcaption></figure>',
            "10. Graph #1 is a tree. It has a node. It has no edges. That's _OK_!",
            "11. The second and third structures are trees. Notice that the third is a LinkedList. A LinkedList is still a tree as it is a connected acyclic graph.",
            "12. The fourth is not a tree. Why? There are two paths from the top node to the bottom node, and so this does not obey our constraint.",
            "13. **Exercise 17.1.1.** Determine the reason why the fifth structure is not a tree. Also, modify the invalid trees above so that they are valid.",
            '14. ### What is a rooted tree? <a href="#what-is-a-rooted-tree" id="what-is-a-rooted-tree"></a>',
            "15. \\\nRecall that a rooted tree is a tree with a designated root (typically drawn as the top most node.)",
            "16. This gives us the notion of two more definitions",
            "17. * A parent. Every node except the root has exactly one parent.\n  * What if a node had 2 parents? Would it be a tree? (Hint: No.)\n* A child. A node can have 0 or more children.\n  * What if a node has 0 children? It's called a leaf.",
            '18. ### What are trees useful for? <a href="#what-are-trees-useful-for" id="what-are-trees-useful-for"></a>',
            "19. \\\nSo far, we've looked at Search Trees, Tries, Heaps, Disjoint Sets, etc. These were extremely useful in our journey to create efficient algorithms: speeding up searching for items, allowing prefixing, checking connectedness, and so on.",
            "20. But the fact of the matter is that they are even more ubiquitous than we realize. Consider an organization chart. Here, the President is the 'root'. The 'VP's are children of the root, and so on.",
            '21. <figure><img src="../.gitbook/assets/image (110).png" alt=""><figcaption><p>Organization Chart</p></figcaption></figure>',
            "22. \\\nAnother tree structure is the `61b/` directory on your Desktop (it is on your Desktop, isn't it?). As we can see, when you traverse to a subfolder it goes to subsequent subfolders and so on. This is exactly tree-like!",
            "23. \\\n**Exercise 17.1.2.** Think of other common uses of trees that weren't mentioned above. Try and determine possible implementations or designs of these trees.&#x20;",
            "24. [\\\n](https://joshhug.gitbooks.io/hug61b/content/chap17/)",
            "25. ",
        ],
        "22.-tree-traversals-and-graphs/22.2-tree-traversals.md": [
            "1. # 22.2 Tree Traversals",
            '2. ## Tree ~~Iteration~~ Traversal <a href="#tree-iteration-traversal" id="tree-iteration-traversal"></a>',
            "3. Remember how we learned to iterate through lists? There was a way to iterate through lists that felt natural. Just start at the beginning... and keep going.",
            "4. Or maybe we did some things that were a little strange, like iterate through the reverse of the list. Recall we also wrote iterators in [discussion](https://sp19.datastructur.es/materials/discussion/disc05.pdf) to skip over students who didn't write descriptions on the Office Hours queue.",
            "5. Now how do you iterate over a tree? What's the correct 'order'?",
            "6. Before we answer that question, we must not use the word iteration. Instead, we'll call it 'traversing through a tree' or a 'tree traversal'. Why? No **real** reason, except that everyone calls iteration through trees 'traversals'. Maybe it's because the world likes alliterations.",
            "7. So what are some natural ways to 'traverse' through a tree? As it turns out, there are a few \u2013\u2013 unlike a list which basically has one natural way to iterate through it:",
            "8. 1. Level order traversal. Breadth First Search (BFS)\n2. Depth-First traversals \u2013\u2013 of which there are three: pre-order, in-order and post-order. (DFS)",
            "9. Let's test out these traversals mentioned above on the tree below.",
            '10. <figure><img src="../.gitbook/assets/Screen Shot 2023-02-26 at 6.29.34 AM.png" alt=""><figcaption><p>Different Traversals on One Tree...</p></figcaption></figure>',
            "11. ###",
            "12. ### Level Order Traversal",
            "13. \\\nWe'll iterate by levels, left to right. Level 0? D. Level 1? B and then F. Level 2? A, C, E, and G.",
            "14. This gives us `D B F A C E G`.",
            "15. Imagine each level was a sentence in an english book, and we just read it off line by line.",
            "16. **Exercise 17.2.1.** Write the code for performing a level order traversal (warning, this is more difficult than the writing the other traversals). _Hint_: You will want to keep track of what level you are at.",
            "17. ",
            "18. ### Pre-order Traversal",
            '19. Here\'s the idea behind pre-order traversal. Start at the root. **Visit the root** (aka, do the **action** you want to do.) The action here is "print".',
            "20. So, we'll print the root. D. There we go.",
            "21. Now, go left, and recurse. Then, go right and recurse.",
            "22. So now we've gone left. We're at the B node. Print it. B. We'll go left after printing. (Remember, after we're done with B's left branch, we'll come back up and visit B's right.)",
            "23. Keep following this logic, and you get `D B A C F E G`.\\",
            "24. # 22.2 Tree Traversals\n\n## Tree ~~Iteration~~ Traversal <a href=\"#tree-iteration-traversal\" id=\"tree-iteration-traversal\"></a>\n\nRemember how we learned to iterate through lists? There was a way to iterate through lists that felt natural. Just start at the beginning... and keep going.\n\nOr maybe we did some things that were a little strange, like iterate through the reverse of the list. Recall we also wrote iterators in [discussion](https://sp19.datastructur.es/materials/discussion/disc05.pdf) to skip over students who didn't write descriptions on the Office Hours queue.\n\nNow how do you iterate over a tree? What's the correct 'order'?\n\nBefore we answer that question, we must not use the word iteration. Instead, we'll call it 'traversing through a tree' or a 'tree traversal'. Why? No **real** reason, except that everyone calls iteration through trees 'traversals'. Maybe it's because the world likes alliterations.\n\nSo what are some natural ways to 'traverse' through a tree? As it turns out, there are a few \u2013\u2013 unlike a list which basically has one natural way to iterate through it:\n\n1. Level order traversal. Breadth First Search (BFS)\n2. Depth-First traversals \u2013\u2013 of which there are three: pre-order, in-order and post-order. (DFS)\n\nLet's test out these traversals mentioned above on the tree below.\n\n<figure><img src=\"../.gitbook/assets/Screen Shot 2023-02-26 at 6.29.34 AM.png\" alt=\"\"><figcaption><p>Different Traversals on One Tree...</p></figcaption></figure>\n\n###\n\n### Level Order Traversal\n\n\\\nWe'll iterate by levels, left to right. Level 0? D. Level 1? B and then F. Level 2? A, C, E, and G.\n\nThis gives us `D B F A C E G`.\n\nImagine each level was a sentence in an english book, and we just read it off line by line.\n\n**Exercise 17.2.1.** Write the code for performing a level order traversal (warning, this is more difficult than the writing the other traversals). _Hint_: You will want to keep track of what level you are at.\n\n\n\n### Pre-order Traversal\n\nHere's the idea behind pre-order traversal. Start at the root. **Visit the root** (aka, do the **action** you want to do.) The action here is \"print\".\n\nSo, we'll print the root. D. There we go.\n\nNow, go left, and recurse. Then, go right and recurse.\n\nSo now we've gone left. We're at the B node. Print it. B. We'll go left after printing. (Remember, after we're done with B's left branch, we'll come back up and visit B's right.)\n\nKeep following this logic, and you get `D B A C F E G`.\\\n\n\n```java\npreOrder(BSTNode x) {\n    if (x == null) return;\n    print(x.key)\n    preOrder(x.left)\n    preOrder(x.right)\n}\n```",
            "25. \\",
            "26. \n### In-order Traversal",
            "27. Slightly different, but same big-picture idea. Here, instead of **visiting** (aka **printing**) first, we'll first visit the left branch. Then we'll print. Then we'll visit the right branch.",
            "28. So we start at D. We don't print. We go left.",
            "29. We start at B. We don't print. We go left.",
            "30. We start at A. We don't print. We go left. We find nothing. We come back up, and print A.",
            "31. Then go to A's right. Find nothing. Go back up. Now we're at B. Remember, we print after visiting left and before visiting right, so now we'll print B, then we'll visit right.",
            "32. Keep following this and you get `A B C D E F G`.",
            "33. **An alternative** way to think about this is as follows:",
            "34. First, we're at D. We know we'll print out the items from left, then D, then items from right.",
            "35. \\[items from left] D \\[items from right].",
            "36. Now what's \\[items from left] equal to? We'll start at B, print out left, then print B, then print stuff from right of B.",
            "37. \\[items from left] = \\[items from B's left] B \\[items from B's right] = A B C.",
            "38. A B C D \\[stuff from right] = A B C D E F G.",
            "39. ```java\ninOrder(BSTNode x) {\n    if (x == null) return;    \n    inOrder(x.left)\n    print(x.key)\n    inOrder(x.right)\n}\n```",
            "40. ",
            "41. ",
            "42. ### Post-order Traversal",
            "43. Again, same big-picture idea, but now we'll print left branch, then right branch, then ourselves.",
            "44. Using the method we devised earlier, the result looks like:",
            "45. \\[items from left] \\[items from right] D.",
            "46. What's \\[items from left]? It's the output from the B subtree.",
            "47. If we're at B, we'd get \\[items from left of B] \\[items from right of B] B, which is equal to A C B.",
            "48. Following this through, we get: `A C B E G F D`",
        ],
        "22.-tree-traversals-and-graphs/22.4-graph-problems.md": [
            "1. # 22.4 Graph Problems",
            '2. {% embed url="https://www.youtube.com/watch?v=-pQ3bcBsA9w" %}\nGraph Problems\n{% endembed %}',
            "3. There are many questions we can ask about a graph.",
            "4. For example,",
            "5. * **s-t Path**: Is there a path between vertices s and t?\n* **Connectivity**: Is the graph connected, i.e. is there a path between all vertices?\n* **Biconnectivity**: Is there a vertex whose removal disconnects the graph?\n* **Shortest s-t Path**: What is the shortest path between vertices s and t?\n* **Cycle Detection**: Does the graph contain any cycles?\n* **Euler Tour**: Is there a cycle that uses every edge exactly once?\n* **Hamilton Tour**: Is there a cycle that uses every vertex exactly once?\n* **Planarity**: Can you draw the graph on paper with no crossing edges?\n* **Isomorphism**: Are two graphs isomorphic (the same graph in disguise)?",
            "6. What's cool and also weird about graph problems is that it's very hard to _tell_ which problems are very hard, and which ones aren't all that hard.",
            "7. For example, consider the Euler Tour and the Hamilton Tour problems. The former... is a solved problem. It was solved as early as 1873. The solution runs in O(E) where E is the number of edges in the graph.",
            "8. The latter? If you were to solve it efficiently today, you would win every Math award there was, become one of the most famous computer scientists, win a million dollars, etc. No one has been able to solve this **efficiently**. The best known algorithms run in exponential times. People have been working on it for many decades!",
            '9. ### One step at a time! <a href="#one-step-at-a-time" id="one-step-at-a-time"></a>',
            "10. Alright, well, before we solve the million dollar problem, let's solve the first one on the list. Given a source vertex s and a target vertex t, is there a path between s and t?",
            '11. {% embed url="https://www.youtube.com/watch?v=qho01LjqOIg" %}\nGraph Connectivity\n{% endembed %}',
            "12. In other words, write a function `connected(s, t)` that takes in two vertices and returns whether there exists a path between the two.",
            "13. To begin, let's guess that we have the following code:",
            "14. ```java\nif (s == t):\n    return true;\n\nfor child in neighbors(s):\n    if isconnected(child, t):\n        return true;\n\nreturn false;\n```",
            "15. **Exercise 17.4.1.** Before you move on, please read the code above, and spend time thinking about it. Does it work? Is it efficient? Run through a couple scenarios.",
            "16. Alright, so, let's try it out.",
            "17. We start with `connected(0, 7)`? That recursively calls `connected(1, 7)`, which then recursively calls `connected(0, 7)`. Uh-oh. Infinite looping has occurred.",
            "18. **Exercise 17.4.2.** How could we fix this? Once again, thinking about this. What was the problem? We visited `s` again... but did we need to?",
            '19. Alright, let\'s try a "remember what we visited" approach.',
            "20. ```java\nmark s  // i.e., remember that you visited s already\nif (s == t):\n    return true;\n\nfor child in unmarked_neighbors(s): // if a neighbor is marked, ignore!\n    if isconnected(child, t):\n        return true;\n\nreturn false;\n```",
            "21. As it turns out, this does work! Follow the example in [these slides](https://docs.google.com/presentation/d/1OHRI7Q\\_f8hlwjRJc8NPBUc1cMu5KhINH1xGXWDfs\\_dA/edit#slide=id.g76e0dad85\\_2\\_380) to see how.",
            '22. ### Woah, what did we just develop? <a href="#woah-what-did-we-just-develop" id="woah-what-did-we-just-develop"></a>',
            '23. {% embed url="https://www.youtube.com/watch?v=5gvFEVMfccA" %}\nDepth First Paths\n{% endembed %}',
            "24. You may not have realized it, but we just developed a **depth-first traversal** (like pre-order, post-order, in-order) but for graphs. What did we do? Well, we marked ourself. Then we visited our first child. Then our first child marked itself, and visited its children. Then our first child's first child marked itself, and visited its children.",
            "25. Intuitively, we're going deep (i.e., down our family tree to our first child, our first child's first child aka our first grandchild, our first grandchild's first child, and so on... visiting this entire lineage), before we even touch our second child.",
            "26. Up next, we'll see the opposite notion, where first we visit all our children, then our grandchildren, and so on.\n",
        ],
        "10.-inheritance-ii-extends-casting-higher-order-functions/10.5-exercises.md":
            [
                "1. # 10.5 Exercises",
                "2. ## Factual",
                "3. Recall that the `maxDog` method has the following signature:&#x20;",
                "4. ```java\npublic static Dog maxDog(Dog d1, Dog d2) { \u2026 }\n```",
                "5. 1. What is the static type of `Dog.maxDog(dogC, dogD)`?",
                '6. ```java\nShowDog dogC = new ShowDog("Franklin", "Malamute", 180, 6);\nShowDog dogD = new ShowDog("Gargamel", "Corgi", 44, 12);\n\nDog.maxDog(dogC, dogD);\n```',
                "7. 2. Which (if any), will compile:&#x20;",
                "8. ```java\nDog md = Dog.maxDog(dogC, dogD);\nShowDog msd = Dog.maxDog(dogC, dogD);\n```",
                "9. 3. In the code below, what are the dynamic types of `o`, `d`, `stuff[0]`, and `stuff[1]`?",
                '10. ```java\nObject o = new Dog("Hammy", "Beagle", 15);\nDog d = new ShowDog("Ammo", "Labrador", 54);\nObject stuff[] = new Object[5];\nstuff[0] = o;\nstuff[1] = d;\nstudd[2] = null;\n```',
                "11. <details>",
                "12. <summary>Problem 1</summary>",
                "13. The static type is `Dog`, the declared return type of the `maxDog` method.",
                "14. </details>",
                "15. <details>",
                "16. <summary>Problem 2</summary>",
                "17. Only the first line compiles. The static return type of `maxDog` is `Dog`, a superclass of `ShowDog`, so the compiler assumes that this method call return value is too broad for our declared variable `msd`.&#x20;",
                "18. ```java\nDog md = Dog.maxDog(dogC, dogD);\n// ShowDog msd = Dog.maxDog(dogC, dogD);\n```",
                "19. </details>",
                "20. <details>",
                "21. <summary>Problem 3</summary>",
                "22. * `o`: `Dog`, since it is instantiated as a `Dog`.\n* `d`: `ShowDog`, since it is instantiated as a `ShowDog`.\n* `stuff[0]`: `Dog`, since it points to the same object as `o`, which has dynamic type `Dog`.&#x20;\n* `stuff[1]`: `ShowDog`, since it points to the same object as `d`, which has dynamic type `ShowDog`.",
                "23. </details>",
                "24. ## Conceptual",
                "25. 1. Is it possible for an interface to extend a class? Provide an argument as to why or why not.\n2. What are the differences between `extends` and `implements` inheritance? Is there a particular time when you would want to use one over the other?",
                "26. <details>",
                "27. <summary>Problem 1</summary>",
                '28. This is not possible in Java. Conceptually, the reasoning is that classes have implementation details for each method, whereas interfaces do not implement methods. Thus, there is no way for an interface to "inherit" implementations from a superclass.',
                "29. </details>",
                "30. <details>",
                "31. <summary>Problem 2</summary>",
                '32. The primary difference is that `extends` inherits implementations, allowing a subclass to reuse code from the superclass, while `implements` only serves as a "contract" that a class will implement certain methods.',
                "33. Use `extends` when you want a subclass to have the same behavior as a superclass in certain methods. Use `implements` when subclasses have different implementations of the same conceptual methods, or when you need to inherit from multiple supertypes.",
                "34. </details>",
                "35. ## Procedural",
                "36. 1. Say there is a class `Poodle` that inherits from `Dog`. The Dog class looks like this:",
                "37. ```java\npublic class Dog {\n  int weight;\n  public Dog(int weight_in_pounds) {\n    weight = weight_in_pounds;\n  }\n}\n```",
                "38. And the Poodle class looks like this:",
                "39. ```java\npublic class Poodle extends Dog {\n public Poodle() {}\n}\n```",
                "40. Is this valid? If so, explain why. If it is not valid, then explain how we can make it valid.",
                "41. 2. The `Monkey` class is a subclass of the `Animal` class and the `Dog` class is a subclass of the `Animal` class. However, a Dog is not a Monkey nor is a Monkey a Dog. What will happen for the following code? Assume that the constructors are all formatted properly.",
                '42. ```java\nMonkey jimmy = new Monkey("Jimmy");\nDog limmy = (Dog) jimmy;\n```',
                "43. 3. How about for this code? Provide brief explanation as to why you believe your answers to be correct.",
                '44. ```java\nMonkey orangutan = new Monkey("fruitful");\nDog mangotan = ((Dog) ((Animal) orangutan));\n```',
                "45. <details>",
                "46. <summary>Problem 1</summary>",
                "47. This will not compile because the `Poodle` constructor implicitly calls the `Dog` constructor (through `super`), but the `Dog` class has no zero-argument constructor.&#x20;",
                "48. To make this valid, we could add a zero-argument `Dog` constructor, or make the `Poodle` constructor take in an `int` argument and call `super(weight_in_pounds)`.",
                "49. </details>",
                "50. <details>",
                "51. <summary>Problem 2</summary>",
                '52. This will not compile, since casting to a "sibling" class (same parent class, but unrelated), will error during compile.',
                "53. </details>",
                "54. <details>",
                "55. <summary>Problem 3</summary>",
                "56. This compiles, since the static types match up: `mangotan` is declared as a `Dog` and cast to a `Dog` on the right-hand side. However, the dynamic type of `orangutan` is a monkey, and we will run into a casting error since the `Monkey` class is unrelated to the `Dog` class.",
                "57. </details>",
                "58. ## Metacognitive",
                "59. 1. [Problem 1](https://drive.google.com/file/d/14yQDPQVcsWAG-QBPxlrX3N-YDRyR2c\\_C/view) from the Spring 2018 Midterm 1\n2. [Problem 1](https://drive.google.com/file/d/12KqULT9XIb3KHQqOdGQprsKELYTrAXN5/view?usp=sharing) from the Spring 2017 Midterm 1",
                "60. <details>",
                "61. <summary>Problem 1</summary>",
                "62. [Solutions](https://drive.google.com/file/d/1hnRfUh2ImLGqwOSS28hFUxnpZ\\_cjMaQT/view) and [walkthrough](https://www.youtube.com/playlist?list=PLnp31xXvnfRqAfvA4R9Oh09PstFymwCif) are linked here and on the course website.",
                "63. </details>",
                "64. <details>",
                "65. <summary>Problem 2</summary>",
                "66. [Solutions](https://drive.google.com/file/d/15Vnd9z34Wlp-D9boc4ZezgpyH\\_ul65qZ/view?usp=sharing) and [walkthrough](https://www.youtube.com/playlist?list=PLnp31xXvnfRr58vXF7T9I2HZ1LrVlxCtG) are linked here and on the course website.",
                "67. </details>",
                "68. ",
            ],
        "10.-inheritance-ii-extends-casting-higher-order-functions/10.1-implementation-inheritance-extends.md":
            [
                "1. # 10.1 Implementation Inheritance: Extends",
                "2. ### **Getting Started**",
                '3. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&index=1&list=PL8FaHk7qbOD6Mi8gDriGGeSnHi68QLuVD&v=990kImS-_nA" %}',
                "4. ### **The Extends Keyword**",
                "5. Up to now we have been writing classes and interfaces, and you may have noticed places where we have to write redundant code for different or similar classes. So, we have the idea of **inheritance**: the idea that a class/object does not need to redefine all its methods, and instead can use properties of a parent class.",
                "6. When a class is a hyponym of an interface, we used implements. **Example below:**",
                "7. ```java\nSLList<Blorp> implements List61B<Blorp>\n```",
                "8. If you want one class to be a hyponym of another class (instead of an interface), you use extends.&#x20;",
                "9. ### Rotating SLList",
                "10. We\u2019d like to build RotatingSLList that can perform any SLList operation as well as: rotateRight(): Moves back item to the front.&#x20;",
                "11. ```java\npublic class RotatingSLList<Blorp> extends SLList<Blorp>{\n       public void rotateRight() {\n              Blorp oldBack = removeLast();\n              addFirst(oldBack);\n\t}\n}\n```",
                "12. Because of extends, RotatingSLList inherits all members of SLList:",
                "13. * All instance and static variables.\n* All methods.\n* All nested classes.\n* Constructors are **not** inherited!",
                "14. **Example:** Suppose we have \\[5, 9, 15, 22]. After rotateRight: \\[22, 5, 9, 15].",
                "15. ### Video Example",
                '16. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&index=2&list=PL8FaHk7qbOD6Mi8gDriGGeSnHi68QLuVD&v=RJ_OpzLeHeQ" %}',
                "17. ### Another Example: VengefulSLList",
                "18. Suppose we want to build an SLList that:",
                "19. * Remembers all Items that have been destroyed by removeLast.\n* Has an additional method printLostItems(), which prints all deleted items.",
                '20. ```java\npublic class VengefulSLList<Item> extends SLList<Item> {\n\tprivate SLList<Item> deletedItems;\n\tpublic VengefulSLList() {\n       deletedItems = new SLList<Item>();\n\t}\n  \n\t@Override\n\tpublic Item removeLast() {\n    \t\tItem oldBack = super.removeLast(); /*calls Superclass\u2019s \nversion of removeLast() */\n    \t\tdeletedItems.addLast(oldBack);\n    \t\treturn oldBack;\n\t}\n \n\tpublic void printLostItems() {\n    \t\tdeletedItems.print();\n\t}\n}\n\npublic static void main(String[] args) {\n    \tVengefulSLList<Integer> vs1 = new VengefulSLList<Integer>();\n    \tvs1.addLast(1);\n    \tvs1.addLast(5);\n    \tvs1.addLast(10);\n    \tvs1.addLast(13);      /* [1, 5, 10, 13] */\n    \tvs1.removeLast();     /* 13 gets deleted. */\n    \tvs1.removeLast();     /* 10 gets deleted. */\n    \tSystem.out.print("The fallen are: ");\n    \tvs1.printLostItems(); /* Should print 10 and 13. */\n}\n```',
                "21. ### Constructor Behavior",
                "22. Constructors are **not** inherited. However, the rules of Java say that all constructors must start with a call to one of the super class\u2019s constructors \\[[Link](https://docs.oracle.com/javase/tutorial/java/IandI/super.html)].",
                "23. * Idea: If every VengefulSLList is-an SLList, every VengefulSLList must be set up like an SLList.\n  * If you didn\u2019t call SLList constructor, sentinel would be null. Very bad.\n* You can explicitly call the constructor with the keyword super (no dot).",
                "24. If you don\u2019t explicitly call the constructor, Java will automatically do it for you.",
                "25. These constructors below are exactly equivalent:",
                "26. ```java\npublic VengefulSLList() {\n   deletedItems = new SLList<Item>();\n}\n\npublic VengefulSLList() {\n   super();\n   deletedItems = new SLList<Item>();\n}\n```",
                "27. On the other hand, if you want to use a super constructor other than the no-argument constructor, can give parameters to super.",
                "28. These constructors below are **not** equivalent! The code to below makes implicit call to super(), not super(x). This is because only the empty argument super() is called.",
                "29. ```java\npublic VengefulSLList(Item x) {\n   super(x);\n   deletedItems = new SLList<Item>();\n}\n\npublic VengefulSLList(Item x) {\n   deletedItems = new SLList<Item>();\n}\n```",
                "30. ### Is-a vs Has-a",
                "31. Important Note: **extends** should only be used for **is-a** (hypernymic) relationships!",
                "32. Common mistake is to use it for \u201chas-a\u201d relationships. (a.k.a. meronymic).",
                "33. * Possible to subclass SLList to build a Set, but conceptually weird, e.g. get(i) doesn\u2019t make sense, because sets are not ordered.",
                "34. ### The Object Class",
                "35. As it happens, every type in Java is a descendant of the Object class.",
                "36. * VengefulSLList extends SLList.\n* SLList extends Object (implicitly).",
                "37. Documentation for Object class: [Object (Java SE 9 & JDK 9 )](https://docs.oracle.com/javase/9/docs/api/java/lang/Object.html)",
                "38. **Abstraction:** As you\u2019ll learn later in this class, programs can get a tad confusing when they are really large. A way to make programs easier to handle is to use abstraction. Abstraction is hiding components of programs that people do not need to see. The user of the hidden methods should be able to use them without knowing how they work.\\\n",
            ],
        "10.-inheritance-ii-extends-casting-higher-order-functions/10.3-casting.md":
            [
                "1. # 10.3 Casting",
                "2. ### Dynamic Method Selection and Type Checking Puzzle",
                "3. ",
                "4. **Static vs. Dynamic Type Reminder:** Every variable in Java has a static type. This is the type specified when the variable is declared, and is checked at compile time. Every variable also has a dynamic type; this type is specified when the variable is instantiated, and is checked at runtime.",
                "5. ### Compile-Time Type Checking and Expressions",
                "6. Compiler allows method calls based on compile-time type of variable. The compiler also allows assignments based on compile-time types.",
                "7. Expressions have compile-time types:",
                "8. * An expression using the new keyword has the specified compile-time type. Example:",
                "9. ```java\nSLList<Integer> sl = new VengefulSLList<Integer>();\n```",
                "10. * Compile-time type of right hand side (RHS) expression is VengefulSLList.\n* A VengefulSLList is-an SLList, so assignment is allowed.",
                "11. ```java\nVengefulSLList<Integer> vsl = new SLList<Integer>();\n```",
                "12. * Compile-time type of RHS expression is SLList.&#x20;\n* An SLList is not necessarily a VengefulSLList, so compilation error results.",
                "13. Expressions have compile-time types:",
                "14. * Method calls have compile-time type equal to their declared type.",
                "15. ```java\npublic static Dog maxDog(Dog d1, Dog d2) { \u2026 }\n```",
                "16. * Any call to maxDog will have compile-time type Dog!",
                "17. Example:",
                '18. ```java\nPoodle frank  = new Poodle("Frank", 5);\nPoodle frankJr = new Poodle("Frank Jr.", 15);\n\nDog largerDog = maxDog(frank, frankJr);\nPoodle largerPoodle = maxDog(frank, frankJr);\n```',
                "19. * Compilation error! RHS has compile-time type Dog",
                "20. ## Casting",
                "21. Java has a special syntax for specifying the compile-time type of any expression.",
                "22. * Put desired type in parenthesis before the expression.\n* Tells compiler to pretend it sees a particular type.",
                "23. Casting is a powerful but dangerous tool.",
                "24. * Tells Java to treat an expression as having a different compile-time type.\n* In example below, effectively tells the compiler to ignore its type checking duties.\n* Does not actually change anything: sunglasses don\u2019t make the world dark.\n",
            ],
        "10.-inheritance-ii-extends-casting-higher-order-functions/README.md": [
            "1. ---\ndescription: By Vanessa Teo\n---",
            "2. # 10. Inheritance II: Extends, Casting, Higher Order Functions",
            "3. This section covers inheritance through extends, along with encapsulation, casting, and higher order functions. Its contents correspond to Lecture 9.",
            "4. ### Video Playlist",
            '5. {% embed url="https://www.youtube.com/playlist?list=PL8FaHk7qbOD6Mi8gDriGGeSnHi68QLuVD" %}\n',
        ],
        "10.-inheritance-ii-extends-casting-higher-order-functions/10.4-higher-order-functions-in-java.md":
            [
                "1. # 10.4 Higher Order Functions in Java",
                "2. **Higher Order Function:** A function that treats another function as data.",
                "3. * e.g. takes a function as input.\n* Example in Python:&#x20;",
                "4. ```python\ndef tenX(x):\n   return 10*x\n \ndef do_twice(f, x):\n   return f(f(x))\n \nprint(do_twice(tenX, 2))\n```",
                "5. ### Higher Order Functions in Java 7",
                "6. Old School (Java 7 and earlier)",
                "7. * Fundamental issue: Memory boxes (variables) cannot contain pointers to functions.\n  * use an interface instead:",
                "8. ```java\npublic interface IntUnaryFunction {\n\tint apply(int x);\n}\n\npublic class TenX implements IntUnaryFunction {\n\tpublic int apply(int x) {\n   \t\treturn 10 * x;\n\t}\n}\n```",
                "9. This code above is the same as the Python code:",
                "10. ```python\ndef tenX(x):\n    return 10*x\n```",
                "11. Now, to finish the rest of the Python code in Java:",
                '12. <pre class="language-java"><code class="lang-java">public interface IntUnaryFunction {\n\tint apply(int x);\n}',
                "13. public class TenX implements IntUnaryFunction {\n\tpublic int apply(int x) {\n   \t\treturn 10 * x;\n\t}\n}",
                "14. <strong>public class HoFDemo {\n</strong>\tpublic static int do_twice(IntUnaryFunction f, int x) {\n   \t\treturn f.apply(f.apply(x));\n\t}\n\t\n\tpublic static void main(String[] args) {\n   \t\tSystem.out.println(do_twice(new TenX(), 2));\n\t}\n}\n</code></pre>",
                "15. This code above is equivalent to the Python code provided earlier.\n",
            ],
        "10.-inheritance-ii-extends-casting-higher-order-functions/10.2-encapsulation.md":
            [
                "1. # 10.2 Encapsulation",
                "2. ### Complexity",
                "3. When building large programs, our enemy is complexity.",
                "4. Some tools for managing complexity:",
                "5. * Hierarchical abstraction.\n  * Create layers of abstraction, with clear abstraction barriers!\n* \u201cDesign for change\u201d (D. Parnas)\n  * Organize program around objects.\n  * Let objects decide how things are done.\n  * Hide information others don\u2019t need.",
                "6. Managing complexity is super important for large projects!",
                "7. ### Modules and Encapsulation",
                "8. **Module:** A set of methods that work together as a whole to perform some task or set of related tasks. A module is said to be **encapsulated** if its implementation is completely hidden, and it can be accessed only through a documented interface.\n",
            ],
        "4.-sllists.md": [
            "1. # 4. SLLists",
            "2. In Chapter 3, we built the `IntList` class, a list data structure that can technically do all the things a list can do. However, in practice, the `IntList` suffers from the fact that it is fairly awkward to use, resulting in code that is hard to read and maintain.",
            "3. Fundamentally, the issue is that the `IntList` is what I call a **naked recursive** data structure. In order to use an `IntList` correctly, the programmer must understand and utilize recursion even for simple list related tasks. This limits its usefulness to novice programmers, and potentially introduces a whole new class of tricky errors that programmers might run into, depending on what sort of helper methods are provided by the `IntList` class.",
            "4. Inspired by our experience with the `IntList`, we'll now build a new class `SLList`, which much more closely resembles the list implementations that programmers use in modern languages. We'll do so by iteratively adding a sequence of improvements.",
            '5. #### Improvement #1: Rebranding <a href="#improvement-1-rebranding" id="improvement-1-rebranding"></a>',
            "6. Our `IntList` class from last time was as follows, with helper methods omitted:",
            "7. ```java\npublic class IntList {\n    public int first;\n    public IntList rest;\n\n    public IntList(int f, IntList r) {\n        first = f;\n        rest = r;\n    }\n...\n```",
            "8. Our first step will be to simply rename everything and throw away the helper methods. This probably doesn't seem like progress, but trust me, I'm a professional.",
            "9. ```java\npublic class IntNode {\n    public int item;\n    public IntNode next;\n\n    public IntNode(int i, IntNode n) {\n        item = i;\n        next = n;\n    }\n}\n```",
            '10. #### Improvement #2: Bureaucracy <a href="#improvement-2-bureaucracy" id="improvement-2-bureaucracy"></a>',
            "11. Knowing that `IntNodes` are hard to work with, we're going to create a separate class called `SLList` that the user will interact with. The basic class is simply:",
            "12. ```java\npublic class SLList {\n    public IntNode first;\n\n    public SLList(int x) {\n        first = new IntNode(x, null);\n    }\n}\n```",
            "13. Already, we can get a vague sense of why a `SLList` is better. Compare the creation of an `IntList` of one item to the creation of a `SLList` of one item.",
            "14. ```java\nIntList L1 = new IntList(5, null);\nSLList L2  = new SLList(5);\n```",
            "15. The `SLList` hides the detail that there exists a null link from the user. The `SLList` class isn't very useful yet, so let's add an `addFirst` and `getFirst` method as simple warmup methods. Consider trying to write them yourself before reading on.",
            '16. #### addFirst and getFirst <a href="#addfirst-and-getfirst" id="addfirst-and-getfirst"></a>',
            "17. `addFirst` is relatively straightforward if you understood chapter 2.1. With `IntLists`, we added to the front with the line of code `L = new IntList(5, L)`. Thus, we end up with:",
            "18. ```java\npublic class SLList {\n    public IntNode first;\n\n    public SLList(int x) {\n        first = new IntNode(x, null);\n    }\n\n    /** Adds an item to the front of the list. */\n    public void addFirst(int x) {\n        first = new IntNode(x, first);\n    }\n}\n```",
            "19. `getFirst` is even easier. We simply return `first.item`:",
            "20. ```java\n/** Retrieves the front item from the list. */\npublic int getFirst() {\n    return first.item;\n}\n```",
            "21. The resulting `SLList` class is much easier to use. Compare:",
            "22. ```java\nSLList L = new SLList(15);\nL.addFirst(10);\nL.addFirst(5);\nint x = L.getFirst();\n```",
            "23. to the `IntList` equivalent:",
            "24. ```java\nIntList L = new IntList(15, null);\nL = new IntList(10, L);\nL = new IntList(5, L);\nint x = L.first;\n```",
            "25. Comparing the two data structures visually, we have: (with the `IntList` version on top and `SLList` version below it)",
            "26. ![IntList\\_vs\\_SLList.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig22/IntList\\_vs\\_SLList.png)",
            "27. Essentially, the `SLList` class acts as a middleman between the list user and the naked recursive data structure. As suggested above in the `IntList` version, there is a potentially undesireable possibility for the `IntList` user to have variables that point to the middle of the `IntList`. As Ovid said: [Mortals cannot look upon a god without dying](https://en.wikipedia.org/wiki/Semele), so perhaps it is best that the `SLList` is there to act as our intermediary.",
            "28. **Exercise 2.2.1**: The curious reader might object and say that the `IntList` would be just as easy to use if we simply wrote an `addFirst` method. Try to write an `addFirst` method to the `IntList` class. You'll find that the resulting method is tricky as well as inefficient.",
            '29. #### Improvement #3: Public vs. Private <a href="#improvement-3-public-vs-private" id="improvement-3-public-vs-private"></a>',
            "30. Unfortunately, our `SLList` can be bypassed and the raw power of our naked data structure (with all its dangers) can be accessed. A programmer can easily modify the list directly, without going through the kid-tested, mother-approved `addFirst` method, for example:",
            "31. ```java\nSLList L = new SLList(15);\nL.addFirst(10);\nL.first.next.next = L.first.next;\n```",
            "32. ![bad\\_SLList.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig22/bad\\_SLList.png)",
            "33. This results in a malformed list with an infinite loop. To deal with this problem, we can modify the `SLList` class so that the `first` variable is declared with the `private` keyword.",
            "34. ```java\npublic class SLList {\n    private IntNode first;\n...\n```",
            "35. Private variables and methods can only be accessed by code inside the same `.java` file, e.g. in this case `SLList.java`. That means that a class like `SLLTroubleMaker` below will fail to compile, yielding a `first has private access in SLList` error.",
            "36. ```java\npublic class SLLTroubleMaker {\n    public static void main(String[] args) {\n        SLList L = new SLList(15);\n        L.addFirst(10);\n        L.first.next.next = L.first.next;\n    }\n}\n```",
            "37. By contrast, any code inside the `SLList.java` file will be able to access the `first` variable.",
            "38. It may seem a little silly to restrict access. After all, the only thing that the `private` keyword does is break programs that otherwise compile. However, in large software engineering projects, the `private` keyword is an invaluable signal that certain pieces of code should be ignored (and thus need not be understood) by the end user. Likewise, the `public` keyword should be thought of as a declaration that a method is available and will work **forever** exactly as it does now.",
            "39. As an analogy, a car has certain `public` features, e.g. the accelerator and brake pedals. Under the hood, there are `private` details about how these operate. In a gas powered car, the accelerator pedal might control some sort of fuel injection system, and in a battery powered car, it may adjust the amount of battery power being delivered to the motor. While the private details may vary from car to car, we expect the same behavior from all accelerator pedals. Changing these would cause great consternation from users, and quite possibly terrible accidents.",
            "40. **When you create a `public` member (i.e. method or variable), be careful, because you're effectively committing to supporting that member's behavior exactly as it is now, forever.**",
            '41. #### Improvement #4: Nested Classes <a href="#improvement-4-nested-classes" id="improvement-4-nested-classes"></a>',
            "42. At the moment, we have two `.java` files: `IntNode` and `SLList`. However, the `IntNode` is really just a supporting character in the story of `SLList`.",
            "43. Java provides us with the ability to embed a class declaration inside of another for just this situation. The syntax is straightforward and intuitive:",
            "44. ```java\npublic class SLList {\n       public class IntNode {\n            public int item;\n            public IntNode next;\n            public IntNode(int i, IntNode n) {\n                item = i;\n                next = n;\n            }\n       }\n\n       private IntNode first; \n\n       public SLList(int x) {\n           first = new IntNode(x, null);\n       } \n...\n```",
            "45. Having a nested class has no meaningful effect on code performance, and is simply a tool for keeping code organized. For more on nested classes, see [Oracle's official documentation](https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html).",
            "46. If the nested class has no need to use any of the instance methods or variables of `SLList`, you may declare the nested class `static`, as follows. Declaring a nested class as `static` means that methods inside the static class can not access any of the members of the enclosing class. In this case, it means that no method in `IntNode` would be able to access `first`, `addFirst`, or `getFirst`.",
            "47. ```java\npublic class SLList {\n       public static class IntNode {\n            public int item;\n            public IntNode next;\n            public IntNode(int i, IntNode n) {\n                item = i;\n                next = n;\n            }\n       }\n\n       private IntNode first;\n...\n```",
            "48. This saves a bit of memory, because each `IntNode` no longer needs to keep track of how to access its enclosing `SLList`.",
            "49. Put another way, if you examine the code above, you'll see that the `IntNode` class never uses the `first` variable of `SLList`, nor any of `SLList`'s methods. As a result, we can use the static keyword, which means the `IntNode` class doesn't get a reference to its boss, saving us a small amount of memory.",
            "50. If this seems a bit technical and hard to follow, try Exercise 2.2.2. A simple rule of thumb is that _if you don't use any instance members of the outer class, make the nested class static_.",
            "51. **Exercise 2.2.2** Delete the word `static` as few times as possible so that [this program](https://joshhug.gitbooks.io/hug61b/content/chap2/exercises/Government.java) compiles (Refresh the page after clicking the link and making sure the url changed). Make sure to read the comments at the top before doing the exercise.",
            '52. #### addLast() and size() <a href="#addlast-and-size" id="addlast-and-size"></a>',
            "53. To motivate our remaining improvements and also demonstrate some common patterns in data structure implementation, we'll add `addLast(int x)` and `size()` methods. You're encouraged to take the [starter code](https://github.com/Berkeley-CS61B/lectureCode/blob/master/lists2/DIY/addLastAndSize/SLList.java) and try it yourself before reading on. I especially encourage you to try to write a recursive implementation of `size`, which will yield an interesting challenge.",
            "54. I'll implement the `addLast` method iteratively, though you could also do it recursively. The idea is fairly straightforward, we create a pointer variable `p` and have it iterate through the list to the end.",
            "55. ```java\n/** Adds an item to the end of the list. */\npublic void addLast(int x) {\n    IntNode p = first;\n\n    /* Advance p to the end of the list. */\n    while (p.next != null) {\n        p = p.next;\n    }\n    p.next = new IntNode(x, null);\n}\n```",
            "56. By contrast, I'll implement `size` recursively. This method will be somewhat similar to the `size` method we implemented in section [2.1](https://joshhug.gitbooks.io/hug61b/content/chap2/chap21.html) for `IntList`.",
            "57. The recursive call for `size` in `IntList` was straightforward: `return 1 + this.rest.size()`. For a `SLList`, this approach does not make sense. A `SLList` has no `rest` variable. Instead, we'll use a common pattern that is used with middleman classes like `SLList` -- we'll create a private helper method that interacts with the underlying naked recursive data structure.",
            "58. This yields a method like the following:",
            "59. ```java\n/** Returns the size of the list starting at IntNode p. */\nprivate static int size(IntNode p) {\n    if (p.next == null) {\n        return 1;\n    }\n\n    return 1 + size(p.next);\n}\n```",
            "60. Using this method, we can easily compute the size of the entire list:",
            "61. ```java\npublic int size() {\n    return size(first);\n}\n```",
            "62. Here, we have two methods, both named `size`. This is allowed in Java, since they have different parameters. We say that two methods with the same name but different signatures are **overloaded**. For more on overloaded methods, see Java's [official documentation](https://docs.oracle.com/javase/tutorial/java/javaOO/methods.html).",
            "63. An alternate approach is to create a non-static helper method in the `IntNode` class itself. Either approach is fine, though I personally prefer not having any methods in the `IntNode` class.",
            '64. ## Improvement #5: Caching <a href="#improvement-5-caching" id="improvement-5-caching"></a>',
            "65. Consider the `size` method we wrote above. Suppose `size` takes 2 seconds on a list of size 1,000. We expect that on a list of size 1,000,000, the `size` method will take 2,000 seconds, since the computer has to step through 1,000 times as many items in the list to reach the end. Having a `size` method that is very slow for large lists is unacceptable, since we can do better.",
            "66. It is possible to rewrite `size` so that it takes the same amount of time, no matter how large the list.",
            "67. To do so, we can simply add a `size` variable to the `SLList` class that tracks the current size, yielding the code below. This practice of saving important data to speed up retrieval is sometimes known as **caching**.",
            "68. ```java\npublic class SLList {\n    ... /* IntNode declaration omitted. */\n    private IntNode first;\n    private int size;\n\n    public SLList(int x) {\n        first = new IntNode(x, null);\n        size = 1;\n    }\n\n    public void addFirst(int x) {\n        first = new IntNode(x, first);\n        size += 1;\n    }\n\n    public int size() {\n        return size;\n    }\n    ...\n}\n```",
            "69. This modification makes our `size` method incredibly fast, no matter how large the list. Of course, it will also slow down our `addFirst` and `addLast` methods, and also increase the memory of usage of our class, but only by a trivial amount. In this case, the tradeoff is clearly in favor of creating a cache for size.",
            '70. #### Improvement #6: The Empty List <a href="#improvement-6-the-empty-list" id="improvement-6-the-empty-list"></a>',
            "71. Our `SLList` has a number of benefits over the simple `IntList` from chapter 2.1:",
            "72. * Users of a `SLList` never see the `IntList` class.\n  * Simpler to use.\n  * More efficient `addFirst` method (exercise 2.2.1).\n  * Avoids errors or malfeasance by `IntList` users.\n* Faster `size` method than possible with `IntList`.",
            "73. Another natural advantage is that we will be able to easily implement a constructor that creates an empty list. The most natural way is to set `first` to `null` if the list is empty. This yields the constructor below:",
            "74. ```java\npublic SLList() {\n    first = null;\n    size = 0;\n}\n```",
            "75. Unfortunately, this causes our `addLast` method to crash if we insert into an empty list. Since `first` is `null`, the attempt to access `p.next` in `while (p.next != null)` below causes a null pointer exception.",
            "76. ```java\npublic void addLast(int x) {\n    size += 1;\n    IntNode p = first;\n    while (p.next != null) {\n        p = p.next;\n    }\n\n    p.next = new IntNode(x, null);\n}\n```",
            "77. **Exercise 2.2.3** Fix the `addLast` method. Starter code [here](https://github.com/Berkeley-CS61B/lectureCode/blob/master/lists2/DIY/fixAddLast/SLList.java).",
            '78. #### Improvement #6b: Sentinel Nodes <a href="#improvement-6b-sentinel-nodes" id="improvement-6b-sentinel-nodes"></a>',
            "79. One solution to fix `addLast` is to create a special case for the empty list, as shown below:",
            "80. ```java\npublic void addLast(int x) {\n    size += 1;\n\n    if (first == null) {\n        first = new IntNode(x, null);\n        return;\n    }\n\n    IntNode p = first;\n    while (p.next != null) {\n        p = p.next;\n    }\n\n    p.next = new IntNode(x, null);\n}\n```",
            "81. This solution works, but special case code like that shown above should be avoided when necessary. Human beings only have so much working memory, and thus we want to keep complexity under control wherever possible. For a simple data structure like the `SLList`, the number of special cases is small. More complicated data structures like trees can get much, much uglier.",
            '82. A cleaner, though less obvious solution, is to make it so that all `SLLists` are the "same", even if they are empty. We can do this by creating a special node that is always there, which we will call a **sentinel node**. The sentinel node will hold a value, which we won\'t care about.',
            "83. For example, the empty list created by `SLList L = new SLList()` would be as shown below:",
            "84. ![empty\\_sentinelized\\_SLList.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig22/empty\\_sentinelized\\_SLList.png)",
            "85. And a `SLList` with the items 5, 10, and 15 would look like:",
            "86. ![three\\_item\\_sentenlized\\_SLList.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig22/three\\_item\\_sentenlized\\_SLList.png)",
            "87. In the figures above, the lavender ?? value indicates that we don't care what value is there. Since Java does not allow us to fill in an integer with question marks, we just pick some abitrary value like -518273 or 63 or anything else.",
            "88. Since a `SLList` without a sentinel has no special cases, we can simply delete the special case from our `addLast` method, yielding:",
            "89. ```java\npublic void addLast(int x) {\n    size += 1;\n    IntNode p = sentinel;\n    while (p.next != null) {\n        p = p.next;\n    }\n\n    p.next = new IntNode(x, null);\n}\n```",
            "90. As you can see, this code is much much cleaner!",
            '91. #### Invariants <a href="#invariants" id="invariants"></a>',
            "92. An invariant is a fact about a data structure that is guaranteed to be true (assuming there are no bugs in your code).",
            "93. A `SLList` with a sentinel node has at least the following invariants:",
            "94. * The `sentinel` reference always points to a sentinel node.\n* The front item (if it exists), is always at `sentinel.next.item`.\n* The `size` variable is always the total number of items that have been added.",
            "95. Invariants make it easier to reason about code, and also give you specific goals to strive for in making sure your code works.",
            "96. A true understanding of how convenient sentinels are will require you to really dig in and do some implementation of your own. You'll get plenty of practice in Project 1. However, we recommend that you wait until after you've finished the next section of this book before beginning Project 1.\n",
        ],
        "9.-inheritance-i-interface-and-implementation-inheritance.md": [
            "1. # 9. Inheritance I: Interface and Implementation Inheritance",
            '2. ### The Problem <a href="#the-problem" id="the-problem"></a>',
            "3. Recall the two list classes we created last week: SLList and AList. If you take a look at their documentation, you'll notice that they are very similar. In fact, all of their supporting methods are the same!",
            "4. Suppose we want to write a class `WordUtils`that includes functions we can run on lists of words, including a method that calculates the longest string in an SLList.",
            "5. **Exercise 4.1.1.** Try writing this method by yourself. The method should take in an SLList of strings and return the longest string in the list.",
            "6. Here is the method that we came up with.",
            "7. ```java\npublic static String longest(SLList<String> list) {\n    int maxDex = 0;\n    for (int i = 0; i < list.size(); i += 1) {\n        String longestString = list.get(maxDex);\n        String thisString = list.get(i);\n        if (thisString.length() > longestString.length()) {\n            maxDex = i;\n        }\n    }\n    return list.get(maxDex);\n}\n```",
            "8. How do we make this method work for AList as well?",
            "9. All we really have to do is change the method's signature: the parameter",
            "10. ```java\nSLList<String> list\n```",
            "11. should be changed to",
            "12. ```java\nAList<String> list\n```",
            "13. Now we have two methods in our `WordUtils` class with exactly the same method name.",
            "14. ```java\npublic static String longest(SLList<String> list)\n```",
            "15. and",
            "16. ```java\npublic static String longest(AList<String> list)\n```",
            "17. This is actually allowed in Java! It's something called _method overloading_. When you call WordUtils.longest, Java knows which one to run according to what kind of parameter you supply it. If you supply it with an AList, it will call the AList method. Same with an SLList.",
            "18. It's nice that Java is smart enough to know how to deal with two of the same methods for different types, but overloading has several downsides:",
            "19. * It's super repetitive and ugly, because you now have two virtually identical blocks of code.\n* It's more code to maintain, meaning if you want to make a small change to the `longest` method such as correcting a bug, you need to change it in the method for each type of list.\n* If we want to make more list types, we would have to copy the method for every new list class.",
            '20. ### Hypernyms, Hyponyms, and Interface Inheritance <a href="#hypernyms-hyponyms-and-interface-inheritance" id="hypernyms-hyponyms-and-interface-inheritance"></a>',
            "21. In the English language and life in general, there exist logical hierarchies to words and objects.",
            "22. Dog is what is called a _hypernym of_ poodle, malamute, husky, etc. In the reverse direction, poodle, malamute, and husky, are _hyponyms_ of dog.",
            '23. These words form a hierarchy of "is-a" relationships:',
            '24. * a poodle "is-a" dog\n* a dog "is-a" canine\n* a canine "is-a" carnivore\n* a carnivore "is-an" animal',
            "25. ![hierarchy](https://joshhug.gitbooks.io/hug61b/content/assets/hierarchy.png)",
            "26. The same hierarchy goes for SLLists and ALists! SLList and AList are both hyponyms of a more general list.",
            "27. We will formalize this relationship in Java: if a SLList is a hyponym of List61B, then the SLList class is a **subclass** of the List61B class and the List61B class is a **superclass** of the SLList class.",
            "28. **Figure 4.1.1** ![subclass](https://joshhug.gitbooks.io/hug61b/content/assets/subclass.png)",
            "29. In Java, in order to _express_ this hierarchy, we need to do **two things**:",
            "30. * Step 1: Define a type for the general list hypernym -- we will choose the name List61B.\n* Step 2: Specify that SLList and AList are hyponyms of that type.",
            "31. The new List61B is what Java calls an **interface**. It is essentially a contract that specifies what a list must be able to do, but it doesn't provide any implementation for those behaviors. Can you think of why?",
            "32. Here is our List61B interface. At this point, we have satisfied the first step in establishing the relationship hierarchy: creating a hypernym.",
            "33. ```java\npublic interface List61B<Item> {\n    public void addFirst(Item x);\n    public void add Last(Item y);\n    public Item getFirst();\n    public Item getLast();\n    public Item removeLast();\n    public Item get(int i);\n    public void insert(Item x, int position);\n    public int size();\n}\n```",
            "34. Now, to complete step 2, we need to specify that AList and SLList are hyponyms of the List61B class. In Java, we define this relationship in the class definition.",
            "35. We will add to",
            "36. `public class AList<Item> {...}`",
            "37. a relationship-defining word: implements.",
            "38. `public class AList<Item> implements List61B<Item>{...}`",
            '39. `implements List61B<Item>` is essentially a promise. AList is saying "I promise I will have and define all the attributes and behaviors specified in the List61B interface"',
            '40. Now we can edit our `longest` method in `WordUtils` to take in a List61B. Because AList and SLList share an "is-a" relationship.',
            '41. ### Overriding <a href="#overriding" id="overriding"></a>',
            "42. We promised we would implement the methods specified in List61B in the AList and SLList classes, so let's go ahead and do that.",
            "43. When implementing the required functions in the subclass, it's useful (and actually required in 61B) to include the `@Override` tag right on top of the method signature. Here, we have done that for just one method.",
            "44. ```java\n@Override\npublic void addFirst(Item x) {\n    insert(x, 0);\n}\n```",
            "45. It is good to note that even if you don\u2019t include this tag, you _are_ still overriding the method. So technically, you don't _have_ to include it. However, including the tag acts as a safeguard for you as the programmer by alerting the compiler that you intend to override this method. Why would this be helpful you ask? Well, it's kind of like having a proofreader! The compiler will tell you if something goes wrong in the process.",
            "46. Say you want to override the `addLast` method. What if you make a typo and accidentally write `addLsat`? If you don't include the @Override tag, then you might not catch the mistake, which could make debugging a more difficult and painful process. Whereas if you include @Override, the compiler will stop and prompt you to fix your mistakes before your program even runs.",
            '47. ### Interface Inheritance <a href="#interface-inheritance" id="interface-inheritance"></a>',
            "48. Interface Inheritance refers to a relationship in which a subclass inherits all the methods/behaviors of the superclass. As in the List61B class we defined in the **Hyponyms and Hypernyms** section, the interface includes all the method signatures, but not implementations. It's up to the subclass to actually provide those implementations.",
            "49. This inheritance is also multi-generational. This means if we have a long lineage of superclass/subclass relationships like in **Figure 4.1.1**, AList not only inherits the methods from List61B but also every other class above it all the way to the highest superclass AKA AList inherits from Collection.",
            '50. ## GRoE <a href="#groe" id="groe"></a>',
            "51. Recall the Golden Rule of Equals we introduced in the first chapter. This means whenever we make an assignment `a = b` , we copy the bits from b into a, with the requirement that b is the same type as a. You can't assign `Dog b = 1` or `Dog b = new Cat()` because 1 is not a Dog and neither is Cat.",
            "52. Let's try to apply this rule to the `longest` method we wrote previously in this chapter.",
            '53. `public static String longest(List61B<String> list)` takes in a List61B. We said that this could take in AList and SLList as well, but how is that possible since AList and List61B are different classes? Well, recall that AList shares an "is-a" relationship with List61B, Which means an AList should be able to fit into a List61B box!',
            "54. **Exercise 4.1.2** Do you think the code below will compile? If so, what happens when it runs?",
            '55. ```java\npublic static void main(String[] args) {\n    List61B<String> someList = new SLList<String>();\n    someList.addFirst("elk");\n}\n```',
            "56. Here are possible answers:",
            '57. * Will not compile.\n* Will compile, but will cause an error on the **new** line\n* When it runs, an SLList is created and its address is stored in the someList variable, but it crashes on someList.addFirst() since the List class doesn\'t implement addFirst;\n* When it runs, and SLList is created and its address is stored in the someList variable. Then the string "elk" is inserted into the SLList referred to by addFirst.',
            '58. ### Implementation Inheritance <a href="#implementation-inheritance" id="implementation-inheritance"></a>',
            "59. Previously, we had an interface List61B that only had method headers identifying **what** List61B's should do. But, now we will see that we can write methods in List61B that already have their implementation filled out. These methods identify **how** hypernyms of List61B should behave.",
            "60. In order to do this, you must include the `default` keyword in the method signature.",
            "61. If we define this method in List61B",
            '62. ```java\ndefault public void print() {\n    for (int i = 0; i < size(); i += 1) {\n        System.out.print(get(i) + " ");\n    }\n    System.out.println();\n}\n```',
            "63. Then everything that implements the List61B class can use the method!",
            "64. However, there is one small inefficiency in this method. Can you catch it?",
            "65. For an SLList, the `get` method needs to jump through the entirety of the list. during each call. It's much better to just print while jumping through!",
            "66. We want SLList to print a different way than the way specified in its interface. To do this, we need to override it. In SLList, we implement this method;",
            '67. ```java\n@Override\npublic void print() {\n    for (Node p = sentinel.next; p != null; p = p.next) {\n        System.out.print(p.item + " ");\n    }\n}\n```',
            "68. Now, whenever we call print() on an SLList, it will call this method instead of the one in List61B.",
            "69. You may be wondering, how does Java know which print() to call? Good question. Java is able to do this due to something called **dynamic method selection**.",
            "70. We know that variables in java have a type.",
            "71. `List61B<String> lst = new SLList<String>();`",
            '72. In the above declaration and instantiation, lst is of type "List61B". This is called the "static type"',
            '73. However, the objects themselves have types as well. the object that lst points to is of type SLList. Although this object is intrinsically an SLList (since it was declared as such), it is also a List61B, because of the \u201cis-a\u201d relationship we explored earlier. But, because the object itself was instantiated using the SLList constructor, We call this its "dynamic type".',
            "74. Aside: the name \u201cdynamic type\u201d is actually quite semantic in its origin! Should lst be reassigned to point to an object of another type, say a AList object, lst\u2019s dynamic type would now be AList and not SLList! It\u2019s dynamic because it changes based on the type of the object it\u2019s currently referring to.",
            "75. When Java runs a method that is overriden, it searches for the appropriate method signature in it's **dynamic type** and runs it.",
            "76. **IMPORTANT: This does not work for overloaded methods!**",
            "77. Say there are two methods in the same class",
            "78. ```java\npublic static void peek(List61B<String> list) {\n    System.out.println(list.getLast());\n}\npublic static void peek(SLList<String> list) {\n    System.out.println(list.getFirst());\n}\n```",
            "79. and you run this code",
            '80. ```java\nSLList<String> SP = new SLList<String>();\nList61B<String> LP = SP;\nSP.addLast("elk");\nSP.addLast("are");\nSP.addLast("cool");\npeek(SP);\npeek(LP);\n```',
            "81. The first call to peek() will use the second peek method that takes in an SLList. The second call to peek() will use the first peek method which takes in a List61B. This is because the only distinction between two overloaded methods is the types of the parameters. When Java checks to see which method to call, it checks the **static type** and calls the method with the parameter of the same type.",
            '82. ### Interface Inheritance vs Implementation Inheritance <a href="#interface-inheritance-vs-implementation-inheritance" id="interface-inheritance-vs-implementation-inheritance"></a>',
            '83. How do we differentiate between "interface inheritance" and "implementation inheritance"? Well, you can use this simple distinction:',
            "84. * Interface inheritance (what): Simply tells what the subclasses should be able to do.\n  * EX) all lists should be able to print themselves, how they do it is up to them.\n* Implementation inheritance (how): Tells the subclasses how they should behave.\n  * EX) Lists should print themselves exactly this way: by getting each element in order and then printing them.",
            '85. When you are creating these hierarchies, remember that the relationship between a subclass and a superclass should be an "is-a" relationship. AKA Cat should only implement Animal Cat **is an** Animal. You should not be defining them using a "has-a" relationship. Cat **has-a** Claw, but Cat definitely should not be implementing Claw.',
            "86. Finally, Implementation inheritance may sound nice and all but there are some drawbacks:",
            "87. * We are fallible humans, and we can't keep track of everything, so it's possible that you overrode a method but forgot you did.\n* It may be hard to resolve conflicts in case two interfaces give conflicting default methods.\n* It encourages overly complex code.\n",
        ],
        "24.-shortest-paths/24.1-introduction.md": [
            "1. # 24.1 Introduction",
            '2. ## Shortest Paths <a href="#shortest-paths" id="shortest-paths"></a>',
            '3. ### Recalls <a href="#recalls" id="recalls"></a>',
            "4. So far, we have methods to do the following",
            "5. * find a path from a given vertex, `s`, to every reachable vertex in the graph.\n* find a **shortest** path from a given vertex, `s`, to every reachable vertex in the graph. (...or do we?)",
            "6. Before we answer the mysterious question posed below, let's further recall the two types of searches we could use to do the above two things: BFS or DFS.",
            "7. Are both going to always be correct? Yes. Does one give better results? BFS finds you the **shortest** paths whereas DFS does not. Is one more efficient than the other, runtime-wise? No. Is one more efficient than the other, space-wise?",
            '8. * DFS is worse for spindly graphs. Imagine a graph with 10000 nodes all spindly. We\'ll end up making 10000 recursive calls, which is bad for space.\n* BFS is worse for "bushy" graphs, because our queue gets used a lot.',
            '9. ### Answering the mysterious question <a href="#answering-the-mysterious-question" id="answering-the-mysterious-question"></a>',
            "10. Did we develop an algorithm to find the **shortest** path from a given vertex to every other reachable vertex? Well, kind of. We developed an algorithm that works well on graphs with no edge labels. Here's what we did: we developed an algorithm that finds us the shortest (**where shortest means the fewest number of edges**) paths from a given source vertex.",
            "11. But that's not always the correct definition of shortest. Sometimes, our graph edges might have 'weights', and A-B is considered farther than A-C if the A-B edge has weight, say, 5 and the A-C edge only has weight, say, 3.",
            "12. That is why we need a different algorithm for finding the shortest path when we have the edge weights in the graph.\n",
        ],
        "24.-shortest-paths/README.md": [
            "1. ---\ndescription: 'By: Mihir Mirchandani and Teresa Luo'\n---",
            "2. # 24. Shortest Paths",
            "3. ",
        ],
        "24.-shortest-paths/24.2-dijkstras-algorithm.md": [
            "1. # 24.2 Dijkstra's Algorithm",
            '2. {% embed url="https://www.youtube.com/watch?t=303s&v=iMoFtG1md3w" %}\nProfessor Hug\'s Lecture on Shortest Paths\n{% endembed %}',
            "3. In [Chapter 23.1](../23.-graph-traversals-and-implementations/23.1-bfs-and-dfs.md) we have implemented BFS & DFS. We discussed the idea of using BFS for finding the shortest path trees however when the graph edges have weight, BFS will upset us.&#x20;",
            "4. Consider the following example. You are on campus and playing a game with your friends around Hearst Memorial Mining Building. You start at the location `s` and you want to go to the location `t`.&#x20;",
            '5. <figure><img src="../.gitbook/assets/dj.png" alt=""><figcaption></figcaption></figure>',
            "6. BFS will yield a route of length 330 m instead of therefore we need an algorithm that takes into account edge distances, also known as \u201cedge weights\u201d.",
            "7. ",
            "8. <div>",
            '9. <figure><img src="../.gitbook/assets/image1 (1).png" alt=""><figcaption><p>Correct Result</p></figcaption></figure>',
            "10.  ",
            '11. <figure><img src="../.gitbook/assets/image2.png" alt=""><figcaption><p>BFS result</p></figcaption></figure>',
            "12. </div>",
            '13. ### Observations <a href="#observations" id="observations"></a>',
            "14. Note that the shortest path (for a graph whose edges have weights) can have many, many edges. What we care to minimize is the sum of the weights of the edges on the selected path.",
            "15. Secondly, note the fact that the shortest paths tree from a source `s` can be created in the following way:",
            '16. * For every vertex `v` (which is not `s`) in the graph, find the shortest path from `s` to `v`.\n* "Combine"/"Union" all the edges that you found above. Tada!',
            '17. Thirdly, note that the "Shortest Path Tree" will **always be a tree**. Why? Well, let\'s think about our original solution, where we maintained an edgeToedgeTo array. For every node, there was exactly one "parent" in the edgeToedgeTo array. (Why does this imply that the "Shortest Path Tree" will be a tree? Hint: A tree has `V-1` edges, where `V` is the number of nodes in the tree.)',
            "18. ",
            '19. ### Dijkstra\'s Algorithm \\[\\[/\u02c8da\u026akstr\u0259/]] <a href="#dijkstras-algorithm-da-kstr" id="dijkstras-algorithm-da-kstr"></a>',
            "20. Dijkstra's algorithm takes in an input vertex \ufffds, and outputs the shortest path tree from \ufffds. How does it work?",
            "21. 1. Create a priority queue.\n2. Add `s` to the priority queue with priority 00. Add all other vertices to the priority queue with priority \u221e\u221e.\n3. While the priority queue is not empty: pop a vertex out of the priority queue, and **relax** all of the edges going out from the vertex.",
            '22. ### What does it mean to **relax**? <a href="#what-does-it-mean-to-relax" id="what-does-it-mean-to-relax"></a>',
            "23. Suppose the vertex we just popped from the priority queue was `v`. We'll look at all of `v`'s edges. Say, we're looking at edge (`v`,`w`) (the edge that goes from `v` to `w`). We're going to try and relax this edge.",
            "24. What that means is: Look at your current best distance to `w` from the source, call it **curBestDistToW**. Now, look at your **curBestDistToV**+weight(`v`,`w`) (let's call it **potentialDistToWUsing**).",
            "25. Is **potentialDistToWUsing** **better, i.e., smaller** than **curBestDistToW**? In that case, set **curBestDistToW=potentialDistToWUsingV**, and update the **edgeTo\\[`w`]** to be `v`.",
            "26. **Important note: we never relax edges that point to already visited vertices.**",
            "27. This whole process of calculating the potential distance, checking if it's better, and potentially updating is called relaxing.",
            "28. ",
            '29. ### Pseudocode <a href="#pseudocode" id="pseudocode"></a>',
            "30. ```\ndef dijkstras(source):\n    PQ.add(source, 0)\n    For all other vertices, v, PQ.add(v, infinity)\n    while PQ is not empty:\n        p = PQ.removeSmallest()\n        relax(all edges from p)\n```",
            "31. ```\ndef relax(edge p,q):\n   if q is visited (i.e., q is not in PQ):\n       return\n\n   if distTo[p] + weight(edge) < distTo[q]:\n       distTo[q] = distTo[p] + w\n       edgeTo[q] = p\n       PQ.changePriority(q, distTo[q])\n```",
            "32. Guarantees",
            "33. As long as the edges are all non-negative, Dijkstra's is guaranteed to be optimal.",
            '34. ### Proofs and Intuitions <a href="#proofs-and-intuitions" id="proofs-and-intuitions"></a>',
            "35. Assume all edges are non-negative.",
            "36. * At start, distTo\\[source] = 0. This is optimal.\n* After relaxing all edges from source, let vertex $$v_1$$ be the vertex with the minimum weight (i.e., the one that's closest to the source.) **Claim: distTo\\[**$$v_1$$**\u200b\u200b] is optimal, i.e., whatever the value of distTo\\[**$$v_1$$**\u200b\u200b] is at this point is the shortest distance from** $$s$$ **to** $$v_1$$**\u200b\u200b**. Why?\n  * Let's try to see why this **MUST** be the case.\n  *   Suppose that it isn't the case. Then that means that there is some other path from $$s$$ to $$v_1$$",
            "37.       which is shorter than the direct path ($$s$$,\u200b\u200b $$v_1$$). Ok, so let's consider this hypothetical cool shorter path... it would have to look like ($$s$$, $$v_a$$\u200b\u200b, $$v_b$$\u200b\u200b,\u2026, $$v_1$$\u200b\u200b). But... ($$s$$, $$v_a$$) is **already** bigger than ($$s$$, $$v_1$$). (Note that this is true because $$v_1$$ is the vertex that is closest to $$s$$ from above.) So how can such a path exist which is actually shorter? It can't!\n* Now, the next vertex to be popped will be $$v_1$$\u200b\u200b. (Why? Note that it currently has the lowest priority in the PQ!)\n* So now, we can make this same argument for $$v_1$$\u200b\u200b and all the relaxation it does. (This is called \"proof by induction\". It's kind of like recursion for proofs.) And that's it; we're done.",
            '38. ### Negative Edges? <a href="#negative-edges" id="negative-edges"></a>',
            "39. Things can go pretty badly when negative edges come into the picture. Consider the following image.",
            '40. <figure><img src="../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>',
            "41. Suppose you're at that vertex labeled 34. Now you're going to try to relax all your edges. You have only one outgoing edge from yourself to 33 with weight \u221267. Ah, but note: vertex 33 is already visited (it's marked with white.) So... we don't relax it. (Recall the pseudocode for the relax method.)",
            "42. Now we go home thinking that the shortest distance to 33 is 82 (marked in pink.) But really, we should have taken the path **through** 34 because that would have given us a distance of 101\u221267=34. Oops.",
            '43. <figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>',
            "44. **Dijkstra's algorithm is not guaranteed to be correct for negative edges. It might work... but it isn't guaranteed to work.**",
            "45. Try this out: suppose that your graph has negative edges, but all the negative edges only go out of the source vertex `s` that you were passed in. Does Dijkstra's work? Why / Why not?",
            '46. ### A noteworthy invariant <a href="#a-noteworthy-invariant" id="a-noteworthy-invariant"></a>',
            "47. Observe that once a vertex is popped off the priority queue, it is never re-added. Its distance is never re-updated. So, in other words, once a vertex is popped from the priority queue, we **know** the true shortest distance to that vertex from the source.",
            "48. One nice consequence of this fact is \"short-circuiting\". Suppose... that I didn't care about the shortest-paths tree, but just wanted to find the shortest path from some source to some other target. Suppose that you wanted to take, like, the cities of the world on a graph, and find the shortest path from Berkeley to Oakland. Running `dijkstra(Berkeley)` will mean that you can't actually stop this powerful beast of an algorithm... you have to let it run... till it finds the shortest path to LA, and Houston, and New York City, and everywhere possible!",
            "49. Well. Once `Oakland` is popped off the priority queue in the algorithm, we can just stop. We can just return the distance and the path we have at that point, and it will be correct. So **sometimes** `dijkstra` takes in not only a source, but also a target. This is for the purposes of short-circuiting.",
            "50. ",
            "51. ### [Demo](https://docs.google.com/presentation/d/1\\_bw2z1ggUkquPdhl7gwdVBoTaoJmaZdpkV6MoAgxlJc/pub?start=false\\&loop=false\\&delayms=3000\\&slide=id.g771336078\\_0\\_180)",
            "52. \\\n\\\n",
        ],
        "24.-shortest-paths/24.4-summary.md": [
            "1. # 24.4 Summary",
            "2. **Dijktra\u2019s Algorithm and Single-Source Shortest Paths.** Suppose we want to record the shortest paths from some source to every single other vertex (so that we can rapidly found a route from s to X, from s to Y, and so forth). We already know how to do this if we\u2019re only counting the number of edges, we just use BFS.",
            "3. But if edges have weights (representing, for example road lengths), we have to do something else. It turns out that even considering edge weights, we can preprocess the shortest route from the source to every vertex very efficiently. We store the answer as a \u201cshortest paths tree\u201d. Typically, a shortest paths tree is stored as an array of edgeTo\\[] values (and optionally distTo\\[] values if we want a constant time distTo() operation).",
            "4. To find the SPT, we can use Dijkstra\u2019s algorithm, which is quite simple once you understand it. Essentially, we visit each vertex in order of its distance from the source, where each visit consists of relaxing every edge. Informally, relaxing an edge means using it if its better than the best known distance to the target vertex, otherwise ignoring it. Or in pseudocode:",
            "5. ```\nDijkstra(G, s):\n    while not every vertex has been visited:\n        visit(unmarked vertex v for which distTo(v) is minimized)\n```",
            "6. Where visit is given by the following pseudocode:",
            "7. ```\nvisit(v):\n    mark(v)\n    for each edge e of s:\n        relax(e)\n```",
            "8. And finally, relax is given by:",
            "9. ```\nrelax(e):\n    v = e.source\n    w = e.target        \n    currentBestKnownWeight = distTo(w)\n    possiblyBetterWeight = distTo(v) + e.weight\n    if possiblyBetterWeight < currentBestKnownWeight\n        Use e instead of whatever we were using before\n```",
            "10. Runtime is $$O(V * logV + V * log V + E * logV)$$, and since $$E > V$$ for any graph we\u2019d run Dijkstra\u2019s algorithm on, this can be written as more simply O(E log V). See slides for runtime description.",
            "11. **A\\* Single-Target Shortest Paths.** If we need only the path to a single target, then Dijkstra\u2019s is inefficient as it explores many many edges that we don\u2019t care about (e.g. when routing from Denver to NYC, we\u2019d explore everything within more than a thousand miles in all directions before reaching NYC).",
            "12. To fix this, we make a very minor change to Dijkstra\u2019s, where instead of visiting vertices in order of distance from the source, we visit them in order of distance from the source + h(v), where h(v) is some heuristic.",
            "13. Or in pseudocode:",
            "14. ```\nA*(G, s):\n    while not every vertex has been visited:\n        visit(unmarked vertex v for which distTo(v) + h(v) is minimized)\n```",
            "15. It turns out (but we did not prove), that as long as h(v) is less than the true distance from s to v, then the result of A\\* will always be correct.",
            "16. Note: In the version in class, we did not use an explicit \u2018mark\u2019. Instead, we tossed everything in the PQ, and we effectively considered a vertex marked if it had been removed from the PQ.\n",
        ],
        "24.-shortest-paths/24.5-exercises.md": [
            "1. # 24.5 Exercises",
            "2. ## Procedural",
            "3. Questions 1 and 2, suppose we run Dijkstra's from A. Break ties alphabetically. Breaking ties alphabetically means that if we insert B into the fringe, and C is in the fringe with the same priority, we have a tie! To break this tie, we will do so alphabetically, putting B before C.",
            "4. 1. What is the order that vertices are visited? Please separate vertices with spaces, e.g. A B C ....\n2. Select all the edges in the shortest paths tree. \\*",
            '5. <figure><img src="../.gitbook/assets/image (146).png" alt=""><figcaption></figcaption></figure>',
            "6. ",
            "7. <details>",
            "8. <summary>Problem 1</summary>",
            "9. A B E C F D G",
            "10. For this, and all questions on this check in, refer to this walkthrough video for an explanation: [https://youtu.be/5MFOu8bNvd8](https://www.google.com/url?q=https://youtu.be/5MFOu8bNvd8\\&sa=D\\&source=editors\\&ust=1679291837950894\\&usg=AOvVaw3zdQsT2ly7PQQGYjid7E6m)",
            "11. </details>",
            "12. <details>",
            "13. <summary>Problem 2</summary>",
            "14. AE",
            "15. AB",
            "16. CE",
            "17. DF",
            "18. EG",
            "19. EF",
            "20. </details>",
            "21. For questions 3-5, suppose we run A\\* from A to D. Break ties alphabetically",
            "22. 3. What is the order that vertices are visited? Note that not all vertices need be visited. Please separate vertices with spaces, e.g. A B C ....\n4. Select all the edges in the shortest path from A -> D, as determined by A\\*. Note that this may not be the correct shortest path. Also note that we are NOT finding the shortest path tree, but just the edges on the shortest path!\n5. In the previous question, you probably noticed that the actual shortest path from A -> D was not found! The reason for this is because the value of one heuristic is too high. Determine which heuristic this is, and what is the maximum its value can be so that A\\* returns the correct shortest path.",
            '23. <figure><img src="../.gitbook/assets/image (28).png" alt=""><figcaption></figcaption></figure>',
            "24. <details>",
            "25. <summary>Problem 3</summary>",
            "26. A E C B D",
            "27. </details>",
            "28. <details>",
            "29. <summary>Problem 4</summary>",
            "30. AE",
            "31. CD",
            "32. CE",
            "33. </details>",
            "34. <details>",
            "35. <summary>Problem 5</summary>",
            "36. F:2",
            "37. </details>\n",
        ],
        "24.-shortest-paths/24.3-a-algorithm.md": [
            "1. # 24.3 A\\* Algorithm",
            "2. ",
            '3. {% embed url="https://www.youtube.com/watch?t=303s&v=iMoFtG1md3w" %}\nProfessor Hug\'s Lecture on Shortest Paths\n{% endembed %}',
            '4. ## A\\* <a href="#a" id="a"></a>',
            "5. We ended the section on Dijkstra's by discussing a possible way to make Dijkstra's short-circuit and just stop once it hits a given target. Is this good enough?",
            "6. To answer the above question, we need to sit down and think about how dijkstra's really works. Pictorially, Dijkstra's starts at the source node (imagine the source node being the center of a circle.) And Dijkstra's algorithm now makes concentric circles around this point, in increasing radii, and 'sweeps' these circles, capturing points.",
            "7. So... the first node Dijkstra's visits is the city closest to the source, then the city next-closest, then the city next-closest, and so on. This sounds like a good idea. What Dijkstra's is doing is first visiting all the cities that are 1-unit distance away, then 2 unit-distance away, and so on. In concentric circles.",
            "8. Now imagine the following: on a map of the US, start somewhere in the center, say, Denver. Now I want you to find me a path to New York using Dijkstra's. You'll end up traversing nodes in 'closest concentric circle' order.",
            "9. ![](https://joshhug.gitbooks.io/hug61b/content/assets/Screen%20Shot%202019-03-23%20at%208.29.56%20PM.png)",
            "10. You'll make a small circle first, just around Denver, visiting all the cities in that circle. Eventually, your circles will get bigger, and you'll make a circle that passes through Las Vegas (and would have visited, by now, all the other cities that fall within the circle.) Then, your circle will be big enough to engulf Los Angeles and Dallas... but you're nowhere close to New York yet. All this effort, all these circles, but still... so far from the target. Short-circuiting helps, but only if you actually hit the target node fast.",
            '11. If only there existed a way to use your prior knowledge: the fact that new-york was eastwards, so you could "hint" your algorithm to prefer nodes that are on the east instead of those that are on the west.',
            '12. <figure><img src="../.gitbook/assets/image (91).png" alt=""><figcaption><p>A*</p></figcaption></figure>',
            '13. ### Introducing: A Star <a href="#introducing-a-star" id="introducing-a-star"></a>',
            "14. No, not the sun. It's an algorithm called A\\*.",
            "15. Observe the following: Dijkstra's is a \"true\" (i.e., not an estimate) measure of the distance **to** a node from the source. So, say, you visit a city in Illinois and your source was Denver, then by that time, you have a true measure of the distance **to** Denver. What we're missing is: some janky, rough estimate of the distance from a node **to** the target node, New York. That would complete the picture. Because then, if you sum these two things up (the measure from the source to the node + the estimate from the node to the target), you get (an estimate from the source to the target.) Of course, the better your original estimate from the node to the target, the better your estimate from the source to the target, the better your A\\* algorithm runs.",
            "16. So, let's modify our Dijkstra's algorithm slightly. In Dijkstra's, we used **bestKnownDistToV** as the priority in our algorithm. This time, we'll use **bestKnownDistToV+estimateFromVToGoal** as our heuristic.",
            "17. Here is a [demo](https://docs.google.com/presentation/d/177bRUTdCa60fjExdr9eO04NHm0MRfPtCzvEup1iMccM/edit#slide=id.g771336078\\_0\\_180)!",
            '18. ### Chicken And Egg <a href="#chicken-and-egg" id="chicken-and-egg"></a>',
            "19. We have a problem. How do we know what the estimate is? I mean, the estimate itself is a **distance**, and we're using A\\* to **find** the distance from some node to some other node.",
            '20. It seems like we\'re in an instance of the classic chicken and egg problem. "What came first? The chicken or the egg?" Aside, FYI, one reddit user had an [idea](https://www.reddit.com/r/dadjokes/comments/97768x/i\\_ordered\\_a\\_chicken\\_and\\_an\\_egg\\_from\\_amazon\\_ill/) about this.',
            "21. Well, it's called an estimate because it's exactly that. We use A\\* to get the **true** shortest path from a source to a target, but the estimate is something we approximate. Coming up with good estimates is hard sometimes.",
            "22. But to give you an example in our Denver - New York case. What we might do is just look up the GPS Coordinates of these cities, and calculate the straight line distance between those somehow. Of course, this wouldn't be correct because there's probably no straight line that one could take from Denver to NYC, but it's a fairly good estimate!",
            '23. ### Bad Heuristics <a href="#bad-heuristics" id="bad-heuristics"></a>',
            "24. Suppose that the shortest path from Denver to New York goes through some city $$C$$. Suppose that my GPS is broken, and so I think that this city $$C$$ is infinity far away from everything, and I set the estimated distance to $$C$$ from every other node in the graph to \u221e.",
            "25. What will happen? Well A\\* will basically never want to visit this city. (Remember what our priorities are in the priority queue; for this city, the priority will always be \u221e\u221e, even if I visit the immediate neighbors of this city. The estimated distances from the immediate neighbors of this city to this city were set to \u221e\u221e after all.)",
            "26. So... now what? We lose. A\\* breaks. We get the wrong answer back. Oops.",
            "27. The takeaway here is that heuristics need to be good. There are two definitions required for goodness.",
            "28. 1. Admissibility. heuristic(v, target) \u2264\u2264 trueDistance(v, target). (Think about the problem above. The true distance from the neighbor of $$C$$ to $$C$$ wasn't infinity, it was much, much smaller. But our heuristic said it was \u221e\u221e, so we broke this rule.)\n2. Consistency. For each neighbor $$v$$ of $$w$$:\n   * heuristic(v, target) \u2264\u2264 dist(v, w) + heuristic(w, target)\n   * where dist(v, w) is the weight of the edge from v to w.\n",
        ],
        "21.-heaps-and-priority-queues/21.4-summary.md": [
            "1. # 21.4 Summary",
            "2. **Priority Queue.** A Max Priority Queue (or PQ for short) is an ADT that supports at least the insert and delete-max operations. A MinPQ supposert insert and delete-min.",
            "3. **Heaps.** A max (min) heap is an array representation of a binary tree such that every node is larger (smaller) than all of its children. This definition naturally applies recursively, i.e. a heap of height 5 is composed of two heaps of height 4 plus a parent.",
            "4. **Tree Representations.** Know that there are many ways to represent a tree, and that we use Approach 3b (see lecture slides) for representing heaps, since we know they are complete.",
            "5. **Running times of various PQ implementations.** Know the running time of the three primary PQ operations for an unordered array, ordered array, and heap implementation.\n",
        ],
        "21.-heaps-and-priority-queues/21.2-heaps.md": [
            "1. # 21.2 Heaps",
            '2. {% embed url="https://www.youtube.com/watch?v=4WHrtiKTkA4" %}\nIntroducing the Heap\n{% endembed %}',
            "3. We previously saw that our known data structures with the best runtime for our PQ operations was the _binary search tree_. Modifying its structure and the constraints, we can further improve the runtime and efficiency of these operations.",
            "4. We will define our binary min-heap as being **complete** and obeying **min-heap** property:",
            "5. * Min-heap: Every node is less than or equal to both of its children\n* Complete: Missing items only at the bottom level (if any), all nodes are as far left as possible.",
            "6. ![](https://joshhug.gitbooks.io/hug61b/content/assets/heap-13.2.1.png)",
            "7. As we can see in the figures above, the green colored heaps are valid and the red ones aren't. The last two aren't because they violate at least one of the properties that we defined above.",
            "8. Now let's consider how this structure lends itself to the abstract data type we described in the previous chapter. We will do this through analyzing our desired operations.",
            "9. **Exercise 13.2.1.** Determine how each method of our Priority Queue interface will be implemented given this heap structure. Don't write actual code, just pseudocode!",
            '10. ### Heap Operations <a href="#heap-operations" id="heap-operations"></a>',
            '11. {% embed url="https://www.youtube.com/watch?v=NNS9srVjnOw" %}\nHeap Operations\n{% endembed %}',
            "12. The three methods we care about for the PriorityQueue ADT are `add`, `getSmallest`, and `removeSmallest`. We will start by conceptually describing how these methods can be implemented given our given schema of a heap.",
            "13. * `add`: Add to the end of heap temporarily. Swim up the hierarchy to the proper place.\n  * Swimming involves swapping nodes if child < parent\n* `getSmallest`: Return the root of the heap (This is guaranteed to be the minimum by our _min-heap_ property\n* `removeSmallest`: Swap the last item in the heap into the root. Sink down the hierarchy to the proper place.\n  * Sinking involves swapping nodes if parent > child. Swap with the smallest child to preserve _min-heap_ property.",
            "14. Great! We have determined how we will approach the operations specified by the PriorityQueue interface in an efficient way. But how do we actually code this?",
            "15. **Exercise 13.2.2.** Give the runtime for each of the methods specified above. Worst cases and best cases.",
            '16. ### Tree Representation <a href="#tree-representation" id="tree-representation"></a>',
            '17. {% embed url="https://www.youtube.com/watch?v=yCrjolcI5Wg" %}\nTree representation\n{% endembed %}',
            "18. There are many approaches we can take to representing trees.",
            '19. #### Approach 1a, 1b, and 1c <a href="#approach-1a-1b-and-1c" id="approach-1a-1b-and-1c"></a>',
            "20. Let us consider the most intuitive and previously used representation for trees. We will create mappings between nodes and their children. There are several ways to do this which we will explore right now.",
            "21. * In approach **Tree1A**, we consider creating pointers to our children and storing the value inside of the node object. These are hardwired links that give us fixed-width nodes. We can observe the code:",
            "22. ```java\npublic class Tree1A<Key> {\n  Key k;\n  Tree1A left;\n  Tree1A middle;\n  Tree1A right;\n  ...\n}\n```",
            "23. The visualization of this type of structure is shown below.",
            '24. <figure><img src="../.gitbook/assets/image (128).png" alt=""><figcaption><p>1a: Fixed-Width Nodes (BSTMap used this approach)</p></figcaption></figure>',
            "25. * Alternatively, in **Tree1B**, we explore the use of arrays as representing the mapping between children and nodes. This would give us variable-width nodes, but also awkward traversals and performance will be worse.",
        ],
        "21.-heaps-and-priority-queues/README.md": [
            "1. ---\ndescription: By Dhruti Pandya and Angel Aldaco\n---",
            "2. # 21. Heaps and Priority Queues",
            "3. ",
        ],
        "21.-heaps-and-priority-queues/21.5-exercises.md": [
            "1. # 21.5 Exercises",
            "2. ## Procedural",
            "3. 1. Represent the following heap as an array.",
            "4. ![](<../.gitbook/assets/image (86).png>)",
            "5. 2. Consider the above heap. What range of values, when inserted, will cause the maximum number of swaps?",
            "6. <details>",
            "7. <summary>Problem 1</summary>",
            "8. To convert a heap into an array, we simply read the values top-to-bottom and left-to-right. As such, the correct representation is `[0, 5, 1, 8, 8, 6, 2]`.",
            "9. </details>",
            "10. <details>",
            "11. <summary>Problem 2</summary>",
            "12. Since `0` is currently the root, any value `< 0` when inserted will have to swapped to from the bottom of the tree to the top, resulting in the maximum number of swaps. Thus, the range is $$[-\\infty, 0)$$.",
            "13. </details>",
            "14. ## Metacognitive",
            "15. 1. Consider an externally-chained hashmap. If there are many hash collisions, what data structure would be most efficient in storing the key-value pairs at each bucket?\n2. Consider an array sorted in descending order. Is this a valid heap? If so, which type: min-heap, or max-heap?\n3. Suppose that we store the root of a heap at the `0`th index in our array. For an arbitrary index `k`, compute the indices of its children and parent.\n4. Explain how you would implement a stack using a priority queue.",
            "16. <details>",
            "17. <summary>Problem 1</summary>",
            "18. An LLRB would be the best choice. Even if all items end up in same bucket, the LLRB will be balanced, resulting in a $$O(\\log n)$$ search time where $$n$$ is the number of items in the bucket. Any other choice of data structure (linked list, array, binary search tree) could have linear search time in the word case within the bucket.",
            "19. Note: Using an LLRB to store buckets requires that the keys be comparable.",
            "20. </details>",
            "21. <details>",
            "22. <summary>Problem 2</summary>",
            "23. An array sorted in descending order is a max-heap (the root is the maximum value, and values decrease in level order).",
            "24. </details>",
            "25. <details>",
            "26. <summary>Problem 3</summary>",
            "27. The children would be `2k + 1` and `2k + 2`. The parent would be `(k - 1) / 2` (note that this assumes integer division).",
            "28. </details>",
            "29. <details>",
            "30. <summary>Problem 4</summary>",
            "31. To implement a stack using a priority queue, keep a counter that increments each time you insert an item. Recently inserted items will always have higher priority, so they will be removed first.",
            "32. </details>\n",
        ],
        "21.-heaps-and-priority-queues/21.3-pq-implementation.md": [
            "1. # 21.3 PQ Implementation",
            '2. {% embed url="https://www.youtube.com/watch?v=nT5xA29-o2w" %}\nPQ Implementation Considerations\n{% endembed %}',
            "3. The actual implementation, which we will use and the book uses, is quite similar to the representation we discussed at the end of the previous chapter. The one difference is that we will leave one empty spot at the beginning of the array to simplify computation.",
            "4. * `leftChild(k)`= $$k * 2$$\n* `rightChild(k)`= $$k * 2 + 1$$\n* `parent(k)`= $$k / 2$$",
            '5. ### Comparing to alternative implementations <a href="#comparing-to-alternative-implementations" id="comparing-to-alternative-implementations"></a>',
            "6. | Methods          | Ordered Array | Bushy BST   | Hash Table | Heap        |\n| ---------------- | ------------- | ----------- | ---------- | ----------- |\n| `add`            | $$\u0398(N)$$      | $$\u0398(logN)$$ | $$\u0398(1)$$   | $$\u0398(logN)$$ |\n| `getSmallest`    | $$\u0398(1)$$      | $$\u0398(logN)$$ | $$\u0398(N)$$   | $$\u0398(1)$$    |\n| `removeSmallest` | $$\u0398(N)$$      | $$\u0398(logN)$$ | $$\u0398(N)$$   | $$\u0398(logN)$$ |",
            "7. Awesome! We can see that we have improved our runtime and we have also solved the problem of duplicate elements. Couple notes:",
            "8. * Heap operations are **amortized** analysis, since the array will have to resize (not a big deal)\n* BST's can have constant time `getSmallest` if pointer is stored to smallest element\n* Array-based heaps take around 1/3rd the memory it takes to represent a heap using approach 1A (direct pointers to children)",
            "9. **Exercise 13.3.1** Some lingering implementation questions:",
            "10. 1. How will the PQ know how to order the items? Say we had a PQ of dogs, would it order by weight or breed?\n2. Is there a way to allow for a flexibility of orderings?\n3. What could we do to make a MinPQ into a MaxPQ?\n",
        ],
        "21.-heaps-and-priority-queues/21.1-priority-queues.md": [
            "1. # 21.1 Priority Queues",
            "2. Binary Search Trees allowed us to efficiently search trees, only taking $$log(N)$$ time. This was because we could eliminate half of the elements of the tree at every step of our search. What if we cared more about quickly finding the _smallest_ or _largest_ element instead of quickly searching?",
            '3. {% embed url="https://www.youtube.com/watch?v=iCG9IDkoorY" %}\nIntroducing the Priority Queue\n{% endembed %}',
            "4. Now we come to the Abstract Data Type of a _Priority Queue_. To understand this ADT, consider a bag of items. You can add items to this bag, you can remove items from this bag, etc. The one caveat is that you can only interact with the smallest items of this bag.",
            "5. ```java\n/** (Min) Priority Queue: Allowing tracking and removal of \n  * the smallest item in a priority queue. */\npublic interface MinPQ<Item> {\n    /** Adds the item to the priority queue. */\n    public void add(Item x);\n    /** Returns the smallest item in the priority queue. */\n    public Item getSmallest();\n    /** Removes the smallest item from the priority queue. */\n    public Item removeSmallest();\n    /** Returns the size of the priority queue. */\n    public int size();\n}\n```",
            '6. ### Using Priority <a href="#using-priority" id="using-priority"></a>',
            "7. Where would we actually use this structure or need it?",
            "8. * Consider the scenario where we are monitoring text messages between citizens and want to keep track of unharmonious conversations.\n* Each day, you prepare a report of $$M$$ messages that are the most unharmonious using a `HarmoniousnessComparator`.",
            "9. Let's take this approach: Collect all of the messages that we receive throughout the day into a single list. Sort this list and return the first $$M$$ messages.",
            "10. ```java\npublic List<String> unharmoniousTexts(Sniffer sniffer, int M) {\n    ArrayList<String>allMessages = new ArrayList<String>();\n    for (Timer timer = new Timer(); timer.hours() < 24; ) {\n        allMessages.add(sniffer.getNextMessage());\n    }\n\n    Comparator<String> cmptr = new HarmoniousnessComparator();\n    Collections.sort(allMessages, cmptr, Collections.reverseOrder());\n\n    return allMessages.sublist(0, M);\n```",
            "11. Potential downsides? This approach will use $$\u0398(N)$$ space when actuality we only really need to use $$\u0398(M)$$ space.",
            "12. **Exercise 13.1.1.** Complete the method listed above, `unharmoniousTexts` with the same functionality as described using only \u0398(M) space.",
            '13. ### Priority Queue Implementation <a href="#priority-queue-implementation" id="priority-queue-implementation"></a>',
            "14. We solved the same problem using the Priority Queue ADT, making memory more efficient. We can observe that the code is slightly more complicated, but this is not always the case.",
            "15. Remember that ADT are similar interfaces and the implementation is still to be defined. Let's consider possible implementations using the data structure implementations we already know, analyzing the **worst case** runtimes of our desired operations:",
            "16. * Ordered Array\n  * `add`: $$\u0398(N)$$\n  * `getSmallest`: $$\u0398(1)$$\n  * `removeSmallest`: $$\u0398(1)$$\n* Bushy BST\n  * `add`: $$\u0398(logN)$$\n  * `getSmallest`: $$\u0398(logN)$$\n  * `removeSmallest`: $$\u0398(logN)$$\n* HashTable\n  * `add`: $$\u0398(1)$$\n  * `getSmallest`: $$\u0398(N)$$\n  * `removeSmallest`: $$\u0398(N)$$",
            "17. **Exercise 13.1.2.** Explain each of the runtimes above. What are the downfalls of each data structure? Describe a way of modifying one of these data structures to improve performance.",
            "18. Can we do better than these suggested data structures?",
            '19. ### Summary <a href="#summary" id="summary"></a>',
            "20. * Priority Queue is an Abstract Data Type that optimizes for handling minimum or maximum elements.\n* There can be space/memory benefits to using this specialized data structure.\n* Implementations for ADTs that we currently know don't give us efficient runtimes for PQ operations.\n  * A binary search tree is the most efficient among the other structures\n",
        ],
        "1.-introduction/1.1-your-first-java-program.md": [
            "1. # 1.1 Your First Java Program",
            '2. {% embed url="https://youtu.be/Jfq90-4qsco" %}',
            "3. The classic first program when introducing any new language is **Hello World**, or a program that prints `Hello World` to the console. In Java, **Hello World** can be written as such:",
            '4. ```java\npublic class HelloWorld {\n    public static void main(String[] args) {\n        System.out.println("Hello world!");\n    }\n}\n```',
            "5. As compared to other languages like Python, this may seem needlessly verbose. However, there are several reasons for the verbosity of Java, which will be covered in the next few chapters. For now, notice some key syntatical features of the code snippet above:",
            "6. * The **class declaration** `public class HelloWorld`: in Java, all code lives within classes.&#x20;\n* The **`main`** function: all the code that runs must be inside of a method declared as `public static void main(String[] args)`. Future chapters will cover the exact meaning of this declaration.\n* **Curly braces** `{}` enclose sections of code (functions, classes, and other types of code that will be covered in future chapters).\n* All statements must end with a **semi-colon**.&#x20;",
            "7. For fun, see [Hello world! in other languages](https://www.rosettacode.org/wiki/Hello\\_world/Text).",
            "8. ",
        ],
        "1.-introduction/1.2-java-workflow.md": [
            "1. # 1.2 Java Workflow",
            '2. {% embed url="https://youtu.be/Y2vC_SW00TE" %}',
            "3. Taking a program from a `.java` file into an executable has two main steps in Java: **compilation** and **interpretation**.&#x20;",
            '4. <figure><img src="../.gitbook/assets/1-2-compile-interpret.svg" alt=""><figcaption></figcaption></figure>',
            "5. To run the code in `Hello.java`, we would first **compile** the code into a `.class` file using the command `javac HelloWorld.java`. Then, to run the code, we would use the command `java HelloWorld`.&#x20;",
            "6. In your terminal, the result would look like the following:",
            "7. ```\n$ javac HelloWorld.java\n$ java HelloWorld\nHello World!\n```",
            "8. ## Class Files",
            "9. There are several reasons for the usage of `.class` files, which we will only cover briefly here. First of all, `.class` files are guaranteed to have been type-checked, making the distributed code safer. They are also more efficient to execute, and protect the actual source code in cases of intellectual property. We will not go into the details of `.class` files in this textbook beyond knowing that they are created after compilation.\n",
        ],
        "1.-introduction/README.md": [
            "1. # 1. Introduction",
            "2. This section covers basic features of Java and how to run programs on the command line.\n",
        ],
        "1.-introduction/1.4-exercises.md": [
            "1. # 1.4 Exercises",
            "2. 1. **True/False**: All variables, parameters, and methods must have a declared type in Java, and the type can never change.\n2. Suppose we have a function `smaller(a, b)` that takes in two `int` arguments `a` and `b` and returns the smaller of the two. What would the expression `String x = smaller(10, 20) + 3;` output?\n3. Choose all statements that are true in Java:&#x20;\n   * All code must be part of a class.\n   * The end and beginning of code segments are delimited using curly brackets `{}`.&#x20;\n   * All statements in Java end with a semi-colon `;`.\n   * Any code we want to run must be inside of a function `public static void main(String[] args)`.&#x20;",
            "3. <details>",
            "4. <summary>Solutions</summary>",
            '5. 1. True. See "Static Typing" for more information.\n2. This line of code would cause a compilation error because the declared type `String` is incompatible with the type returned by `smaller` and adding `3`, which would be an `int.`\n3. All the following statements are true.',
            "6. </details>\n",
        ],
        "1.-introduction/1.3-basic-java-features.md": [
            "1. # 1.3 Basic Java Features",
            '2. #### Variables and Loops <a href="#variables-and-loops" id="variables-and-loops"></a>',
            '3. {% embed url="https://youtu.be/xX04gYy9en0" %}',
            "4. The program below will print out the integers from 0 through 9.",
            '5. ```java\npublic class HelloNumbers {\n    public static void main(String[] args) {\n        int x = 0;\n        while (x < 10) {\n            System.out.print(x + " ");\n            x = x + 1;\n        }\n    }\n}\n```',
            "6. When we run this program, we see:",
            "7. ```\n$ javac HelloNumbers.java\n$ java HelloNumbers\n$ 0 1 2 3 4 5 6 7 8 9 \n```",
            "8. Some interesting features of this program that might jump out at you:",
            "9. * Our variable x must be declared before it is used, _and it must be given a type!_\n* Our loop definition is contained inside of curly braces, and the boolean expression that is tested is contained inside of parentheses.\n* Our print statement is just `System.out.print` instead of `System.out.println`. This means we should not include a newline (a return).\n* Our print statement adds a number to a space. This makes sure the numbers don't run into each other. Try removing the space to see what happens.\n* When we run it, our prompt ends up on the same line as the numbers (which you can fix in the following exercise if you'd like).",
            "10. Of these features the most important one is the fact that variables have a declared type. We'll come back to this in a bit, but first, an exercise.",
            "11. **Exercise 1.1.2.** Modify `HelloNumbers` so that it prints out the cumulative sum of the integers from 0 to 9. For example, your output should start with 0 1 3 6 10... and should end with 45.",
            "12. Also, if you've got an aesthetic itch, modify the program so that it prints out a new line at the end.",
            "13. The program below will print out the integers from 0 through 9.",
            '14. ```java\npublic class HelloNumbers {\n    public static void main(String[] args) {\n        int x = 0;\n        while (x < 10) {\n            System.out.print(x + " ");\n            x = x + 1;\n        }\n    }\n}\n```',
            "15. When we run this program, we see:",
            "16. ```\n$ javac HelloNumbers.java\n$ java HelloNumbers\n$ 0 1 2 3 4 5 6 7 8 9 \n```",
            "17. Some interesting features of this program that might jump out at you:",
            "18. * Our variable x must be declared before it is used, _and it must be given a type!_\n* Our loop definition is contained inside of curly braces, and the boolean expression that is tested is contained inside of parentheses.\n* Our print statement is just `System.out.print` instead of `System.out.println`. This means we should not include a newline (a return).\n* Our print statement adds a number to a space. This makes sure the numbers don't run into each other. Try removing the space to see what happens.\n* When we run it, our prompt ends up on the same line as the numbers (which you can fix in the following exercise if you'd like).",
            "19. Of these features the most important one is the fact that variables have a declared type. We'll come back to this in a bit, but first, an exercise.",
            "20. **Exercise 1.1.2.** Modify `HelloNumbers` so that it prints out the cumulative sum of the integers from 0 to 9. For example, your output should start with 0 1 3 6 10... and should end with 45.",
            "21. Also, if you've got an aesthetic itch, modify the program so that it prints out a new line at the end.",
            '22. #### Static Typing <a href="#code-style-comments-javadoc" id="code-style-comments-javadoc"></a>',
            "23. Java is a **statically typed language**, which means that all variables, parameters, and methods must have a declared type. After declaration, _the type can never change_. Expressions also have an implicit type; for example, the expression `3 + 5` has type `int`.&#x20;",
            "24. Because all types are declared statically, the compiler checks that types are compatible before the program even runs. This means that expressions with an incompatible type will fail to compile instead of crashing the program at runtime.&#x20;",
            "25. The advantages of static typing include:",
            "26. * catching type errors earlier in the coding process, reducing the debugging burden on the programmer.\n* avoiding type errors for end users.\n* making it easier to read and reason about code.\n* avoiding expensive runtime type checks, making code more efficient.",
            "27. However, static typing also has several disadvantages; namely:",
            "28. * more verbose code.\n* less generalizable code.&#x20;",
            "29. One of the most important features of Java is that all variables and expressions have a so-called `static type`. Java variables can contain values of that type, and only that type. Furthermore, the type of a variable can never change.",
            "30. One of the key features of the Java compiler is that it performs a static type check. For example, suppose we have the program below:",
            '31. ```java\npublic class HelloNumbers {\n    public static void main(String[] args) {\n        int x = 0;\n        while (x < 10) {\n            System.out.print(x + " ");\n            x = x + 1;\n        }\n        x = "horse";\n    }\n}\n```',
            "32. Compiling this program, we see:",
            '33. ```\n$ javac HelloNumbers.java \nHelloNumbers.java:9: error: incompatible types: String cannot be converted to int\n        x = "horse";\n                ^\n1 error\n```',
            "34. The compiler rejects this program out of hand before it even runs. This is a big deal, because it means that there's no chance that somebody running this program out in the world will ever run into a type error!",
            "35. This is in contrast to dynamically typed languages like Python, where users can run into type errors during execution!",
            "36. In addition to providing additional error checking, static types also let the programmer know exactly what sort of object he or she is working with. We'll see just how important this is in the coming weeks. This is one of my personal favorite Java features.",
            "37. To summarize, static typing has the following advantages:",
            "38. * The compiler ensures that all types are compatible, making it easier for the programmer to debug their code.\n* Since the code is guaranteed to be free of type errors, users of your compiled programs will never run into type errors. For example, Android apps are written in Java, and are typically distributed only as .class files, i.e. in a compiled format. As a result, such applications should never crash due to a type error since they have already been checked by the compiler.\n* Every variable, parameter, and function has a declared type, making it easier for a programmer to understand and reason about code.",
            "39. However, static typing also has several disadvantages, which will be discussed further in later chapters. To name a few:",
            "40. * More verbose code.\n* Less generalizable code.&#x20;",
            "41. **Extra Thought Exercise**",
            '42. In Java, we can say `System.out.println(5 + " ");`. But in Python, we can\'t say `print(5 + "horse")`, like we saw above. Why is that so?',
            "43. Consider these two Java statements:",
            '44. ```java\nString h = 5 + "horse";\n```',
            "45. and",
            '46. ```java\nint h = 5 + "horse";\n```',
            "47. The first one of these will succeed; the second will give a compiler error. Since Java is strongly typed, if you tell it `h` is a string, it can concatenate the elements and give you a string. But when `h` is an `int`, it can't concatenate a number and a string and give you a number.",
            "48. Python doesn't constrain the type, and it can't make an assumption for what type you want. Is `x = 5 + \"horse\"` supposed to be a number? A string? Python doesn't know. So it errors.",
            '49. In this case, `System.out.println(5 + "horse");`, Java interprets the arguments as a string concatentation, and prints out "5horse" as your result. Or, more usefully, `System.out.println(5 + " ");` will print a space after your "5".',
            '50. What does `System.out.println(5 + "10");` print? 510, or 15? How about `System.out.println(5 + 10);`?',
            '51. #### Defining Functions in Java <a href="#defining-functions-in-java" id="defining-functions-in-java"></a>',
            '52. {% embed url="https://youtu.be/ToJrue6Kg9A" %}',
            "53. In languages like Python, functions can be declared anywhere, even outside of functions. For example, the code below declares a function that returns the larger of two arguments, and then uses this function to compute and print the larger of the numbers 8 and 10:",
            "54. ```python\ndef larger(x, y):\n    if x > y:\n        return x\n    return y\n\nprint(larger(8, 10))\n```",
            '55. Since all Java code is part of a class, we must define functions so that they belong to some class. Functions that are part of a class are commonly called "methods". We will use the terms interchangably throughout the course. The equivalent Java program to the code above is as follows:',
            "56. ```java\npublic class LargerDemo {\n    public static int larger(int x, int y) {\n        if (x > y) {\n            return x;\n        }\n        return y;\n    }\n\n    public static void main(String[] args) {\n        System.out.println(larger(8, 10));\n    }\n}\n```",
            "57. The new piece of syntax here is that we declared our method using the keywords `public static`, which is a very rough analog of Python's `def` keyword. We will see alternate ways to declare methods in the next chapter.",
            "58. The Java code given here certainly seems much more verbose! You might think that this sort of programming language will slow you down, and indeed it will, in the short term. Think of all of this stuff as safety equipment that we don't yet understand. When we're building small programs, it all seems superfluous. However, when we get to building large programs, we'll grow to appreciate all of the added complexity.",
            "59. As an analogy, programming in Python can be a bit like [Dan Osman free-soloing Lover's Leap](https://www.youtube.com/watch?v=NCByLWtM7y4). It can be very fast, but dangerous. Java, by contrast is more like using ropes, helmets, etc. as in [this video](https://www.youtube.com/watch?v=tr6UIfPEuI0).",
            '60. #### Code Style, Comments, Javadoc <a href="#code-style-comments-javadoc" id="code-style-comments-javadoc"></a>',
            "61. Code can be beautiful in many ways. It can be concise. It can be clever. It can be efficient. One of the least appreciated aspects of code by novices is code style. When you program as a novice, you are often single mindedly intent on getting it to work, without regard to ever looking at it again or having to maintain it over a long period of time.",
            "62. In this course, we'll work hard to try to keep our code readable. Some of the most important features of good coding style are:",
            "63. * Consistent style (spacing, variable naming, brace style, etc)\n* Size (lines that are not too wide, source files that are not too long)\n* Descriptive naming (variables, functions, classes), e.g. variables or functions with names like `year` or `getUserName` instead of `x` or `f`.\n* Avoidance of repetitive code: You should almost never have two significant blocks of code that are nearly identical except for a few changes.\n* Comments where appropriate. Line comments in Java use the `//` delimiter. Block (a.k.a. multi-line comments) comments use `/*` and `*/`.",
            "64. The golden rule is this: Write your code so that it is easy for a stranger to understand.",
            "65. Here is the course's official [style guide](https://sp19.datastructur.es/materials/guides/style-guide.html). It's worth taking a look!",
            "66. Often, we are willing to incur slight performance penalties, just so that our code is simpler to [grok](https://en.wikipedia.org/wiki/Grok). We will highlight examples in later chapters.",
            "67. **Comments**",
            "68. We encourage you to write code that is self-documenting, i.e. by picking variable names and function names that make it easy to know exactly what's going on. However, this is not always enough. For example, if you are implementing a complex algorithm, you may need to add comments to describe your code. Your use of comments should be judicious. Through experience and exposure to others' code, you will get a feeling for when comments are most appropriate.",
            "69. One special note is that all of your methods and almost all of your classes should be described in a comment using the so-called [Javadoc](https://en.wikipedia.org/wiki/Javadoc) format. In a Javadoc comment, the block comment starts with an extra asterisk, e.g. `/**`, and the comment often (but not always) contains descriptive tags. We won't discuss these tags in this textbook, but see the link above for a description of how they work.",
            "70. As an example without tags:",
            "71. ```java\npublic class LargerDemo {\n    /** Returns the larger of x and y. */           \n    public static int larger(int x, int y) {\n        if (x > y) {\n            return x;\n        }\n        return y;\n    }\n\n    public static void main(String[] args) {\n        System.out.println(larger(8, 10));\n    }\n}\n```",
            "72. The widely used [javadoc tool](http://docs.oracle.com/javase/8/docs/technotes/tools/windows/javadoc.html) can be used to generate HTML descriptions of your code. We'll see examples in a later chapter.",
            "73. ",
        ],
        "19.-hashing-i/19.1-introduction-to-hashing-data-indexed-arrays/19.1.2-a-second-attempt-dataindexedwordset.md":
            [
                "1. # 19.1.2 A second attempt: DataIndexedWordSet",
                '2. {% embed url="https://youtu.be/FYvABY2WYPo" %}',
                '3. Our `DataIndexedIntegerSet` only allowed for integers, but now we want to insert the `String` `"cat"` into it. We\'ll call our data structure that can insert strings `DataIntexedEnglishWordSet` Here\'s a crazy idea: let\'s give every string a number. Maybe "cat" can be `1`, "dog" can be `2` and "turtle" can be `3`.',
                '4. (The way this would work is \u2013\u2013 if someone wanted to add a "cat" to our data structure, we would \'figure out\' that the number for "cat" is 1, and then set `present[1]` to be `true`. If someone wanted to ask us if "cat" is in our data structure, we would \'figure out\' that "cat" is 1, and check if `present[1]` is true.)',
                '5. But then if someone tries to insert the word "potatocactus", we don\'t know what to do. We need to develop a general strategy so that given a string, we can figure out a number representation for it.',
                "6. Here are the two main strategies we chose to use:",
                '7. #### Strategy 1: Use the first letter. <a href="#strategy-1-use-the-first-letter" id="strategy-1-use-the-first-letter"></a>',
                "8. A simple idea is to just use the first character of any given string to convert it to its number representation. However, if someone tried to insert two words with the same first letter, we have a **collision**, which we deal with using the next strategy.",
                '9. #### Strategy 2: Avoid Collisions. <a href="#strategy-1-use-the-first-letter" id="strategy-1-use-the-first-letter"></a>',
                "10. There are $$26$$ unique characters in the English lowercase alphabet. We assign each one a number: $$a=1, b=2, ...,z=26$$. Now, we can write any unique lowercase string in **base 26**. (Note that **base 26** simply means that we will use **26** as the multiplier, much like we used **10** and **2** as examples above.)",
                '11. * $$``cat"=3*26^2+1*26^1+20 * 26^0$$',
                "12. **This representation gives a unique integer to every English word containing lowercase letters, much like using base 10 gives a unique representation to every number. We are guaranteed to not have collisions.**\\\n",
            ],
        "19.-hashing-i/19.1-introduction-to-hashing-data-indexed-arrays/19.1.1-a-first-attempt-dataindexedintegerset.md":
            [
                "1. # 19.1.1 A first attempt: DataIndexedIntegerSet",
                "2. Let us begin by considering the following approach. This approach was introduced in [Hashing Video 1](https://youtu.be/rSqSlu8sEkI).",
                "3. For now, we're only going to try to improve complexity from $$\\Theta(logN)$$ to $$\\Theta(1)$$. We're going to not worry about comparability. In fact, we're going to only consider storing and searching for `int`s.",
                "4. Here's an idea: let's create an ArrayList of type `boolean` and size 2 billion. Let everything be false by default.",
                "5. * The `add(int x)` method simply sets the `x` position in our ArrayList to true. This takes $$\\Theta(1)$$ time.\n* The `contains(int x)` method simply returns whether the `x` position in our ArrayList is `true` or `false`. This also takes $$\\Theta(1)$$ time!",
                "6. ```\npublic class DataIndexedIntegerSet {\n    private boolean[] present;\n\n    public DataIndexedIntegerSet() {\n        present = new boolean[2000000000];\n    }\n\n    public void add(int x) {\n        present[i] = true;\n    }\n\n    public boolean contains(int x) {\n        return present[i];\n    }\n}\n```",
                "7. There we have it. That's all folks.",
                "8. Well, not really. What are some potential **issues** with this approach?",
                "9. * Extremely wasteful. If we assume that a `boolean` takes 1 byte to store, the above needs `2GB` of space per `new DataIndexedIntegerSet()`. Moreover, the user may only insert a handful of items...\n* What do we do if someone wants to insert a `String`? Or other data types?\n  * Let's look at this next. Of course, we may want to insert other things, like `Dog`s. That'll come soon!\n",
            ],
        "19.-hashing-i/19.1-introduction-to-hashing-data-indexed-arrays/19.1.3-a-third-attempt-dataindexedstringset.md":
            [
                "1. # 19.1.3 A third attempt: DataIndexedStringSet",
                '2. {% embed url="https://youtu.be/kLxyiFWZBDs" %}',
                "3. There is a character format called **ASCII**, which has an integer per character. Here, we see that the largest value (i.e., the base/multiplier we need to use) is 126. Let's just do that. The same thing as `DataIndexedEnglishWordSet`, but just with base `126`.",
                "4. ```\npublic static int asciiToInt(String s) {\n    int intRep = 0;\n    for (int i = 0; i < s.length(); i += 1) {           \n        intRep = intRep * 126;\n        intRep = intRep + s.charAt(i);\n    }\n    return intRep;\n}\n```",
                "5. What about adding support for Chinese? The largest possible representation is 40959, so we need to use that as the base.&#x20;",
                "6. So... to store a 3-character Chinese word, we need an array of size larger than **39 trillion** (with a T)!. This is getting out of hand... so let's explore what we can do to improve this, namely, using hashCode.",
                "7. ",
            ],
        "19.-hashing-i/19.1-introduction-to-hashing-data-indexed-arrays/README.md":
            [
                "1. # 19.1 Introduction to Hashing: Data Indexed Arrays",
                "2. So far in the course, we've taken a look at many ways to store things in data structures, but they're not always the most efficient in terms of runtime. Enter an incredible data structure that can provide insertion, removal, and contains checks, regardless of how many elements are inside of it (even if theres millions of items)\u2013 all in O(1) runtime in the best case! Sound too good to be true?&#x20;",
                "3. We will explore the magic of hashing in this chapter to see how this is possible.",
                "4. ### Quick Recap of Data Structures we've seen so far",
                "5. We've looked at a few data structures that efficiently search for the existence of items within the data structure. We looked at Binary Search Trees, then made them balanced using 2-3 Trees.",
                "6. However, there are some limitations that these structures impose (yes, even 2-3 trees!)",
                '7. 1. They require that items be comparable. How do you decide where a new item goes in a BST? You have to answer the question "are you smaller than or bigger than the root"? For some objects, this question may make no sense.\n2. They give a complexity of $$\\Theta(logN)$$. Is this good? Absolutely. But maybe we can do better.',
                '8. {% embed url="https://www.youtube.com/watch?index=1&list=PL8FaHk7qbOD67rFIKNVkDcucFwNjUq9-d&v=rSqSlu8sEkI" %}\nProfessor Hug\'s Lecture on Hash Tables.\n{% endembed %}',
                "9. ### Using Data as Indices",
                "10. Arrays have amazing runtime for its basic operations. Is there a good way to convert data into indices and store them in an array? In the next couple of sub-sections, we will walk through the steps that will eventually lead us to our final creation -- the hash table. Watching the videos linked in the subsections is helpful for understanding the process through which we arrived at the invention of hash tables. Hash tables will be the main focus of the rest of the chapter.&#x20;\n",
            ],
        "19.-hashing-i/19.2-hash-code.md": [
            "1. ---\ndescription: The core mechanism of hashing!\n---",
            "2. # 19.2 Hash Code",
            "3. We face the problem that not every object in Java can easily be converted to a number. However, the key idea behind hashing is the transformation of any object into a numeric representation. The key is to have a hashing function transform our keys into different values, and convert that number into an index to then access the array.&#x20;",
            "4. We achieve this through our own implementation of a `hashCode()` function, with a return value of an `int` type. This `int` type is our _hash value_.&#x20;",
            "5. The built-in String class in Java, for example, might have the following code block:",
            "6. ```java\npublic class String {\n    public int hashCode() {\n        //implementation here\n    }\n}\n```",
            "7. Based on this example, we can call `key.hashCode()` to generate an integer hash code for a `String` instance called `key`.&#x20;",
            '8. {% embed url="https://www.youtube.com/watch?index=4&list=PL8FaHk7qbOD67rFIKNVkDcucFwNjUq9-d&v=ix2frc8dHbw" %}\nProfessor Hug\'s Lecture on Hash Codes\n{% endembed %}',
            "9. ## Memory Inefficiency in Hash Codes",
            "10. An issue mentioned earlier is memory inefficiency: for a small range of hash values, we can get away with an array that individuates each hash value. That is, every index in the array would represent a unique hash value. This works well if our indices are small and close to zero. But remember that Java\u2019s 32-bit integer type can support numbers anywhere between -2,147,483,648 and 2,147,483,647. Now, most of the time, our data won\u2019t use anywhere near that many values. But even if we only wanted to support special characters, our array would still need to be 1,112,064 elements long!",
            "11. Instead, we'll slightly modify our indexing strategy. Let's say we only want to support an array of length 10 so as to avoid allocating excessive amounts of memory. How can we turn a number that is potentially millions or billions large into a value between 0 and 9, inclusive?",
            "12. ### Wrapping!",
            "13. The **modulus operator (%)** allows us to achieve this.&#x20;",
            '14. _Review_: The result of the modulo operator is like a remainder in fractional division. For example, `65 % 10` returns `5` because after dividing 65 by 10, we are left with a remainder of 5. Thus as other examples, `3 % 10 = 3`, `20 % 10 = 0`, and `19 % 10 = 9`. For an intuitive understanding of this, think about how we used the modulo operator in Project 1: Deques to ensure you avoided `IndexOutOfBounds` Exceptions and could accurately index into your deque via the concept of "wrapping around" the array.&#x20;',
            "15. Returning back to our original problem, we want to be able to convert any number to a value between 0 and 9, inclusive. Given our discussion on the modulo operator, we can see that any number mod 10 will return an integer value between 0 and 9. This is what we need to index into an array of size 10!",
            "16. More generally, we can locate the correct index for any key with the following:",
            "17. ```java\nMath.floorMod(key.hashCode(), array.length)\n```",
            "18. where `array` is the underlying array representing our hash table.",
            "19. _Quick Note_: In Java, the `Math.floorMod` function will perform the modulus operation while correctly accounting for negative integers, whereas `%` does not.\n",
        ],
        "19.-hashing-i/19.4-handling-collisions-linear-probing-and-external-chaining.md":
            [
                "1. # 19.4 Handling Collisions: Linear Probing and External Chaining",
                "2. In hashing, a collision occurs when we have multiple elements that have the same index in our array.",
                '3. {% embed url="https://www.youtube.com/watch?index=5&list=PL8FaHk7qbOD67rFIKNVkDcucFwNjUq9-d&v=5caI6XD_YLA" %}\nProfessor Hug\'s Lecture on Collisions\n{% endembed %}',
                "4. There are two common methods to deal with collisions in hash tables:",
                "5. 1. **Linear Probing:** Store the colliding keys elsewhere in the array, potentially in the next open array space. This method can be seen with distributed hash tables, which you will see in later computer science courses that you may take.\n2. **External Chaining:** A simpler solution is to store all the keys with the same hash value together in a collection of their own, such as a `LinkedList`. This collection of entries sharing a single index is called a **bucket**.",
                "6. ",
            ],
        "19.-hashing-i/19.3-valid-and-good-hashcodes.md": [
            '1. # 19.3 "Valid" & "Good" Hashcodes',
            "2. ",
            '3. {% embed url="https://www.youtube.com/watch?index=8&list=PL8FaHk7qbOD67rFIKNVkDcucFwNjUq9-d&v=14f8LxYREFQ" %}\nProfessor Hug\'s Lecture on Valid/Good Hashcodes\n{% endembed %}',
            "4. ## Valid Hashcodes!",
            '5. You may see this term in discussions and potentially on exams. What exactly makes a hash code "valid"? There are two properties:',
            "6. 1. _**Deterministic**_**:** The `hashCode()` function of two objects A and B who are equal to each other (`A.equals(B) == true`) have the same hashcode. _This also means the hash function cannot rely on attributes of the object that are not reflected in the_ `.equals()` _method._\n2. _**Consistent**_**:** The `hashCode()` function returns the same integer every time it is called on the same instance of an object. This means the `hashCode()` function must be independent of time/stopwatches, random number generators, or any methods that would not give us a consistent `hashCode()` across multiple `hashCode()` function calls on the same object instance.",
            "7. Note that there are no requirements that state that unequal objects should have different hash function values.",
            "8. One could argue that these two requirements are in fact the same requirement. We can restate the requirement of consistency. Imagine we make a pointer named `A` to an object `O` at 12:00 pm and a pointer named `B` to this same object `O` at 1:00 pm. We know that the hash code should return the same integer for both objects, due to the consistency requirement. However, how do we formally define our statement \u201cthis same object `O`\u201d above? Technically, the only reason we consider `B` to be pointing to the same thing as `A` is because of the `.equals()`method! This is starting to sound a lot like the determinism requirement.",
            "9. ## Good Hashcodes!",
            '10. You\'ll probably see this term a lot as well. But what makes a hashcode "good"? There are a few properties that can make a good `hashCode()`:',
            "11. 1. The `hashCode()` function must be valid.\n2. The `hashCode()` function values should be spread as uniformly as possible over the set of all integers.\n3. The `hashCode()` function should be relatively quick to compute \\[ideally O(1) constant time mathematical operations]",
            "12. Now let\u2019s think more specifically about the impact of the hashing function. In general, we assume most hash functions will be \u201crelatively quick\u201d. Why do we make this assumption? Given how intrinsic the hashing function is to our data structure, the runtime of this function will have a significant effect on the overall runtime of our data structure. This means we want our hash code to be \u201ceasily\u201d computable (ideally constant time), so that we may maintain the O(1) runtime characteristic that makes hashing so special and efficient!",
            "13. ",
        ],
        "19.-hashing-i/README.md": [
            "1. ---\ndescription: By William Lee and Angel Aldaco\n---",
            "2. # 19. Hashing I",
            "3. ",
        ],
        "19.-hashing-i/19.7-exercises.md": [
            "1. # 19.7 Exercises",
            "2. ## 1. Potpourri",
            "3. (a) True or False: Resizes are triggered if adding a key-value pair causes the load factor to be greater than or equal to the specified maximum load factor.",
            "4. <details>",
            "5. <summary>Answer to Q1(a)</summary>",
            "6. **False**. A resize will be triggered if adding another key value pair would cause the load factor to **exceed** the specified maximum load factor. A resize is not triggered when the load factor is equal to the specified maximum load factor.",
            "7. </details>",
            "8. (b) True or False: The `hashCode()` function can have varied return types.",
            "9. <details>",
            "10. <summary>Answer to Q1(b)</summary>",
            "11. **False**. In Java, the hashCode() function can **only** have an `int` return type to serve as a hash value.",
            "12. </details>",
            "13. ## 2. hashCode()....",
            "14. In order for a hash code to be valid, objects that are equivalent to each other (i.e. .equals() returns true) must return equivalent hash codes. If an object does not explicitly override the `hashCode()` method, it will inherit the `hashCode()` method defined in the Object class, which returns the object\u2019s address in memory.",
            "15. Here are four potential implementations of Integer's `hashCode()` function. Assume that `intValue()` returns the value represented by the Integer object.&#x20;",
            "16. Categorize each `hashCode()` implementation as either a valid or an invalid hash function. If it is valid, point out a flaw or disadvantage. If it is invalid, explain why.&#x20;",
            "17. Question (a)",
            "18. ```java\npublic int hashCode() {\n    return -1;\n}\n```",
            "19. <details>",
            "20. <summary>Answer for Q2(a)</summary>",
            "21. Valid. As required, this hash function returns the same hash code for Integers that are .equals() to each other. However, this is a terrible hash code because collisions are extremely frequent and occur 100% of the time.",
            "22. </details>",
            "23. Question (b)",
            "24. ```java\npublic int hashCode() {\n    return intValue() * intValue();\n}\n```",
            "25. <details>",
            "26. <summary>Answer for Q2(b)</summary>",
            "27. Valid. Similar to (a), this hash function returns the same hash code for Integers that are .equals(). However, Integers that share the same absolute values will collide (for example, x = 5 and x = -5 will both return the same hashcode). A better hash function would be to just return intValue() itself.",
            "28. </details>",
            "29. Question (c)",
            "30. ```java\npublic int hashCode() {\n    Random rand = new Random();\n    return rand.nextInt();\n}\n```",
            "31. <details>",
            "32. <summary>Answer for Q2(c)</summary>",
            "33. Invalid. If we call hashCode() multiple times on the same Integer object, we will get different hash codes returned each time.",
            "34. </details>",
            "35. Question (d)",
            "36. ```java\npublic int hashCode() {\n    return super.hashCode();\n}\n```",
            "37. <details>",
            "38. <summary>Answer for Q2(d)</summary>",
            "39. Invalid. This hash function returns some integer corresponding to the Integer object\u2019s location in memory. Different Integer objects will exist in different locations in memory, so even if they represent the same value they will return different hash codes.",
            "40. </details>",
            "41. ## 3. Hashin' and Resizin'",
            "42. Given the provided `hashCode()` implementation, hash the items listed below with external chaining (the first item is already inserted for you). Assume the load factor is 1, and the initial underlying array has size of 4. Use geometric resizing with a resize factor of 2. You may draw more boxes to extend the array when you need to resize.",
            "43. ```java\n/** Returns 0 if word begins with \u2019a\u2019, 1 if it begins with \u2019b\u2019, etc. */\npublic int hashCode() {\n    return word.charAt(0) - 'a';\n}\n```",
            '44. ```\n["apple", "cherry", "fig", "guava", "durian", "apricot", "banana"]\n```',
            "45. <details>",
            "46. <summary>Answer for Question 3</summary>",
            "47. Here is what the hash table should look like after inserting `guava`:",
            "48. ![](<../.gitbook/assets/screenshot 2023-03-01 at 6.05.07 PM.png>)",
            "49. Here is what the hash table should look like after inserting `durian`:",
            "50. ![](<../.gitbook/assets/image (142).png>)",
            "51. Here is what the hash table should look like after all insertions have been completed:",
            "52. ![](<../.gitbook/assets/screenshot 2023-03-01 at 6.06.50 PM.png>)",
            "53. </details>\n",
        ],
        "19.-hashing-i/19.5-resizing-and-hash-table-performance.md": [
            "1. # 19.5 Resizing & Hash Table Performance",
            "2. No matter how good our `hashCode()` method is, if the underlying array of our hash table is small and we add a lot of keys to it, then we will start getting more and more collisions. Because of this, a hash table should expand its underlying array once it starts to fill up (much like how an `ArrayList` expands once it fills up).",
            '3. {% embed url="https://www.youtube.com/watch?index=6&list=PL8FaHk7qbOD67rFIKNVkDcucFwNjUq9-d&v=KHHi_LVjDLs" %}\nProfessor Hug\'s Lecture on why we have resizing and how it works!\n{% endembed %}',
            "4. To keep track of how full our hash table is, we define the term **load factor**, which is the ratio of the number of elements inserted over the total physical length of the array.",
            '5. <figure><img src="../.gitbook/assets/image (55).png" alt="A picture of an equation which states that the load factor is equal to the number of elements divided by the total capacity of the array."><figcaption><p>A very important equation</p></figcaption></figure>',
            "6. For our hash table, we will define the maximum load factor that we will allow. **If adding another key-value pair would cause the load factor to exceed the specified maximum load factor, then the hash table should resize.** This is usually done by doubling the underlying array length. Java\u2019s default maximum load factor is 0.75 which provides a good balance between a reasonably-sized array and reducing collisions.",
            "7. Note that if we are trying to add a key-value pair and the key already exists in the hash map, the corresponding value should be updated but no resizing should occur.",
            "8. As an example, let\u2019s consider what happens if our hash table has an array length of 10 and currently contains 7 elements. Each of these 7 elements are hashed modulo 10 because we want to get an index within the range of 0 through 9. The current load factor is 7/10, or 0.7, just under the threshold.",
            "9. If we try to insert one more element, we would have a total of 8 elements in our hash table and a load factor of 0.8. Because this would cause the load factor to exceed the maximum load factor, we must resize the underlying array to length 20 before we insert the element. Remember that since our procedure for locating an entry in the hash table is to take the `hashCode() % array.length` and our array\u2019s length has changed from 10 to 20, all the elements in the hash table need to be relocated. Once all the elements have been relocated and our new element has been added, we will have a load factor of 8/20, or 0.4, which is below the maximum load factor.\n",
        ],
        "19.-hashing-i/19.6-summary.md": [
            "1. # 19.6 Summary",
            "2. In this chapter, we learned about hashing, a powerful technique for turning a more complex object like a `String` into a numerically representable value like an `int`.&#x20;",
            "3. The _hash table_ is a data structure that combines the _hash function_ with the fact that arrays can be indexed in constant time. Using the hash table and the map abstract data type, we can build a `HashMap` which allows for amortized constant time access to any key-value pair so long as we know which bucket the key falls into.",
            "4. However, we quickly demonstrated that this naive implementation has several drawbacks: the ability to represent all different kinds of objects, memory efficiency, and collisions.&#x20;",
            "5. We investigated the importance of the `hashCode()` function to gain an understanding of how it affects the runtime of the hash table. To allow for smaller `size()` in the array, we used the modulo operator to shrink hash values down to a specified range of numbers. We then added external chaining to solve collisions by allowing multiple entries to live in a single bucket in the form of a LinkedList, and explored resizing functionality based on the load factor!\n",
        ],
        "5.-dllists.md": [
            "1. # 5. DLLists",
            "2. In Chapter 2.2, we built the `SLList` class, which was better than our earlier naked recursive `IntList` data structure. In this section, we'll wrap up our discussion of linked lists, and also start learning the foundations of arrays that we'll need for an array based list we'll call an `AList`. Along the way, we'll also reveal the secret of why we used the awkward name `SLList` in the previous chapter.",
            '3. #### addLast <a href="#addlast" id="addlast"></a>',
            "4. Consider the `addLast(int x)` method from the previous chapter.",
            "5. ```java\npublic void addLast(int x) {\n    size += 1;\n    IntNode p = sentinel;\n    while (p.next != null) {\n        p = p.next;\n    }\n\n    p.next = new IntNode(x, null);\n}\n```",
            "6. The issue with this method is that it is slow. For a long list, the `addLast` method has to walk through the entire list, much like we saw with the `size` method in chapter 2.2. Similarly, we can attempt to speed things up by adding a `last` variable, to speed up our code, as shown below:",
            "7. ```java\npublic class SLList {\n    private IntNode sentinel;\n    private IntNode last;\n    private int size;    \n\n    public void addLast(int x) {\n        last.next = new IntNode(x, null);\n        last = last.next;\n        size += 1;\n    }\n    ...\n}\n```",
            "8. **Exercise 2.3.1:** Consider the box and pointer diagram representing the `SLList` implementation above, which includes the last pointer. Suppose that we'd like to support `addLast`, `getLast`, and `removeLast` operations. Will the structure shown support rapid `addLast`, `getLast`, and `removeLast` operations? If not, which operations are slow?",
            "9. ![sllist\\_last\\_pointer.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/sllist\\_last\\_pointer.png)",
            "10. **Answer 2.3.1:** `addLast` and `getLast` will be fast, but `removeLast` will be slow. That's because we have no easy way to get the second-to-last node, to update the `last` pointer, after removing the last node.",
            '11. #### SecondToLast <a href="#secondtolast" id="secondtolast"></a>',
            "12. The issue with the structure from exercise 2.3.1 is that a method that removes the last item in the list will be inherently slow. This is because we need to first find the second to last item, and then set its next pointer to be null. Adding a `secondToLast` pointer will not help either, because then we'd need to find the third to last item in the list in order to make sure that `secondToLast` and `last` obey the appropriate invariants after removing the last item.",
            "13. **Exercise 2.3.2:** Try to devise a scheme for speeding up the `removeLast` operation so that it always runs in constant time, no matter how long the list. Don't worry about actually coding up a solution, we'll leave that to project 1. Just come up with an idea about how you'd modify the structure of the list (i.e. the instance variables).",
            "14. We'll describe the solution in Improvement #7.",
            '15. #### Improvement #7: Looking Back <a href="#improvement-7-looking-back" id="improvement-7-looking-back"></a>',
            "16. The most natural way to tackle this issue is to add a previous pointer to each `IntNode`, i.e.",
            "17. ```java\npublic class IntNode {\n    public IntNode prev;\n    public int item;\n    public IntNode next;\n}\n```",
            '18. In other words, our list now has two links for every node. One common term for such lists is the "Doubly Linked List", which we\'ll call a `DLList` for short. This is in contrast to a single linked list from chapter 2.2, a.k.a. an `SLList`.',
            "19. The addition of these extra pointers will lead to extra code complexity. Rather than walk you through it, you'll build a doubly linked list on your own in project 1. The box and pointer diagram below shows more precisely what a doubly linked list looks like for lists of size 0 and size 2, respectively.",
            "20. ![dllist\\_basic\\_size\\_0.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_basic\\_size\\_0.png)",
            "21. ![dllist\\_basic\\_size\\_2.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_basic\\_size\\_2.png)",
            '22. #### Improvement #8: Sentinel Upgrade <a href="#improvement-8-sentinel-upgrade" id="improvement-8-sentinel-upgrade"></a>',
            "23. Back pointers allow a list to support adding, getting, and removing the front and back of a list in constant time. There is a subtle issue with this design where the `last` pointer sometimes points at the sentinel node, and sometimes at a real node. Just like the non-sentinel version of the `SLList`, this results in code with special cases that is much uglier than what we'll get after our 8th and final improvement. (Can you think of what `DLList` methods would have these special cases?)",
            "24. One fix is to add a second sentinel node to the back of the list. This results in the topology shown below as a box and pointer diagram.",
            "25. ![dllist\\_double\\_sentinel\\_size\\_0.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_double\\_sentinel\\_size\\_0.png)",
            "26. ![dllist\\_double\\_sentinel\\_size\\_2.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_double\\_sentinel\\_size\\_2.png)",
            "27. An alternate approach is to implement the list so that it is circular, with the front and back pointers sharing the same sentinel node.",
            "28. ![dllist\\_circular\\_sentinel\\_size\\_0.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_circular\\_sentinel\\_size\\_0.png)",
            "29. ![dllist\\_circular\\_sentinel\\_size\\_2.png](https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist\\_circular\\_sentinel\\_size\\_2.png)",
            "30. Both the two-sentinel and circular sentinel approaches work and result in code that is free of ugly special cases, though I personally find the circular approach to be cleaner and more aesthetically beautiful. We will not discuss the details of these implementations, as you'll have a chance to explore one or both in project 1.",
            '31. #### Generic DLLists <a href="#generic-dllists" id="generic-dllists"></a>',
            "32. Our DLLists suffer from a major limitation: they can only hold integer values. For example, suppose we wanted to create a list of Strings:",
            '33. ```java\nDLList d2 = new DLList("hello");\nd2.addLast("world");\n```',
            "34. The code above would crash, since our `DLList` constructor and `addLast` methods only take an integer argument.",
            "35. Luckily, in 2004, the creators of Java added **generics** to the language, which will allow you to, among other things, create data structures that hold any reference type.",
            "36. The syntax is a little strange to grasp at first. The basic idea is that right after the name of the class in your class declaration, you use an arbitrary placeholder inside angle brackets: `<>`. Then anywhere you want to use the arbitrary type, you use that placeholder instead.",
            "37. For example, our `DLList` declaration before was:",
            "38. ```java\npublic class DLList {\n    private IntNode sentinel;\n    private int size;\n\n    public class IntNode {\n        public IntNode prev;\n        public int item;\n        public IntNode next;\n        ...\n    }\n    ...\n}\n```",
            "39. A generic `DLList` that can hold any type would look as below:",
            "40. ```java\npublic class DLList<BleepBlorp> {\n    private IntNode sentinel;\n    private int size;\n\n    public class IntNode {\n        public IntNode prev;\n        public BleepBlorp item;\n        public IntNode next;\n        ...\n    }\n    ...\n}\n```",
            "41. Here, `BleepBlorp` is just a name I made up, and you could use most any other name you might care to use instead, like `GloopGlop`, `Horse`, `TelbudorphMulticulus` or whatever.",
            "42. Now that we've defined a generic version of the `DLList` class, we must also use a special syntax to instantiate this class. To do so, we put the desired type inside of angle brackets during declaration, and also use empty angle brackets during instantiation. For example:",
            '43. ```java\nDLList<String> d2 = new DLList<>("hello");\nd2.addLast("world");\n```',
            "44. Since generics only work with reference types, we cannot put primitives like `int` or `double` inside of angle brackets, e.g. `<int>`. Instead, we use the reference version of the primitive type, which in the case of `int` case is `Integer`, e.g.",
            "45. ```java\nDLList<Integer> d1 = new DLList<>(5);\nd1.insertFront(10);\n```",
            "46. There are additional nuances about working with generic types, but we will defer them to a later chapter of this book, when you've had more of a chance to experiment with them on your own. For now, use the following rules of thumb:",
            "47. * In the .java file **implementing** a data structure, specify your generic type name only once at the very top of the file after the class name.\n* In other .java files, which use your data structure, specify the specific desired type during declaration, and use the empty diamond operator during instantiation.\n* If you need to instantiate a generic over a primitive type, use `Integer`, `Double`, `Character`, `Boolean`, `Long`, `Short`, `Byte`, or `Float` instead of their primitive equivalents.",
            "48. Minor detail: You may also declare the type inside of angle brackets when instantiating, though this is not necessary, so long as you are also declaring a variable on the same line. In other words, the following line of code is perfectly valid, even though the `Integer` on the right hand side is redundant.",
            "49. ```java\nDLList<Integer> d1 = new DLList<Integer>(5);\n```",
            "50. At this point, you know everything you need to know to implement the `LinkedListDeque` project on project 1, where you'll refine all of the knowledge you've gained in chapters 2.1, 2.2, and 2.3.\n",
        ],
        "25.-minimum-spanning-trees/25.4-chapter-summary.md": [
            "1. # 25.4 Chapter Summary",
            "2. In this chapter, we learned about Minimum Spanning Trees and the Cut Property:",
            "3. * **MST:** the lightest set of edges in a graph possible such that all the vertices are connected and acyclic.&#x20;\n* **The Cut Property**: given any cut, the minimum weight crossing edge is in the MST.\n  * _Cut_:  an assignment of a graph\u2019s nodes to two non-empty sets&#x20;\n  * _Crossing Edge:_ an edge which connects a node from one set to a node from the other set.",
            "4. We also learned about how to find MSTs of a graph with two algorithms:&#x20;",
            "5. * **Prim's Algorithm**: Construct MST through a mechanism similar to Dijkstra's Algorithm, with the only difference of inserting vertices into the fringe not based on distance to goal vertex but distance to the MST under construction.&#x20;\n  * _Runtime_: $$O((|V| + |E| )log(|V|))$$\n* **Kruskal's Algorithm**: Construct MST by first sorting edges from lightest to heaviest, then add edges sequentially if no cycles are formed until there are V - 1 edges.\n  * Runtime:&#x20;\n    * $$O(|E| log |E|)$$ (unsorted edges)\n    * $$O(|E| log* |V|)$$ (sorted edges)&#x20;\n",
        ],
        "25.-minimum-spanning-trees/25.1-msts-and-cut-property.md": [
            "1. # 25.1 MSTs and Cut Property",
            "2. Before we dive into the chapter, let's hear a couple words from Professor Hug on MSTs",
            '3. {% embed url="https://youtu.be/vnKK38JS9Ik" %}\nLet\'s do a quick warmup!\n{% endembed %}',
            "4. ",
            '5. {% embed url="https://youtu.be/VwwWsr4MLME" %}\nSpanning Tree Definition\n{% endembed %}',
            "6. ",
            '7. {% embed url="https://www.youtube.com/watch?v=r_4Ei251fDU" %}\nSpanning Tree Usefulness\n{% endembed %}',
            "8. ",
            '9. {% embed url="https://www.youtube.com/watch?v=50K-QvOHfOE" %}\nMSTs vs SPTs\n{% endembed %}',
            "10. ",
            '11. ## Minimum Spanning Trees <a href="#minimum-spanning-trees" id="minimum-spanning-trees"></a>',
            '12. \\\nA minimum spanning tree (MST) is the lightest set of edges in a graph possible such that all the vertices are connected. Because it is a tree, it must be connected and acyclic. And it is called "spanning" since all vertices are included.',
            "13. In this chapter, we will look at two algorithms that will help us find a MST from a graph.",
            "14. Before we do that, let's introduce ourselves to the Cut Property, which is a tool that is useful for finding MSTs.",
            "15. ",
            '16. ### Cut Property <a href="#cut-property" id="cut-property"></a>',
            "17. \\\nWe can define a **cut** as an assignment of a graph\u2019s nodes to two non-empty sets (i.e. we assign every node to either set number one or set number two).",
            "18. We can define a **crossing edge** as an edge which connects a node from one set to a node from the other set.",
            "19. With these two definitions, we can understand the **Cut Property**; given any cut, the minimum weight crossing edge is in the MST.",
            "20. ![](https://joshhug.gitbooks.io/hug61b/content/assets/Screen%20Shot%202019-04-14%20at%208.57.22%20PM.png)",
            "21. The proof for the cut property is as follows: Suppose (for the sake of contradiction) that the minimum crossing edge _e_ were not in the MST. Since it is not a part of the MST, if we add that edge, a cycle will be created. Because there is a cycle, this implies that some other edge f must also be a crossing edge (for a cycle, if _e_ crosses from one set to another, there must be another edge that crosses back over to the first set). Thus, we can remove _f_ and keep _e_, and this will give us a lower weight spanning tree. But this is a contradiction because we supposedly started with a MST, but now we have a collection of edges which is a spanning tree but that weighs less, thus the original MST was not actually minimal. As a result, the cut property must hold.\n",
        ],
        "25.-minimum-spanning-trees/README.md": [
            "1. ---\ndescription: By Teresa Luo and Mihir Mirchandani\n---",
            "2. # 25. Minimum Spanning Trees",
            "3. ",
        ],
        "25.-minimum-spanning-trees/25.2-prims-algorithm.md": [
            "1. ---\ndescription: Finding MST.\n---",
            "2. # 25.2 Prim's Algorithm",
            "3. One way to find the MST of a graph is the Prim's Algorithm. In this section, we will discuss both the conceptual and concrete implementation of this algorithm, as well as its runtime analysis.&#x20;",
            "4. ## Conceptual Visualization",
            '5. {% embed url="https://www.youtube.com/watch?v=ZCMTccvfaTQ" %}',
            "6. Prim's Algorithm is one way to find a MST from a graph. It is as follows:",
            "7. 1. Start from some arbitrary node.\n2. Repeatedly add the shortest edge that has **one node inside the MST in construction.**\n3. Repeat until there are V - 1 edges.",
            "8. ### Why does it work?",
            "9. Prim's algorithm works because at all stages of the algorithm, we can reason as follows:&#x20;",
            "10. * Consider dividing all the nodes in the graph into two sets:\n  * Set 1: nodes that are part of the existing MST that's under construction\n  * Set 2: all other nodes\n* According to the algorithm, we always add the **lightest, or minimally weighted, edge** that crosses this cut.\n* By **the Cut Property**, the added edge is necessarily part of the final MST.",
            "11. ## Implementation",
            '12. {% embed url="https://www.youtube.com/watch?v=JoS9ZegarJs" %}',
            "13. Essentially, this algorithm runs via the same mechanism as [Dijkstra's algorithm](../24.-shortest-paths/24.2-dijkstras-algorithm.md).&#x20;",
            "14. The only difference is that while Dijkstra's considers candidate nodes by their distance from the source node, Prim's looks at each candidate node's **distance from the MST under construction.**",
            "15. ## Runtime Analysis&#x20;",
            "16. Because this algorithm runs through the same mechanism as Dijkstra's algorithm, its runtime is also identical to Dijkstra's:",
            "17. $$\nO((|V| + |E| )log(|V|))\n$$",
            "18. Remember, this is because we need to add to a priority queue fringe once for every edge we have, and we need to dequeue from it once for every vertex we have.\n",
        ],
        "25.-minimum-spanning-trees/25.3-kruskals-algorithm.md": [
            "1. ---\ndescription: Finding MST.\n---",
            "2. # 25.3 Kruskal's Algorithm",
            "3. Another algorithm that finds the MST of a graph is Kruskal's Algorithm. Instead of constructing the MST by traversing through the **nodes** like Prim's Algorithm, Kruskal's Algorithm finds the MST by traversing through the **edges**.&#x20;",
            "4. ## Conceptual Visualization and Implementation",
            '5. {% embed url="https://www.youtube.com/watch?v=hSf_jir40ho" %}',
            "6. The algorithm is as follows:",
            "7. 1. Sort all the edges from lightest to heaviest.\n2. Taking one edge at a time (in sorted order), add it to the MST under construction if doing so **does not introduce a cycle.**\n3. Repeat until there are V - 1 edges.",
            "8. ### Why does it work?",
            "9. Kruskal's algorithm works because of the following reasons:",
            '10. * Recall the two "sets" introduced in last section. Any edge we add to the MST will be connecting one node from "set one" (nodes that are in the MST under construction), and another node from "set two" (all other nodes).\n* The added edge is not part of cycle because we are only adding an edge if it does not introduce a cycle.\n* By looking at edge candidates in order from lightest to heaviest, the added edge must be the lightest edge across this cut. (if there was a lighter edge that would be across this cut, it would have been added before this, and adding this one would cause a cycle to appear).&#x20;',
            "11. Hence, this algorithm works also by **the Cut Property.**",
            "12. ## Kruskal's vs Prim's",
            '13. {% embed url="https://www.youtube.com/watch?v=vmWSnkBVvQ0" %}',
            "14. It's important to note that the MST returned by Kruskal's might not be the same one returned by Prim's, but both algorithms will always return a MST. &#x20;",
            "15. Since both are minimal (optimal), they will both give valid optimal answers (they are tied as equally minimal / same total weight, and this is as low as it can be).",
            "16. ## Runtime Analysis",
            '17. {% embed url="https://www.youtube.com/watch?t=1s&v=4TV-b64HNaA" %}',
            "18. ### Runtime with Unsorted Edges",
            "19. Since the underlying data structures of implementing Kruskal's Algorithm are a Priority Queue (to sort the edges), and a Disjoined Set (to connect the edges), the runtime of Kruskal's Algorithm is in tandem with the runtime analysis of [Heaps](../21.-heaps-and-priority-queues/) and [WQU](../14.-disjoint-sets/14.4-weighted-quick-union-wqu.md).&#x20;",
            "20. Specifically, we are using  [Weighted Quick Union with Path Compression](../14.-disjoint-sets/14.5-weighted-quick-union-with-path-compression.md) to check whether or not an added edge will introduce a cycle.&#x20;",
            "21. The operations that this algorithm utilizes and their corresponding runtime are:",
            "22. * Insert: $$O(|E| log|E|)$$\n* Delete minimum: $$O(|E| log|E|)$$\n* Union: $$O(|V| log* |V|)$$\n* isConnected: $$O(|E|log*|V|)$$",
            "23. The bottleneck of the algorithm is sorting all of the edges to start (insert and delete minimum. Hence, the runtime of Kruskal's Algorithm is:",
            "24. $$\nO(|E| log |E|)\n$$",
            "25. ### Runtime with Sorted Edges",
            '26. Since there are no need to sort the edges, the "Insert" and "Delete minimum" operations of a heap are not called, where only WQU operations are called.',
            "27. Therefore, if we are given pre-sorted edges and don't have to pay for that, then the runtime is:",
            "28. $$\nO(|E| log* |V|)\n$$",
            "29. Where $$log*$$is the Ackermann function.\n",
        ],
        "25.-minimum-spanning-trees/25.5-mst-exercises.md": [
            "1. # 25.5 MST Exercises",
            "2. ## Factual",
            "3. 1. Select all valid MSTs in the diagram below.",
            '4. <figure><img src="../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>',
            "5. 2. **True/False**: It is possible that the only Shortest Path Tree is the only Minimum Spanning Tree.",
            "6. <details>",
            "7. <summary>Problem 1</summary>",
            "8. Only B. C and D have cycles; A does not span all vertices.",
            "9. </details>",
            "10. <details>",
            "11. <summary>Problem 2</summary>",
            "12. True. In a tree, there is only one SPT and MST.",
            "13. </details>",
            "14. ## Procedural",
            "15. 1. Run Prim's from `A` in the graph below. In what order are vertices visited? Break ties alphabetically.\n2. Run Kruskal's in the graph below. In what order are edges added to the tree? Break ties alphabetically.",
            '16. <figure><img src="../.gitbook/assets/image (51).png" alt=""><figcaption></figcaption></figure>',
            "17. 3. Design an algorithm to find the min-product spanning tree; ie the spanning tree with the minimum product of its edges. You may assume all edge weights are > 1.",
            "18. <details>",
            "19. <summary>Problem 1</summary>",
            "20. Order: `A B D C F G E`. Prim's repeatedly picks the lightest edge between the current tree and any node not in the tree.",
            "21. </details>",
            "22. <details>",
            "23. <summary>Problem 2</summary>",
            "24. `AB, BD, CF, FG, AC, EG`. Kruskal's keeps adding the next lightest edge as long as it doesn't form a cycle.",
            "25. </details>",
            "26. <details>",
            "27. <summary>Problem 3</summary>",
            "28. Simply take the logarithm of each edge weight, then run any MST algorithm on it. This is guaranteed to work since $$\\log a + \\log b = \\log ab$$, and minimizing the logarithm of the product is the same as minimizing the product for positive weight edges.",
            "29. </details>",
            "30. ## Metacognitive",
            "31. 1. If we add 1 to the weight of each edge in an arbitrary graph, will the MST created by Kruskal\u2019s change?\n2. **True/False**: Prim\u2019s Algorithm and Kruskal\u2019s algorithm will always return the same result. If this is true, explain why. If this is false, provide a counterexample, breaking ties alphabetically.\n3. Prove the following, known as the cycle property: Given any cycle in an edge weighted graph (all edge weights distinct), the edge of maximum weight in the cycle does not belong to the MST of the graph.",
            "32. <details>",
            "33. <summary>Problem 1</summary>",
            "34. Adding 1 to each number will not change the order of the edges when we sort them, therefore we will get the same MST.",
            "35. </details>",
            "36. <details>",
            "37. <summary>Problem 2</summary>",
            "38. False. A counterexample is the following graph:",
            "39. ![](<../.gitbook/assets/image (59).png>)",
            "40. Prim\u2019s starting from A will select AD, BD, and CD, whereas Kruskals will select AD, BC, and BD.",
            "41. </details>",
            "42. <details>",
            "43. <summary>Problem 3</summary>",
            "44. Suppose, for contradiction, the maximum-weight edge `f` in a cycle is present in the MST.&#x20;",
            "45. ![](<../.gitbook/assets/image (150).png>)",
            "46. Removing `f` disconnects our MST `T`. Form a cut with the two sides of the MST after `f` is removed.",
            "47. Since `f` is part of a cycle, there must be also some edge `e` crossing that same cut. However, if we replace `f` with `e`, we now have a spanning tree that has less weight than our MST `T`.",
            "48. This is a contradiction, so the maximum weight edge in a cycle cannot be part of the MST.",
            "49. </details>\n",
        ],
        "38.-compression-and-complexity/38.2-prefix-free-codes.md": [
            "1. # 38.2 Prefix-free Codes",
            "2. Consider the representation of English text in Java. We represent text as a sequence of characters, each taking 8 bits of memory.",
            "3. One easy way to compress, then, is to simply use less than 8 bits per character. To do this, we have to decide which **codewords** (bit sequences) go with each **symbol** (character).",
            "4. ## Mapping Alphanumeric Symbols",
            "5. ### Morse Code",
            "6. As an introductory example, consider the Morse code alphabet. Looking at the alphabet below, what does the sequence \u2013 \u2013 \u2022 \u2013 \u2013 \u2022 represent? It\u2019s ambiguous! The same sequence of symbols can represent either MEME, or GG, depending on what you choose \u2013 \u2013 \u2022 to represent",
            '7. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-24 at 5.44.37 PM.png" alt=""><figcaption><p>Ambiguity in morse code</p></figcaption></figure>',
            "8. In real usage, operators must pause between codewords to indicate a break. The pause acts as an implicit third symbol, but we can't encode this real-time information into our code.",
            "9. ### Prefix-free Codes",
            "10. An alternate strategy to avoid the need for real-time is to use **prefix-free codes**. In a prefix-free code, no codeword is a prefix of any other. In the Morse Code example, there would be no confusion whether the \u2013 \u2013 in the pattern \u2013 \u2013 \u2022 \u2013 \u2013 \u2022 is supposed to represent M, or the start of G.",
            "11. Let's represent Morse code as a tree of codewords leading to symbols. As we can see from the tree, several symbols have representations that are prefixes of other symbols.",
            '12. <figure><img src="../.gitbook/assets/image (4).png" alt=""><figcaption><p>Morse code is not prefix-free.</p></figcaption></figure>',
            "13. As an example of an (arbitrary) prefix-free code, consider the following encoding:",
            '14. <figure><img src="../.gitbook/assets/image (64).png" alt=""><figcaption><p>One prefix-free code.</p></figcaption></figure>',
            "15. The following code is also prefix-free:",
            '16. <figure><img src="../.gitbook/assets/image (60).png" alt=""><figcaption><p>Another prefix-free code.</p></figcaption></figure>',
            "17. Note that some codes are more efficient for certain strings than others: in the first representation, `I ATE` uses less bits than the second code. However, this is highly dependent on what string we're trying to encode.\n",
        ],
        "38.-compression-and-complexity/38.1-introduction-to-compression.md": [
            "1. # 38.1 Introduction to Compression",
            "2. ",
            "3. As an introduction to compression, consider the processes of creating and unzipping a zip file.",
            '4. <pre class="language-bash"><code class="lang-bash">$ zip mobydick.zip mobydick.txt \nadding: mobydick.txt (deflated 59%)',
            "5. $ ls -l\n<strong>-rw-rw-r-- 1 jug jug 643207 Apr 24 10:55 mobydick.txt\n</strong>-rw-rw-r-- 1 jug jug 261375 Apr 24 10:55 mobydick.zip\n</code></pre>",
            "6. Note that before and after unzipping, the file size changes!",
            "7. ## Compression Model 1: Algorithms on Bits",
            "8. In our first model of compression, we consider compression as applying a _compression algorithm_ on a sequence of bits. To reverse the compression, we apply the inverse _decompression algorithm._",
            '9. <figure><img src="../.gitbook/assets/image (25).png" alt=""><figcaption><p>Compression and decompression.</p></figcaption></figure>',
            "10. Say you had a text file called `example.txt`. If you were to zip that text file, you'd get `example.zip`, a zip file with a size much lesser than the original `example.txt` file. This is the main idea behind compression--a technique used to reduce file size.",
            "11. Then, if you were to unzip `example.zip` into a file called `unzippedexample.txt`, you would notice no difference between `example.txt` and `unzippedexample.txt.` This is an indicator of **lossless** compression, where no information is lost.\n",
        ],
        "38.-compression-and-complexity/38.3-shannon-fano-codes.md": [
            "1. # 38.3 Shannon-Fano Codes",
            "2. Shannon-Fano codes are an approach to create prefix-free codes based on a set of symbols/characters and their probabilities. The main idea is that we want shorter prefix-free codes for more popular characters, and longer codes for lesser used characters.",
            "3. The algorithm is:&#x20;",
            "4. * Count relative frequencies of all characters in a text.\n* Split into \u2018left\u2019 and \u2018right halves\u2019 of roughly equal frequency.\n  * Left half gets a leading zero. Right half gets a leading one.\n  * Repeat.",
            "5. At the end, you will get a tree as shown below, with shorter paths for characters with a higher frequency, and longer paths for characters with a lower frequency.",
            '6. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-24 at 6.06.15 PM.png" alt=""><figcaption></figcaption></figure>',
            "7. However, Shannon-Fano coding is NOT optimal, so it is not used very often.\n",
        ],
        "38.-compression-and-complexity/38.5-compression-theory.md": [
            "1. # 38.5 Compression Theory",
            "2. ## Compression Ratios",
            "3. * The goal of data compression is to reduce the size of a sequence of data while retaining as much information as possible. For example, the letter `e` appears more frequently in the English dictionary than `z`, so we would want to represent `e` with smaller bits.\n* **Compression ratio** is a measure of how much the size of the compressed data differs from the original data.&#x20;\n  * **Huffman Coding** is a compression technique that represents common symbols with smaller numbers of bits, resulting in a more efficient encoding.&#x20;\n  * **Run-length encoding** is another compression technique that replaces repeated characters with the character itself and the number of times it occurs.&#x20;\n  * **LZW** is a compression technique that searches for common repeated patterns in the input and replaces them with a shorter code.&#x20;\n* The general idea behind most compression techniques is to exploit any existing redundancy or order within the sequence to reduce the size of the data. However, if a sequence has no existing redundancy or order, compression may not be possible.",
            "4. ## Self-Extracting Bits",
            "5. * Self-extracting bits is a compression technique that wraps the compressed bits and the decompression algorithm into a **single sequence of bits**.\n* The goal is to simplify the compression and decompression process by combining the two steps into one. Self-extracting bits can be used to create executable files that can be run on any system with an interpreter (e.g. Java interpreter).",
            '6. <figure><img src="../.gitbook/assets/Screenshot 2023-04-24 at 9.34.44 PM.png" alt=""><figcaption></figcaption></figure>',
            "7. ## HugPlant Example",
            "8. * To compress an image file like `hugplant.bmp`, we can break it into 8-bit chunks and Huffman encode each chunk.\n* We package the compressed data plus decoder into a single self-extracting `.java` file, represented as a `byte[]` array.\n* When the `byte[]` array is passed to an interpreter, the interpreter executes the Huffman decoding algorithm and produces the original `hugplant.bmp` image file.\n* The size of the compressed bitstream and the Huffman decoding algorithm combined is **smaller** than the original image file, making it more efficient to store and transmit.\n* However, the receiver of the compressed file must have access to the appropriate interpreter to decode and reconstruct the original image.\n",
        ],
        "38.-compression-and-complexity/38.6-lzw-compression.md": [
            "1. # 38.6 LZW Compression",
            "2. ## Key Idea",
            "3. The LWZ approach is based on the idea of exploiting redundancy and patterns in the input data to achieve compression. Basically, each codeword can represent multiple symbols.",
            "4. For example, imagine a sequence of symbols `ABCABCA`. In traditional compression, each symbol would be mapped to a fixed-length codeword, resulting in a compressed sequence like `010001001000100100`. With the LWZ approach, the codewords can be based on patterns in the input data. In this case, the algorithm might start with a codeword table. (A --> 0, B --> 1, C --> 2). This could result in a compressed sequence looking something like `01201201`. By allowing for codewords that can represent multiple symbols, the LWZ approach can achieve more efficient compression than traditional approaches.",
            "5. ## Algorithm",
            "6. * The algorithm starts with a simple codeword table where each codeword corresponds to a single symbol.\n* Whenever a codeword is used, a new codeword is created by concatenating the previous codeword with the next symbol.\n* The algorithm does not specify what happens when the codeword table becomes full, but there are many variants of the algorithm that handle this differently.\n* A neat fact about the LWZ approach is that it is possible to reconstruct the codeword table from the compressed bitstream alone, without needing to send the table along with the compressed data.\n* LWZ decompression [demo](https://docs.google.com/presentation/d/1U8XO6CWfcU4QgrFOZmGjAgmaKxLc8HXk6qB1JQVlqrg/edit#slide=id.g53705ba95\\_0259).",
            "7. ## Fun Facts",
            "8. * The algorithm is named after its inventors, Lempel, Ziv, and Welch.\n* The LWZ algorithm is used as a component in many compression tools, including .gif files, .zip files, and more.\n* The LWZ algorithm was once controversial due to attempts to enforce licensing fees, but the patent expired in 2003.\n",
        ],
        "38.-compression-and-complexity/README.md": [
            "1. ---\ndescription: By Dhruti Pandya and Stella Kaval\n---",
            "2. # 38. Compression and Complexity",
            "3. ",
        ],
        "38.-compression-and-complexity/38.8-exercises.md": [
            "1. # 38.8 Exercises",
            "2. ## Factual",
            "3. 1. Suppose we build a Shannon Fano or Huffman code for the text of this question including spaces and punctuation characters. Which characters would have the longest code?\n2. What two ways could we represent a Huffman code for characters in Java?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. `?` and `.`, since both are only used once in the above sentence.",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. A `HashMap<Character, BitSequence>` or a `BitSequence[]`. Note that the two are equivalent in Java because a Character is a number.",
            "11. </details>",
            "12. ## Procedural",
            "13. 1. Suppose we have a string `abcdefg` which repeats 1000 times. How many bits would be in the compressed bitstream?",
            "14. <details>",
            "15. <summary>Problem 1</summary>",
            "16. Since all 8 characters are equal in frequency, we get a balanced binary tree as our Huffman encoding, so all codewords are 3 bits long. 1000 \\* 8 \\* 3 = 24000 bits.",
            "17. </details>",
            "18. ## Metacognitive",
            "19. 1. Using the idea of self-extracting bits, come up with an encoding for the sequence `abdefg` repeated 1000 times that uses less than 2000 bits.&#x20;",
            "20. <details>",
            "21. <summary>Problem 1</summary>",
            "22. The idea of self-extracting bits includes writing code or an interpreter that can generate the original uncompressed sequence. This can be done with the following code:",
            "23. ```java\npublic class Sequence {\n    public static void main(String[] args) {\n        for (int i = 0; i < 1000; i++) {\n            for (int j = 0; j < 8; j++) {\n                System.out.print(String.format(\"%c\", 'a' + j));\n            }\n        }\n    }\n}\n```",
            "24. This code uses exactly 239 characters, or 1912 bits. This demonstrates the power of the self-extracting bits model: compare this to the 24000 bits required for a Huffman code.",
            "25. </details>\n",
        ],
        "38.-compression-and-complexity/38.4-huffman-coding-conceptuals.md": [
            "1. # 38.4 Huffman Coding Conceptuals",
            "2. ## Core Idea",
            "3. Huffman coding takes a bottom-up approach to prefix-free codes, as opposed to the top-down approach taken by Shannon-Fano codes. The algorithm is as follows:",
            "4. * Calculate relative frequencies.\n  * Assign each symbol to a node with weight = relative frequency.\n  * Take the two smallest nodes and merge them into a super node with weight equal to sum of weights.\n  * Repeat until everything is part of a tree.",
            '5. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-24 at 6.12.04 PM.png" alt=""><figcaption><p>Huffman Coding: step by step example (Part 1)</p></figcaption></figure>',
            '6. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-24 at 6.12.56 PM.png" alt=""><figcaption><p>Huffman Coding: step by step example (Part 2)</p></figcaption></figure>',
            "7. ## Data Structures",
            "8. Let's now think about the data structures we would use for the encoding and decoding processes of the Huffman Coding process. Recall that encoding will translate symbols to code words and decoding will do the opposite. An example is as follows:",
            "9. * Encoding translates `I ATE` into `0000011000100101.`\n* Decoding translates `0000011000100101` into `I ATE`.",
            "10. <details>",
            "11. <summary>For encoding (bitstream to compressed bitstream), what data structure would we use? </summary>",
            "12. There are two options!",
            "13. 1. **HashMap/TreeMap**: create a map from character to bit sequence, calling the `get()` method to look up each character\n2. **Array**: Each index of the array would represent the character, with the bit sequence in that slot of the array. Recall that each character is just an integer. For example, the letter `A` is `65`).&#x20;",
            "14. What's the difference? Arrays are faster than maps but might use more memory if indices are unused.",
            "15. </details>",
            "16. <details>",
            "17. <summary>For decoding (compressed bitstream back to bitstream), what data structure would we use?</summary>",
            "18. There is really only one good data structure that can help us find longest prefixes of bit streams.",
            "19. **Trie**: Here, we can use a Binary Trie with numbers 0 and 1. When we get the bitstream, the trie allows easy lookup to the longest prefix. Below is an image that demonstrates how the trie could look.",
            '20. <img src="../.gitbook/assets/Screenshot 2023-04-24 at 8.50.11 PM.png" alt="" data-size="original">',
            "21. </details>",
            "22. ## In Practice",
            "23. Now that we have talked about what Huffman Coding is, let's talk about some practical issues that arise when we want to use it. There are two main philosophies we can use.",
            "24. ### Corpus",
            "25. * For each input type (English text, Chinese text, images), assemble huge numbers of sample inputs for each category. Use corpus to create a standard code for English, Chinese, etc.&#x20;\n* A corpus is a collection of pieces of language to be used as a sample of the language. Below is an example where we specify that we want to use the `ENGLISH` corpus to compress `mobydick.txt`",
            "26. ```\n$ java HuffmanEncodePh1 ENGLISH mobydick.txt\n```",
            "27. **Problem:** Suboptimal encoding, which means our corpus does not exactly match our input. What this means in the context of this example is that `mobydick.txt` might not match up well with the general frequencies of `ENGLISH` texts and instead might have some other quirks or specifications from the author.",
            "28. ### Unique Code",
            "29. * For every possible input file, create a unique code just for that file. Then, when someone receives that file, they will know how to decode it using the code we send along the compressed file.&#x20;\n* As seen in the example below, we do not specify a corpus and we send along another file to help the decoding process for that specific file.",
            "30. ```\n$ java HuffmanEncodePh2 mobydick.txt\n```",
            "31. **Problem**: This approach requires us to use extra space for the codeword table in the compressed bitstream. However, this generally works better than the corpus philosophy so is used commonly in the real world.",
            "32. &#x20;\n",
        ],
        "38.-compression-and-complexity/38.7-summary.md": [
            "1. # 38.7 Summary",
            "2. **Compression Model #1: Algorithms Operating on Bits.** Given a sequence of bits B, we put them through a compression algorithm C to form a new bitstream C(B). We can run C(B) through a corresponding decompression algorithm to recover B. Ideally, C(B) is less than B.",
            "3. **Variable Length Codewords.** Basic idea: Use variable length codewords to represent symbols, with shorter keywords going with more common symbols. For example, instead of representing every English character by a 8 bit ASCII value, we can represent more common values with shorter sequences. Morse code is an example of a system of variable length codewords.",
            "4. **Prefix Free Codes.** If some codewords are prefixes of others, then we have ambiguity, as seen in Morse Code. A prefix free code is a code where no codeword is a prefix of any other. Prefix free codes can be uniquely decoded.",
            "5. **Shannon-Fano Coding.** Shannon-Fano coding is an intuitive procedure for generating a prefix free code. First, one counts the occurrence of all symbols. Then you recursively split characters into halves over and over based on frequencies, with each half either having a 1 or a 0 appended to the end of the codeword.",
            "6. **Huffman Coding.** Huffman coding generates a provably optimal prefix free code, unlike Shannon-Fano, which can be suboptimal. First, one counts the occurrence of all symbols, and create a \u201cnode\u201d for each symbol. We then merge the two lowest occurrence nodes into a tree with a new supernode as root, with each half either having a 1 or a 0 appended to the beginning of the codeword. We repeat this until all symbols are part of the tree. Resulting code is optimal.",
            "7. **Huffman Implementation.** To compress a sequence of symbols, we count frequencies, build an encoding array and a decoding trie, write the trie to the output, and then look up each symbol in the encoding array and write out the appropriate bit sequence to the output. To decompress, we read in the trie, then repeatedly use longest prefix matching to recover the original symbol.",
            "8. **General Principles Behind Compression.** Huffman coding is all about representing common symbols with a small number of bits. There are other ideas, like run length encoding where you replace every character by itself followed by its number of occurrences, and LZW which searches for common repeated patterns in the input. More generally, the goal is to exploit redundancy and existing order in the input.",
            "9. **Universal Compression is Impossible.** It is impossible to create an algorithm that can compress any bitstream by 50%. Otherwise, you could just compress repeatedly until you ended up with just 1 bit, which is clearly absurd. A second argument is that for an input bitstream of say, size 1000, only 1 in 2^499 is capable of being compressed by 50%, due to the pigeonhole principle.",
            "10. **Compression Model #2: Self Extracting Bits.** Treating the algorithm and the input bitstream separately (like we did in model #1) is a more accurate model, but it seems to leave open strange algorithms like one in which we simply hardcode our desired output into the algorithm itself. For example, we might have a .java decompression algorithm that has a giant `byte[]` array of your favorite TV show, and if the algorithm gets the input `010`, it outputs this `byte[]` array.",
            "11. In other words, it seems to make more sense to include not just the compressed bits when considering the size of our output, but also the algorithm used to do the decompression.",
            "12. One conceptual trick to make this more concrete is to imagine that our algorithm and the bits themselves are a single entity, which we can think of a self-extracting bit sequence. When fed to an interpreter, this self-extracting bit sequence generates a particular output sequence.",
            "13. **Hugplant Example.** If we have an image file of something like the hugplant.bmp from lecture, we can break it into 8 bit chunks and then Huffman encode it. If we give this file to someone else, they probably won\u2019t know how to decompress it, since Huffman coding is not a standard compression algorithm supported by major operating systems. Thus, we also need to provide the Huffman decoding algorithm. We could send this as a separate .java file, but for conceptual convenience and in line with compression model #2, we\u2019ll imagine that we have packaged our compressed bit stream into a `byte[]` array in a .java file. When passed to an interpreter, this bitstream yields the original hugplant.bmp, which is 4 times larger than the compressed bitstream + huffman interpreter.\n",
        ],
        "20.-hashing-ii/20.4-mutable-vs.-immutable-types.md": [
            "1. # 20.4 Mutable vs. Immutable Types",
            "2. ## Immutable Data Types",
            '3. {% embed url="https://www.youtube.com/watch?index=6&list=PL8FaHk7qbOD637Q-6p7nn5dKz1tK6WAjJ&v=fHuU2zeBwIs" %}',
            "4. An immutable data type is one for which an instance cannot change in any observable way after instantiation.",
            "5. Examples:",
            "6. * Mutable: ArrayDeque, Percolation.\n* Immutable: Integer, String, Date.",
            "7. ```java\npublic class Date {\n   public final int month;\n   public final int day;\n   public final int year;\n   private boolean contrived = true;\n   public Date(int m, int d, int y) {\n       month = m; day = d; year = y;\n   }\n}\n```",
            "8. The `final` keyword will help the compiler ensure immutability.",
            "9. * `final` variable means you may assign a value once (either in the constructor of the class or in the initializer), but after it can never change.\n* `final` is neither sufficient nor necessary for a class to be immutable.",
            "10. ### Advantages vs. Disadvantages of Immutability:",
            "11. Advantage: Less to think about; avoids bugs and makes debugging easier.",
            "12. * Analogy: Immutable classes have some buttons you can press / windows you can look inside. Results are ALWAYS the same, no matter what.",
            "13. Disadvantage: Must create a new object anytime anything changes.",
            "14. * Example: String concatenation is slow!",
            '15. <figure><img src="../.gitbook/assets/image (161).png" alt=""><figcaption></figcaption></figure>',
            "16. ## Mutable Types",
            '17. {% embed url="https://www.youtube.com/watch?index=7&list=PL8FaHk7qbOD637Q-6p7nn5dKz1tK6WAjJ&v=7lqlRGO_mGY" %}',
            "18. ### **Mutable HashSet Keys**",
            "19. In principle, we can create a HashSet\\<List>.",
            "20. Weird stuff happens if:",
            "21. * We insert a List into a HashSet.\n* We later mutate that List.",
            "22. **Key Point**: Never mutate (modify) an object being used as a key. Incorrect results arise, and the item gets lost. [The slides for the Hashing II Lecture ](https://docs.google.com/presentation/d/1U\\_-RQCJB3j9B-k-kY8I4nS-FuxIvO8EgVIrOthx2InU/edit#slide=id.g2165b69ef3f\\_0\\_291)provide a very thorough visual example of this point.",
            "23. ",
            "24. ### Actual Implementation of HashSet/Map in Java",
            "25. In this last section, Professor Hug gives a quick walkthrough of the code for HashSets and HashMaps in Java's official implementation.",
            '26. {% embed url="https://youtu.be/pc5SDicUqWo" %}\n',
        ],
        "20.-hashing-ii/20.1-hash-table-recap-default-hash-function.md": [
            '1. ---\ndescription: >-\n  "The whole point is that we have a bunch of lists that are all short." -\n  Professor Hug.\n---',
            "2. # 20.1 Hash Table Recap, Default Hash Function",
            '3. {% embed url="https://youtu.be/iL3z4nSsGBc" %}\nHash Table Recap, Default Hash Function\n{% endembed %}',
            "4. Let's continue understanding Hashing. We've now seen implementations for sets and maps.",
            "5. 1.  Red-Black Based Tree Approach: TreeSet/TreeMap",
            "6.     \u2022 requires the items to be comparable (the notion of less or greater than)",
            "7.     \u2022 logarithmic time complexity\n2.  HashTable based approach: HashSet/HashMap",
            "8.     \u2022 constant time operations if the hashCode spreads the item nicely (few collisions)",
            '9. Recall that with a hash table, the idea is that for any piece of data, like a String (or any other type of object) we want to store in our hash table, we need to turn this into a number called a hash code. So a string like "Mihir" can be converted to -2101281024.&#x20;',
            '10. {% hint style="info" %}\nFun Fact: And that integer is anything between about negative two billion and about two billion, the space of all Java Integers. This range yields about 4 billion integers or 2^32 integers. If you are interested in why this is the case, please take CS 61C!\n{% endhint %}',
            "11. Once we have our number, we want to convert this number to our bucket number (i.e. which of the many linked lists I want to add this new entry to). In the first example, we will use the`Math.floorMod(x, 4)`, since the length of the underlying buckets array has length 4.",
            '12. If we take the converted hash code for "Mihir", -2101281024, and then mod this by 4, we get 0. This reduces our hash code, -2101281024 to a valid index, 0. This means that we would use the 0th bucket to place our data, "Mihir", in our LinkedList at the 0th bucket.',
            "13. > You can essentially think of Java HashTables as just an array of LinkedLists categorized under bucket labels and hopefully have better performance by utilizing these LinkedLists.&#x20;",
            "14. But how many LinkedLists/Buckets should we use? This is an important question, because as we insert more and more items into our buckets, the length of the LinkedLists will inevitably grow, which in turn compromises the runtime for the hash table's operations.&#x20;",
            '15. For example, let\'s say we inserted strings like "Mihir", "Mirchandani", "loves", and "61B", all of which yielded hash codes that were divisible by 4. In this case, all strings would be put into the 0th index bucket and all strings would be inserted into the same LinkedList. What happens to the runtime for a search operation? Well, we would have to search through an entire LinkedList to look up our data! This runs in linear time and is too slow for HashMap\'s famous title of holding fast lookup times.',
            "16. Last time, we saw that we can have a variable number of LinkedLists. The idea is that we resize that array of linked lists whenever the load factor (N/M, where N is the number of elements in our table and M is the number of buckets) exceeds some constant. Java picks 0.75 as we'll see later. This prevents collisions from happening too frequently. So long as our items are spread out nicely, between the buckets, the LinkedLists at each bucket for the most part have a very small size, which means we'll on average get constant run time!&#x20;",
            "17. Example: If the HashTable has load factor 3, and our `hashCode()` function spreads out the entries evenly, we are going to end up with just 3 items per bucket, and our search operation takes $$\\Theta(1)$$time.&#x20;",
            "18. ",
            "19. ### Comparing Data Structure Run Times!&#x20;",
            "20. |                                               | contains(x) | add(x)   |\n| --------------------------------------------- | ----------- | -------- |\n| Bushy BSTs                                    | \u0398(log N)    | \u0398(log N) |\n| Separate Chaining Hash Table with NO resizing | \u0398(N)        | \u0398(N)     |\n| Separate Chaining Hash Table with resizing    | \u0398(1)        | \u0398(1)     |",
            "21. ",
            "22. ",
            "23. ",
            "24. ",
        ],
        "20.-hashing-ii/20.2-distribution-by-other-hash-functions.md": [
            '1. ---\ndescription: >-\n  HashMaps/Tables have fast lookup times, but behind that "superpower" is a hash\n  function.\n---',
            "2. # 20.2 Distribution By Other Hash Functions",
            '3. {% embed url="https://youtu.be/YWVVvmzwy4Q" %}\nDistribution by other Hash Functions\n{% endembed %}',
            "4. ",
            "5. Suppose our `hashCode()` implementation simply returns 0.",
            "6. ```\n@Override\npublic int hashCode() {\n    return 0;\n}\n```",
            "7. <details>",
            "8. <summary>What distribution do we expect?</summary>",
            "9. We would expect all of the items in our Hash Table to be in bucket 0. As we discussed in the [previous section](20.1-hash-table-recap-default-hash-function.md), our Hash Table would place all elements in the 0th bucket because the hashCode tells it to. In the 0th bucket, there will be a LinkedList of all elements from the data yielding a very inefficient linear lookup time compared to the constant time we are expecting. No matter what key we provide, our hashCode always tells the HashMap to only add to the 0th bucket which is why we get this long LinkedList. So what do we do to make sure we get constant lookup time? We use a better hash function!",
            "10. </details>",
            "11. In order to get a more even distribution, what we can do is something to what we tried in the [previous section](20.1-hash-table-recap-default-hash-function.md) where we utilize modulo. Let's say that we define the size of our Hash Table to have 4 buckets. This means it has 4 corresponding LinkedLists and 4 bucket indices labeled {0, 1, 2, 3}. The modding is not required in our `hashCode()` function as it is being done for us in the hash table to guarantee we can add to that bucket. As we said the hash code could really be any integer in the range of 4 billion unique values!",
            "12. By using modulo, we ensure that our hashcode yields a number that can be represented as an index and clearly identifies which LinkedList to add to. Additionally, when adding a series of numbers at once, we see that we get an even distribution of numbers in our LinkedList yielding a constant lookup time.",
            "13. ```\n@Override\npublic int hashCode() {\n    return num;\n}\n```",
            "14. This hash function should yield a much more even distribution! Objects with different `num` will now be more spread out across the buckets instead of all living in the 0th bucket. If our class does not explicitly override the `hashCode()` function, Java will use the default implementation, which returns the object's address in memory as its hash code!",
            "15. ",
            "16. ## Why Bother With Custom Hash Functions?",
            '17. {% embed url="https://youtu.be/4_gRlDS9AMQ" %}',
            "18. Let's discuss if the default hashCode function is a good hashCode function! It actually is a good spread as it relies on the fact that different objects will live in different places in the memory, and the memory address is effectively random. We will get a good distribution, since objects are basically assigned random indices to insert into the hash table.",
            "19. This really raises an interesting question: why do we care about other custom hash functions when the default hashcode gets good spread? We'll read about this in the [next section.](20.3-contains-and-duplicate-items.md)&#x20;\n",
        ],
        "20.-hashing-ii/20.3-contains-and-duplicate-items.md": [
            "1. # 20.3 Contains & Duplicate Items",
            "2. ## Contains",
            '3. {% embed url="https://www.youtube.com/watch?index=4&list=PL8FaHk7qbOD637Q-6p7nn5dKz1tK6WAjJ&v=O6UlxmISOx4" %}',
            "4. ### The `equals()` Method for a ColoredNumber Object",
            "5. Suppose the `equals()` method for ColoredNumber is as below, i.e. two ColoredNumbers are equal if they have the same num.",
            "6. ```java\n@Override\npublic boolean equals(Object o) {\n   if (o instanceof ColoredNumber otherCn) {\n       return this.num == otherCn.num;\n   }\n   return false;\n}\n```",
            "7. ### HashSet Behavior for Checking `contains()`",
            "8. Suppose the `equals()` method for ColoredNumber is on the previous slide, i.e. two ColoredNumbers are equal if they have the same num.&#x20;",
            "9. ```java\nint N = 20;\nHashSet<ColoredNumber> hs = new HashSet<>();\nfor (int i = 0; i < N; i += 1) {\n    hs.add(new ColoredNumber(i));\n}\n```",
            "10. Suppose we now check whether 12 is in the hash table.",
            "11. ```java\nColoredNumber twelve = new ColoredNumber(12);\nhs.contains(twelve); //returns true\n```",
            '12. {% hint style="info" %}\nWhat do we **expect** to be returned by `contains`?\n{% endhint %}',
            "13. <details>",
            "14. <summary>Answer</summary>",
            "15. We expect the `contains` call to be true, all `12`s are created equal!",
            "16. </details>",
            "17. ### Finding an Item Using the Default Hashcode",
            "18. Suppose we are using the default hash function (uses memory address):",
            "19. ```java\nint N = 20;\nHashSet<ColoredNumber> hs = new HashSet<>();\nfor (int i = 0; i < N; i += 1) {\n   hs.add(new ColoredNumber(i));\n}\nColoredNumber twelve = new ColoredNumber(12);\nhs.contains(twelve); // returns ??\n```",
            "20. which yields the table below:",
            "21. ![](https://lh4.googleusercontent.com/cFvLVhOYg31lSg1a8moftrr30qpThw3Bc7drJWVLNrSTCgTdO4yxjis1epmRlRMLWIkh73alL6OrblQxqGGrjMo1XMoOfPjSFH3tPDDbaxSbLXe1-HXJcfQOglsU4Dp74PiDDqMWXNOmYeQu\\_D7-l\\_B2SBcdTVohYMYLZdnzzMVn8\\_hAbvSKDAKyC-\\_ev3nN=s2048)",
            "22. Suppose equals returns true if two ColoredNumbers have the same num (as we've defined previously).",
            '23. {% hint style="info" %}\nWhat is actually returned by `contains`?\n{% endhint %}',
            "24. <details>",
            "25. <summary>Answer</summary>",
            "26. Returns false with probability 5/6ths.",
            "27. Default `hashCode()` is based on memory address. equals is based on `num`.",
            "28. There are two ColoredNumber objects with `num = 12`. One of them is in the HashSet and one of them was created by the code above.",
            "29. Each memory address is random, with only a 1/6th chance they modulo to the same bucket.",
            "30. Example: If the `ColoredNumber` object `twelve` created by the code above is in memory location 6000000, its hashCode % 6 is 0. HashSet looks in bucket zero, and doesn't find 12.",
            "31. </details>",
            '32. {% hint style="info" %}\nHard Question: If the default hash code achieves a good spread, why do we even bother to create custom hash functions?&#x20;\n{% endhint %}',
            "33. <details>",
            "34. <summary>Answer</summary>",
            "35. It is necessary to have consistency between `equals()` and `hashCode()` for the hash table's operations to function.",
            "36. </details>",
            "37. **Basic rule (also definition of deterministic property of a valid hashcode):** If two objects are equal, they **must** have the same hash code so the hash table can find it.",
            "38. ## Duplicate Values",
            '39. {% embed url="https://www.youtube.com/watch?index=5&list=PL8FaHk7qbOD637Q-6p7nn5dKz1tK6WAjJ&v=3HmCkRYsAGg" %}',
            "40. ### Overriding `equals()` but Not `hashCode()`",
            "41. Suppose we have the same `equals()` method (comparing `num`), but we do not override `hashCode()`.",
            "42. ```java\npublic boolean equals(Object o) {\n   ...  return this.num == otherCn.num; ...\n}\n```",
            "43. The result of adding 0 through 19 is shown below:",
            "44. ![](<../.gitbook/assets/image (10).png>)",
            "45. ```java\nColoredNumber zero = new ColoredNumber(0);\nhs.add(zero); // does another zero appear?\n```",
            '46. {% hint style="info" %}\nWhich can happen when we call add(zero)?',
            "47. Answer Choices:",
            "48. 1. We get a 0 to bin zero.\n2. We add another 0 to bin one.\n3. We add a 0 to some other bin.\n4. We do not get a duplicate zero\n{% endhint %}",
            "49. <details>",
            "50. <summary>Answer </summary>",
            "51. 3 Choices are Correct:&#x20;",
            "52. \\#1, #3, #4.",
            "53. We get a 0 to bin zero, We add a 0 to some other bin, and we do not get a duplicate zero.",
            "54. The new zero ends up in a random bin.",
            "55. * 5/6ths chance: In bin 0, 2, 3, 4, or 5. Duplicate!\n* 1/6 chance: In bin 1, no duplicate! (`equals()` blocks it)",
            "56. </details>",
            "57. ## Key Takeaway: `equals()` and `hashCode()`",
            "58. Bottom line: If your class override equals, you should also override hashCode in a consistent manner.",
            "59. * If two objects are equal, they must always have the same hash code.\\",
            "60. \nIf you don\u2019t, everything breaks:",
            "61. * `Contains` can\u2019t find objects (unless it gets lucky).\n* `Add` results in duplicates.",
            "62. ",
        ],
        "20.-hashing-ii/README.md": [
            "1. ---\ndescription: By Mihir Mirchandani and William Lee\n---",
            "2. # 20. Hashing II",
            "3. ",
        ],
        "26.-prefix-operations-and-tries/26.4-summary.md": [
            "1. # 26.4 Summary",
            "2. The _search problem_ is to store a collection of objects such that they can be rapidly retrieved (i.e. how do we implement a Map or Set). In particular, we are interested in searching by letter or digit.",
            "3. ## Terminology",
            "4. * Length of string key usually represented by L.\n* Alphabet size usually represented by R.",
            "5. ## Tries",
            "6. Know how to insert and search for an item in a Trie. Know that Trie nodes typically do not contain letters, and that instead letters are stored implicitly on edge links. Know that there are many ways of storing these links, and that the fastest but most memory hungry way is with an array of size R. We call such tries R-way tries.",
            "7. **Advantages of Tries.** Tries have very fast lookup times, as we only ever look at as many characters as they are in the data we\u2019re trying to retrieve. However, their chief advantage is the ability to efficiently support various operations not supported by other map/set implementations including:",
            "8. * longestPrefixOf\n* prefixMatches\n* spell checking\n",
        ],
        "26.-prefix-operations-and-tries/26.3-trie-string-operations.md": [
            "1. ---\ndescription: Trie, Trie again.\n---",
            "2. # 26.3 Trie String Operations",
            "3. Tries give us the ability to have constant time lookup and insertion, but they do not always perform better than BSTs and Hash Tables. For any string, we have to traverse through every character, whereas in BSTs and Hash Tables we have access to the entire string immediately. However, Tries are _much_ more useful in the specific application of String Operations.",
            '4. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=wOrSoyxlXXg" %}',
            "5. &#x20;The main appeal of tries is the ability to efficiently support specific string operations like _prefix matching_. You can imagine why tries make this extremely efficient! Say we were trying to find the longestPrefixOf. Just take the word you're looking for, compare each character with characters in your trie until you can go no longer. Similarly, if we wanted keyWithPrefix, we can traverse to the end of the prefix and return all remaining keys in the Trie.",
            "6. Let's attempt to define a method `collect` which returns all of the keys in a Trie. The pseudocode will be as follows:",
            "7. ```\ncollect():\n    Create an empty list of results x\n    For character c in root.next.keys():\n        Call colHelp(c, x, root.next.get(c))\n    Return x\n\ncolHelp(String s, List<String> x, Node n):\n    if n.isKey:\n        x.add(s)\n    For character c in n.next.keys():\n        Call colHelp(s + c, x, n.next.get(c))\n```",
            "8. We first initialize our values inside of the parent function, and then create a recursive helper function to hold more parameters throughout the recursive calls. We only add the current string if it is a key, otherwise we concatenate the character to the string/path we are currently traversing and call the helper on the next child.",
            "9. Now we can try writing the method `keysWithPrefix` which returns all keys that contain the prefix passed in as an argument. We will borrow heavily from the collect method above.",
            '10. ```\nkeysWithPrefix(String s):\n    Find the end of the prefix, alpha\n    Create an empty list x\n    For character in alpha.next.keys():\n        Call colHelp("sa" + c, x, alpha.next.get(c))\n    Return x\n```',
            "11. ### Autocomplete",
            '12. When you type into any search browser, for example Google, there are always suggestions of what you are about to type. This is extremely helpful and convenient. Say we were searching "How are you doing", if we just type in "how are" into google, we will see that it suggests this exact query.',
            "13. One way to achieve this is using a Trie! We will build a map from strings to values.",
            "14. * Values will represent how important Google thinks that string is (Probably frequency)\n* Store billions of strings efficiently since they share nodes, less wasteful duplicates\n* When a user types a query, we can call the method `keysWithPrefix(x)` and return the 10 strings with the highest value",
            "15. One major flaw with this system is if the user types in short length strings. You can imagine that the number of keys with the prefix of the input is in the millions when in reality we only want 10. A possible solution to this issue is to store the best value of a substring in each node. We can then consider children in the order of the best value.",
            '16. Another optimization is to merge nodes that are redundant. This would give us a "radix trie", which holds characters as well as strings in each node. We won\'t discuss this in depth.',
            "17. ### Summary",
            "18. Knowing the types of data that you are storing can give you great power in creating efficient data structures. Specifically for implementing Maps and Sets, if we know that all keys will be Strings, we can use a Trie:",
            "19. * Tries theoretically have better performances for searching and insertion than hash tables or balanced search trees\n* There are more implementations for how to store the children of every node of the trie, specifically three. These three are all fine, but hash table is the most natural\n  * _DataIndexedCharMap_ (Con: excessive use of space, Pro: speed efficient)\n  * _Bushy BST_ (Con: slower child search, Pro: space efficient)\n  * _Hash Table_ (Con: higher cost per link, Pro: space efficient)\n* Tries may not actually be faster in practice, but they support special string operations that other implementations don't\n  * `longestPrefixOf` and `keysWithPrefix` are easily implemented since the trie is stored character by character\n  * `keysWithPrefix` allows for algorithms like autocomplete to exist, which can be optimized through use of a priority queue.",
            '20. <table><thead><tr><th>Name</th><th width="142.33333333333331">key type</th><th>get(x)</th><th></th></tr></thead><tbody><tr><td>Balanced BST</td><td>comparable</td><td><span class="math">\\Theta (log N)</span></td><td><span class="math">\\Theta (log N)</span></td></tr><tr><td>RSC Hash Table</td><td>hashable</td><td><span class="math">\\Theta (1)^{\\dag}</span></td><td><span class="math">\\Theta (1)^{*\\dag}</span></td></tr><tr><td>Data Indexed Array</td><td>chars</td><td><span class="math">\\Theta (1)</span></td><td><span class="math">\\Theta (1)</span></td></tr><tr><td>Tries (BST, HT, DICM)</td><td>strings</td><td><span class="math">\\Theta (1)</span></td><td><span class="math">\\Theta (1)</span></td></tr></tbody></table>',
            "21. \\*: on average, $$\\dag$$: items are evenly spread\n",
        ],
        "26.-prefix-operations-and-tries/26.2-trie-implementation.md": [
            "1. ---\ndescription: Giving it the old college Trie.\n---",
            "2. # 26.2 Trie Implementation",
            '3. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=DqfZ4BEVDgk" %}',
            "4. Now, we will walk through an implementation of a Trie. We'll take a first approach with the idea that each node stores a letter, its children, and a color. Since we know each node key is a character, we can use our `DataIndexedCharMap` class we defined earlier to map to all of a nodes' children. Remember that each node can have at most the number of possible characters as its number of children.",
            "5. ```java\npublic class TrieSet {\n   private static final int R = 128; // ASCII\n   private Node root;    // root of trie\n\n   private static class Node {\n      private char ch;  \n      private boolean isKey;   \n      private DataIndexedCharMap next;\n\n      private Node(char c, boolean blue, int R) {\n         ch = c; \n         isKey = blue;\n         next = new DataIndexedCharMap<Node>(R);\n      }\n   }\n}\n```",
            "6. Note that for any given node, the DataIndexedCharMap object for that node will have mostly null values if nodes in our tree have relatively few children. For a node with only one child, we will have 128 links with 127 equal to null and 1 being used. This means that we are wasting a lot of excess space! We will explore alternative representations further on.",
            "7. From this, we can make another important observation: each link corresponds to a character if and only if that character **exists**. Therefore, we can remove the Node's character variable and instead base the value of the character from its position in the parent `DataIndexedCharMap`.",
            "8. ```java\npublic class TrieSet {\n   private static final int R = 128; // ASCII\n   private Node root;    // root of trie\n\n   private static class Node {\n      // no more 'ch' instance variable\n      private boolean isKey;   \n      private DataIndexedCharMap next;\n\n      private Node(boolean blue, int R) {\n         isKey = blue;\n         next = new DataIndexedCharMap<Node>(R);\n      }\n   }\n}\n```",
            "9. ### Performance",
            "10. Given a Trie with N keys the runtime for our Map/Set operations are as follows:",
            "11. * `add`: $$\\Theta (1)$$&#x20;\n* `contains`: $$\\Theta (1)$$",
            "12. Why is this the case? It doesn't matter how many items we have in our Trie, the runtime will always be _independent_ of the number of keys. We only traverse the length of one key in the worst case ever, which is unrelated to the number of keys in the Trie. Therefore, let's analyze the runtime with a more appropriate measurement: L, the length of the key we are searching for:",
            "13. * `add`: $$\\Theta (L)$$\n* `contains`: $$\\Theta (L)$$",
            "14. We have achieved constant runtime without having to worry about amortized resizing times or having an even spreading of keys! Our only issue is that as we mentioned above, our current design is extremely wasteful memory-wise since each node contains an array for every single character even if that character doesn't exist.",
            "15. ### Improvement: Child Tracking",
            '16. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=NTdJQB8yr2I" %}',
            "17. To address the issue of wasted space, let us explore two possible solutions:",
            "18. * _Alternate Idea #1_: Hash-Table based Trie. This won't create an array of 128 spots but instead initialize the default value and resize the array only when necessary with the load factor.\n* _Alternate Idea #2_: BST based Trie. Again this will only create children pointers when necessary, and we will store the children in the BST. We will have to worry about the runtime for searching in this BST, but this is not a bad approach.",
            "19. When we implement a Trie, we have to pick a map to our children. A Map is an ADT, so we must also choose the underlying implementation for the map. What does this reiterate to us? There is an **abstraction** barrier between the implementations and the ADT that we are trying to create. This abstraction barrier allows us to take advantage of what each implementation has to offer when we try to meet the ADT behavior. Let's consider each advantage:",
            "20. * DataIndexedCharMap\n  * Space: 128 links per node\n  * Runtime: $$\\Theta (1)$$\n* BST\n  * Space: $$C$$ links per node, where $$C$$ is the number of children\n  * Runtime: O(log$$R$$), where $$R$$ is the size of the alphabet\n* Hash Table\n  * Space: $$C$$ links per node, where $$C$$ is the number of children\n  * Runtime: O($$R$$), where $$R$$ is the size of the alphabet",
            "21. Note: Cost per link is higher in BST and Hash Tables; R is a fixed number (this means we can think of the runtimes as constant)",
            "22. We can takeaway a couple of things. There is a slight memory and efficiency trade off (with BST/Hash Tables vs. DataIndexedCharMap). The runtimes for Trie operations are still constant without any caveats. Tries will especially thrive with some special operations.\n",
        ],
        "26.-prefix-operations-and-tries/README.md": [
            "1. ---\ndescription: 'By: Thomas Lee'\n---",
            "2. # 26. Prefix Operations and Tries",
            '3. {% embed url="https://www.youtube.com/watch?ab_channel=JoshHug&v=i-OuY5o_G8g" %}',
            "4. ### The Search Problem",
            "5. To motivate this next section, let us consider the **search problem**. In this problem, we are given a stream of data, and our goal is to retrieve the information of interest. For example, a website which allows users to post content to their personal page could want to serve that content only to friends. Another example is if a new station receives logs from thousands of weather stations, and it wants to display a weather map for a specified date and time.&#x20;",
            "6. Both of these are examples of the search problem, just in different flavors! The data structures we have built so far have been to solve the search problems for various domains of interest.",
            "7. Let us review the data structures we have seen so far:",
            '8. <table><thead><tr><th width="101">Name</th><th width="222">Storage Operations</th><th width="241">Primary Retrieval Operation</th><th>Retrieve By</th></tr></thead><tbody><tr><td>List</td><td><code>add(key)</code>, <code>insert(key, index)</code></td><td><code>get(index)</code></td><td>index</td></tr><tr><td>Map</td><td><code>put(key, value)</code></td><td><code>get(key)</code></td><td>key identity</td></tr><tr><td>Set</td><td><code>add(key)</code></td><td><code>containsKey(key)</code></td><td>key identity</td></tr><tr><td>Priority Queue</td><td><code>add(key)</code></td><td><code>getSmallest()</code></td><td>key order (smallest to largest)</td></tr><tr><td>Disjoint Sets</td><td><code>connect(int_a, int_b</code></td><td><code>isConnected(int_a, int_b)</code></td><td>two integer values</td></tr></tbody></table>',
            "9. All of these data structures are used to solve different instances of the search problem. They all have their own applications depending on how the data of interest needs to be retrieved. \\\nOne important thing to note is that these are **abstract** data types (ADTs), which means that we define the behavior of the data structure, not the implementation. There are multiple possible implementations for each of the data structures, and we can even use one data structure in the implementation of another! We often use these ADTs to create even more complex data structures.&#x20;",
            "10. ### Abstraction",
            "11. Abstraction is the idea of only being concerned with the behavior of something and not the underlying implementation. This concept is not as foreign as you might think! We apply principles of abstraction in our day to day lives without even realizing it. For example, using a keyboard can be considered an abstraction of writing text onto a computer. There can be multiple implementations of a keyboard's circuitry depending on what company produced it, but we do not worry about what is going on under the hood, we just care that it can allow us to type text onto a computer.&#x20;",
            "12. Abstraction is often applied in _layers_ when creating data structures.&#x20;",
            "13. When implementing a Priority Queue, we could choose to use a Heap Ordered Tree to store the elements of the priority queue. We do not worry about the implementation of the Heap Ordered Tree--we just care about the methods that a Heap Ordered Tree provides.&#x20;",
            "14. In the same vein, the Heap Ordered Tree does not care about the implementation of the Tree data structure that it uses in it's underlying implementation.&#x20;",
            "15. Finally, whoever ends up using the Priority Queue we create is also unconcerned with how we made the Priority Queue. They would only care that our Priority Queue is able to support adding elements and returning the smallest element efficiently.&#x20;",
            "16. In summary, we can often think of an ADT by the use of another ADT. ADTs have layers of abstraction, each defining behavior that is more specific than the idea that came before&#x20;\n",
        ],
        "26.-prefix-operations-and-tries/26.5-exercises.md": [
            "1. # 26.5 Exercises",
            "2. ## Factual",
            '3. 1. Suppose we use the following trie to represent a map. What would `get("sea")` return? What about `get("sell")`?',
            '4. <figure><img src="../.gitbook/assets/image (147).png" alt=""><figcaption></figcaption></figure>',
            '5. 2. Consider the Trie-based set below. What does `keysWithPrefix("sp")` return? What nodes does it visit during this call?',
            "6. ![](<../.gitbook/assets/image (18).png>)",
            "7. 3. What is the worst-case runtime when searching for a single word in a trie? Let $$R$$ be the size of the alphabet, and $$N$$ be the number of items in the trie, and $$L$$ be the length of the word being operated on.",
            "8. <details>",
            "9. <summary>Problem 1</summary>",
            "10. `sea` terminates at the node with value 6. `sell` does not exist in the trie (since it does not terminate at the node with `l`, so the `get` operation returns null.",
            "11. </details>",
            "12. <details>",
            "13. <summary>Problem 2</summary>",
            "14. `keysWithPrefix` follows the prefix to the final letter of the prefix, then performs DFS from that node to get all children.&#x20;",
            "15. During this procedure, it traverses the nodes `s, p, i, t, e, y`. The final return value is `spit, spite, spy`.",
            "16. </details>",
            "17. <details>",
            "18. <summary>Problem 3</summary>",
            "19. $$\\Theta(L)$$. In the worst case, the word is a prefix of some other word in the trie, but is not present in the trie itself. In this case, we go through all the letters of the word.",
            "20. </details>",
            "21. ## Metacognitive",
            "22. 1. Compare the worst-case number of character comparisons required to insert a word into an LLRB, hash table, and R-way trie. Let $$R$$ be the size of the alphabet, and $$N$$ be the number of items in the trie, and $$L$$ be the maximum length of any word.",
            "23. <details>",
            "24. <summary>Problem 1</summary>",
            "25. **LLRB**: We always insert at the bottom of the LLRB, so there are $$\\Theta(\\log N)$$comparisons to figure out where the new node goes. Each word comparison takes up to $$L$$ character comparisons. Thus, there are $$\\Theta(L \\log N)$$ comparisons.",
            "26. **Hash table**: In the worst case, all items hash to the same bucket. On an insertion, we must compare a word to all other words in the bucket for equality. Assuming this bucket has $$N$$ items, this takes $$\\Theta(LN)$$ comparisons.",
            "27. **R-way trie**: In the worst case, we follow or create $$L$$ nodes to the end of the word. Thus, there are at most $$\\Theta(L)$$ comparisons.",
            "28. </details>\n",
        ],
        "26.-prefix-operations-and-tries/26.1-introduction-to-tries.md": [
            "1. ---\ndescription: Do or do not. There is no Trie.\n---",
            "2. # 26.1 Introduction to Tries",
            "3. Let us first consider a potential improvement for our current HashMap implementation.&#x20;",
            "4. HashMaps are already incredibly fast. For our Resizing Separate Chaining Hash Table, `contains(x)` method has $$\\Theta (1)$$ runtime (assuming even spread), and our `add(x)` method also has $$\\Theta(1)$$ runtime (assuming even spread and amortized). This is generally the best that we can do, but if we have some additional insight on the data we are storing, we could get even faster.&#x20;",
            "5. #### Special Case 1: Character Keyed Map",
            "6. If we know that our keys are _only_ ASCII characters, we can do away with our general-purpose HashMap and instead use an array where each index in the array corresponds to a specific ASCII character:",
            "7. ```java\npublic class DataIndexedCharMap<V> {\n    private V[] items;\n    public DataIndexedCharMap(int R) {\n        items = (V[]) new Object[R];\n    }\n    public void put(char c, V val) {\n        items[c] = val;\n    }\n    public V get(char c) {\n        return items[c];\n    }\n}\n```",
            "8. The above is a possible implementation for a map that takes in character keys. The value R represents the number of possible characters (e.g. 128 for ASCII). We no longer have to store any buckets which could hurt our runtime (at the cost of additional memory). We know that our data will be evenly spread.&#x20;",
            "9. #### Special Case 2: String Keyed Map",
            "10. Suppose we know that our keys are always strings. We can use a special data structure called a Trie. This data structure stores each letter of a string as a node in a tree. It has great performance for getting words, adding words, and some special string operations.&#x20;",
            "11. ### Trie Demo",
            '12. Suppose we wanted to store the words "sam", "sad", "sap", "same", "a", and "awls". We want to create a data structure that will allow us to add these words in and make it clear that our set contains those words and not any suffixes or prefixes of those words.&#x20;',
            "13. There are a few key ideas for Tries:",
            "14. * Every node stores only one letter.\n* Nodes can be shared by multiple keys.",
            '15. Consider a Trie with the words "sam" and "sad" already in it:',
            '16. <figure><img src="../.gitbook/assets/trie 1 resized.png" alt=""><figcaption></figcaption></figure>',
            '17. When we add the word "sap", we can make use of the fact that we already have the prefix "sa" in the Trie:',
            '18. <figure><img src="../.gitbook/assets/trie 2 resized.png" alt=""><figcaption></figcaption></figure>',
            '19. Adding "same" follows a similar procedure. We have the prefix "sam" in the Trie, so we can use it to our advantage:',
            '20. <figure><img src="../.gitbook/assets/trie 3 resized.png" alt=""><figcaption></figcaption></figure>',
            '21. When adding "a", our first instinct may be to add an edge between the root and the existing "a" in our Trie:',
            '22. <figure><img src="../.gitbook/assets/trie 4 resized.png" alt=""><figcaption></figcaption></figure>',
            '23. However, this way would be a bit misleading because we do not know if the "a" is the start of the word "ame". Instead, we create an entirely new node:',
            '24. <figure><img src="../.gitbook/assets/trie 5 resized.png" alt=""><figcaption></figcaption></figure>',
            '25. Now when adding "awls", we can just use our similar procedure of using the prefix "a":',
            '26. <figure><img src="../.gitbook/assets/trie 6 resized.png" alt=""><figcaption></figcaption></figure>',
            '27. This is looking great already! We can see the words that we added very clearly in the Trie. However, there is an issue. We are supposed to have _just_ the words "sam", "sad", "sap", "same", "a", and "awls" in the Trie. With our current structure, we cannot say for sure which prefixes should be considered to be in the Trie and which should not be. For example, we want the prefix "sam" to be in the Trie, but we do not want "awl" or "aw" to be considered to be in the Trie.&#x20;',
            "28. To address this issue, we will _color_ the last character of each string to be blue to indicate that there is a word that ends with that character:",
            '29. <figure><img src="../.gitbook/assets/trie 7 resized.png" alt=""><figcaption></figcaption></figure>',
            "30. Now we are finished! To search, we will traverse our Trie from the root and compare with each character of the string we are searching for as we go down. Thus, there are only two cases when we wouldn't be able to find a string; either the final node is white or we fall off the tree.",
            "31. Examples:",
            '32. * `contains("sam")`: true, blue node\n* `contains("sa")`: false, white node\n* `contains("a")`: true, blue node\n* `contains("saq")`: false, fell off tree',
            "33. ### Tries as Maps",
            "34. Tries can also be maps if we also store data in the nodes. For example, we can have each blue node also hold a number, which could represent the multiplicity of that word.&#x20;",
            "35. ### Summary",
            "36. ",
            "37. See an animated demo of creation of a Trie map [here](http://www.cs.princeton.edu/courses/archive/spring15/cos226/demo/52DemoTrie.mov).\n",
        ],
        "29.-basic-sorts/29.5-summary.md": [
            "1. # 29.5 Summary",
            '2. <figure><img src="../.gitbook/assets/image (69).png" alt=""><figcaption><p>A summary of the sorts covered so far.</p></figcaption></figure>',
            "3. ### Overview&#x20;",
            "4. **Inversions.** The number of pairs of elements in a sequence that are out of order. An array with no inversions is ordered.",
            "5. **Selection sort.** One way to sort is by selection: Repeatedly identifying the most extreme element and moving it to the end of the unsorted section of the array. The naive implementation of such an algorithm is in place.",
            "6. **Naive Heapsort.** A variant of selection sort is to use a heap based PQ to sort the items. To do this, insert all items into a MaxPQ and then remove them one by one. The first such item removed is placed at the end of the array, the next item right before the end, and so forth until that last item deleted is placed in position 0 of the array. Each insertion and deletion takes O(log N) time, and there are N insertions and deletions, resulting in a O(N log N) runtime. With some more work, we can show that heapsort is \u03b8(N log N) in the worst case. This naive version of heapsort uses \u03b8(N) for the PQ. Creation of the MaxPQ requires O(N) memory. It is also possible to use a MinPQ instead.",
            "7. **In place heapsort.** When sorting an array, we can avoid the \u03b8(N) memory cost by treating the array itself as a heap. To do this, we first heapify the array using bottom-up heap construction (taking \u03b8(N) time). We then repeatedly delete the max item, swapping it with the last item in the heap. Over time, the heap shrinks from N items to 0 items, and the sorted list from 0 items to N items. The resulting version is also \u03b8(N log N).",
            "8. **Mergesort.** We can sort by merging, as discussed in an earlier lecture. Mergesort is \u03b8(N log N) and uses \u03b8(N) memory.",
            "9. **Insertion Sort.** For each item, insert into the output sequence in the appropriate place. Naive solution involves creation of a separate data structure. The memory efficient version of this algorithm swaps items one-by-one towards the left until they land in the right place. The invariant for this type of insertion sort is that every item to the left of position i is in sorted order, and everything to the right has not yet been examined. Every swap fixes exactly one inversion.",
            "10. **Insertion Sort Runtime.** In the best case, insertion sort takes \u03b8(N) time. In the worst case, \u03b8(N^2) time. More generally, runtime is no worse than the number of inversions.\n",
        ],
        "29.-basic-sorts/29.6-exercises.md": [
            "1. # 29.6 Exercises",
            "2. ## Factual",
            "3. 1. How many inversions are there in the array `[10, 100, 60, 40, 50]`?\n2. What is the space complexity of selection sort?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. 5; the violations are 100 > 60, 100 > 40, 100 > 50, 60 > 40, and 60 > 50.",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. $$\\Theta(1)$$. All swaps in selection sort happen in-place.",
            "11. </details>",
            "12. ## Procedural",
            "13. 1. Draw the process of heapsort on the array `[5, 9, 2]`, starting with the heapified array and removing the maximum each time.\n2. If we are insertion sorting `[5, 6, 7, 1, 8, 9, 2]`, how many total swaps will occur?",
            "14. <details>",
            "15. <summary>Problem 1</summary>",
            "16. After heapification: `[9, 5, 2]`. We sink in reverse level order, which means that we swap `5` with `9`.",
            "17. Then, the algorithm proceeds by popping off `9` (`[5, 2 | 9]`), then `5` (`[2 | 5, 9]`), then `2` (`[2, 5, 9]`).",
            "18. </details>",
            "19. <details>",
            "20. <summary>Problem 2</summary>",
            "21. 8; simply count the number of inversions (5 > 1, 5 > 2, 6 > 1, 6 > 2, 7 > 1, 7 > 2, 8 > 2, 9 > 2).",
            "22. </details>",
            "23. ## Metacognitive",
            "24. 1. Which sort do you expect to run more quickly on a reversed array, selection sort or insertion sort?",
            "25. <details>",
            "26. <summary>Problem 1</summary>",
            "27. Asymptotically, both selection and insertion sort run in $$\\Theta(N^2)$$ on a reverse-sorted array. However, note that selection sort only does $$N$$ total swaps (finding the maximum, then swapping to the front), while insertion sort does on the order of $$N^2$$ swaps (swapping each item to the front), so insertion sort will actually be slower by a constant factor.",
            "28. </details>\n",
        ],
        "29.-basic-sorts/29.1-the-sorting-problem.md": [
            "1. # 29.1 The Sorting Problem",
            "2. ## Sorting",
            "3. For the remaining part of this textbook, we'll discuss the sorting problem, which can be informally defined as putting a given set of items in a particular order.",
            "4. This is a useful task in its own right, but can also be a subproblem in larger algorithmic problems. Sorting can be applied to problems like duplicate finding (after sorting, equivalent items are adjacent), binary search, and balancing data structures.",
            "5. The other reason we introduce sorting is that the different sorts provide general ideas about how to approach computational problems. The solution(s) to sorting problems will often involve data structures covered in the earlier parts of this course.",
            "6. ### Sorting: Definitions",
            "7. An **ordering relation** $$<$$ for keys a, b, and c has the following properties:",
            "8. * _Law of Trichotomy_: Exactly one of a $$<$$ b, a = b, b $$<$$ a is true.\n* _Law of Transitivity_: If a $$<$$ b, and b $$<$$ c, then a $$<$$ c.",
            "9. An ordering relation with the properties above is also known as a **total order**.&#x20;",
            "10. A **sort** is a permutation of a sequence of elements that puts the keys into non-decreasing order relative to a given ordering relation, such that x1 \u2264 x2 \u2264 x3\u2264 ...\u2264 xN.",
            "11. #### Example: String Length",
            "12. One example of a ordering relation is the length of strings. To see how the two laws apply:",
            "13. * _trichotomy_: only one of the following can be true for two strings `a` and `b`--`len(a)` < `len(b)`, `len(a)` = `len(b)`, or `len(a)` > `len(b)`.\n* _transitivity:_ if `len(a)` < `len(b)` and `len(b)` < `len(c)`, then clearly `len(a)` < `len(c)`.",
            '14. Suppose we use the ordering relation above to sort `["cows", "get", "going", "the"]`. Then two valid sorts would be `["the", "get", "cows", "going"]` and `["get", "the", "cows", "going"]`. Note that in this sort, `the` and `get` are equivalent since their lengths are equal.',
            "15. ### Ordering Relations in Java",
            "16. In Java, ordering relations are typically given by the `compareTo` or `compare` methods. For example:",
            "17. ```java\nimport java.util.Comparator;\n \npublic class LengthComparator implements Comparator<String> {\n    public int compare(String x, String b) {\n        return x.length() - b.length();\n    }\n}\n```",
            "18. Note by the relation above, `the` and `get` are equal in ordering, but _not_ equal by the `.equals()` method.",
            "19. ### Inversions",
            "20. An alternate way to view sorting is as fixing inversions within a sequence of elements. An **inversion** is a pair of elements that are out of order with respect to the defined ordering relation.",
            "21. For example, in the following sequence of 11 elements, there are 55 possible inversions at most (11 choose 2), and the sequence itself has 6 inversions.",
            '22. <figure><img src="../.gitbook/assets/image (33).png" alt=""><figcaption><p>The sequence above has 6 inversions</p></figcaption></figure>',
            "23. Sorting, then, can be viewed as follows: given a sequence of elements with Z inversions, perform some sequence of operations to reduce the total number of inversions to zero.",
            "24. ### Sorting: Performance",
            "25. Previously, we have seen characterizations of of the runtime efficiency of an algorithm, also called the **time complexity** of an algorithm. For example, we can say that Dijkstra\u2019s has time complexity O(E log V).",
            "26. Characterizations of the \u201cextra\u201d memory usage of an algorithm is sometimes called the **space complexity** of an algorithm. For example, Dijkstra\u2019s has space complexity \u0398(V) to store the queue, `distTo`, and `edgeTo` arrays. Note that the graph takes up space \u0398(V+E), but we don\u2019t count this as part of the space complexity of Dijkstra since the graph is an input to Dijkstra\u2019s. In other words, we are only concerned with the _extra_ space used by the algorithm.\n",
        ],
        "29.-basic-sorts/29.4-insertion-sort.md": [
            "1. # 29.4 Insertion Sort",
            "2. ## Naive Insertion Sort",
            "3. In insertion sort, we start with an empty output sequence. Then, we select an item from the input, inserting into the output array at the correct location.",
            "4. Naively, we can do this by creating a completely seperate output array and simply putting items from the input there. You can see a demo of this algorithm [here](http://goo.gl/bVyVCS).",
            '5. <figure><img src="../.gitbook/assets/image (19).png" alt=""><figcaption><p>Naive insertion sort with a seperate output array</p></figcaption></figure>',
            "6. For the naive approach, if the output sequence contains $$k$$ items so far, then an insertion takes $$O(k)$$ time (shifting every item over in the output array).",
            "7. ## In-place Insertion Sort",
            "8. We can improve the time and space complexity of insertion sort by using in-place swaps instead of building a seperate output array.&#x20;",
            "9. You can see a demo of this algorithm [here](https://docs.google.com/presentation/d/10b9aRqpGJu8pUk8OpfqUIEEm8ou-zmmC7b\\_BE5wgNg0/edit#slide=id.g463de7561\\_042).",
            "10. Essentially, we move from left to right in the array, selecting an item to be swapped each time. Then, we swap that item as far to the front as it can go. The front of our array gradually becomes sorted until the entire array is sorted.",
            '11. <figure><img src="../.gitbook/assets/image (130).png" alt=""><figcaption><p>Insertion sort algorithm. Purple items are the selected item being swapped to the front, and black items are the ones with which the purple items are being swapped.</p></figcaption></figure>',
            "12. ## Insertion Sort Runtime",
            "13. The best-case runtime of insertion sort is $$\\Theta(N)$$--when there are no swaps, we simply do a linear scan through the array. The worst-case runtime of insertion sort is $$\\Theta(N^2)$$--in a reverse-sorted array, we have to swap every item all the way to the front.&#x20;",
            "14. ### Insertion Sort Advantages",
            "15. Note that on sorted or almost-sorted arrays, insertion sort does very little work. In fact, the number of swaps that it does is equal to the number of inversions in the array.",
            '16. <figure><img src="../.gitbook/assets/image (75).png" alt=""><figcaption><p>The left array has only 5 inversions, and the right array has only 3. Insertion sort does very little work.</p></figcaption></figure>',
            "17. In other words, on arrays with a small number of inversions, insertion sort is probably the fastest sorting algorithm. The runtime is $$\\Theta(N + K)$$, where $$K$$ is the number of inversions in the array. If we define an almost-sorted array as one where the number of inversions $$K < cN$$ for some constant $$c$$, then insertion sort runs in linear time.",
            '18. A less obvious empirical fact is that insertion sort is extremely fast on small arrays, usually of size 15 or less. The analysis of this is beyond the scope of the course, but the general idea is that divide-and-conquer algorithms (like heapsort and mergesort) spend too much time on the "dividing" phase, whereas insertion sort starts sorting immediately. In fact, the Java [implementation](http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/java/util/Arrays.java#Arrays.mergeSort%28java.lang.Object%5B%5D%2Cjava.lang.Object%5B%5D%2Cint%2Cint%2Cint%29) of mergesort uses insertion sort when the split becomes less than 15 items.&#x20;\n',
        ],
        "29.-basic-sorts/README.md": ["1. # 29. Basic Sorts", "2. "],
        "29.-basic-sorts/29.3-mergesort.md": [
            "1. # 29.3 Mergesort",
            "2. We've covered mergesort in the past, but as a reminder, the algorithm is as follows:",
            "3. 1. Split the items into half.\n2. Mergesort each half.\n3. Merge the two sorted halves to form the final result.",
            "4. You can see a demo of the algorithm [here](https://docs.google.com/presentation/d/1h-gS13kKWSKd\\_5gt2FPXLYigFY4jf5rBkNFl3qZzRRw/edit#slide=id.g463de7561\\_042).&#x20;",
            "5. Mergesort has a runtime of $$\\Theta(N \\log N)$$-- we will not reanalyze the algorithm here but you may refer to this [link](https://docs.google.com/presentation/u/1/d/1\\_LhI5V5JlcRHYU55\\_SF7ZHxPemBr9OVlNzj7ScYdg64/edit#slide=id.g84271d11b\\_2\\_77) if you'd like to remind yourself.",
            "6. The auxiliary array used during the merge step requires $$\\Theta(N)$$ extra space. Note that in-place mergesort is possible; however it is very complex and the runtime suffers by a significant constant factor, so we will not cover it here.\n",
        ],
        "29.-basic-sorts/29.2-selection-sort-and-heapsort.md": [
            "1. # 29.2 Selection Sort & Heapsort",
            "2. ## Selection Sort",
            "3. Selection sort uses the following algorithm:",
            "4. 1. Find the smallest item.\n2. Swap that item to the front.\n3. Repeat until all items are fixed (there are no inversions).",
            "5. You can see a demo of the sorting algorithm [here](https://docs.google.com/presentation/u/1/d/1p6g3r9BpwTARjUylA0V0yspP2temzHNJEJjCG41I4r0/edit?usp=sharing).",
            "6. Selection sort runs in $$\\Theta(N^2)$$ time using an array or similar data structure.&#x20;",
            "7. You may have noticed that selection sort seems inefficient, and you'd be right--we look through the entire remaining array each time to find the minimum, examining the same items over and over.",
            "8. ## Heapsort",
            "9. ### Naive Heapsort",
            "10. To avoid the inefficiency mentioned above regarding selection sort, we can leverage a max-oriented heap instead of scanning over the array linearly.&#x20;",
            '11. _Note: Because of the array-based representation of a heap, using a max heap results in a simpler implementation where we can maintain a "sorted" and "unsorted" portion of the array. This will be explained further in the next section._',
            "12. Then, to heapsort N items, we can insert all the items into a max heap and create and output array. Then, we repeatedly delete the largest item from the max heap and put the largest item at the end part of the output array.",
            "13. #### Naive Heapsort Analysis",
            "14. The overall runtime of this algorithm is $$\\Theta(N \\log N)$$. There are three main components to this runtime:",
            "15. * Inserting N items into the heap: $$O(N \\log N)$$.\n* Selecting the largest item: $$\\Theta(1)$$.\n* Removing the largest item: $$O(\\log N)$$.",
            "16. This is a large improvement over selection sort's $$\\Theta(N^2)$$ runtime.",
            "17. In terms of memory usage, the output array takes an extra $$\\Theta(N)$$space. This is worse than selection sort, which uses no extra space, but the improvement in runtime far outweighs this downside.",
            "18. Even more, we can use a trick with heapsort to get rid of the extra output array.",
            "19. ### In-place Heapsort",
            "20. As an alternate approach, we can use the input array itself to form the heap and output array.",
            "21. Rather than inserting into a new array that represents our heap, we can use a process known as _bottom-up heapification_ to convert the input array into a heap. Bottom-up heapification involves moving in reverse level order up the heap, sinking nodes to their appropriate location as you move up.",
            '22. By using this approach, we avoid the need for an extra copy of the data. Once heapified, we use the naive heapsort approach of popping off the maximum and placing it at the end of our array. In doing so, we maintain an "unsorted" front portion of the array (representing the heap) and a "sorted" back portion of the the array (representing the sorted items so far).',
            '23. <figure><img src="../.gitbook/assets/image (120).png" alt=""><figcaption><p>Bottom-up heapfication</p></figcaption></figure>',
            "24. You can see a demo of this algorithm [here](https://docs.google.com/presentation/d/1SzcQC48OB9agStD0dFRgccU-tyjD6m3esrSC-GLxmNc/edit?usp=sharing).",
            "25. #### In-place Heapsort Runtime",
            "26. This process overall is still $$O(N \\log N)$$, since bottom-up heapification requires at most $$N$$ sink-down operations that take at most $$\\log N$$ time each.",
            "27. _Note: it is possible to prove that bottom-up heapficiation is bounded by_ $$\\Theta(N)$$. _However, this proof is out of scope for this class._",
            "28. #### In-place Heapsort Memory",
            "29. Using in-place heapsort, we reduce the memory usage of heapsort to $$\\Theta(1)$$. Since we are reusing the input array, no additional space is used (and remember that we do not count the input when assessing memory complexity).\n",
        ],
        "30.-quicksort/30.3-quicksort-performance-caveats.md": [
            "1. # 30.3 Quicksort Performance Caveats",
            '2. "Quicksort" was chosen by Tony Hoare as the name for this particular partition sort algorithm. Coincidentally, quicksort is empirically the fastest sort for most common situations.&#x20;',
            "3. How fast is Quicksort exactly? To answer this question, we need to count the number and calculate the difficulty of the partition operations employed in Quicksort.&#x20;",
            "4. In the theoretical runtime analysis, partitioning costs $$\\Theta(K)$$ time, where $$\\Theta(K)$$ is the number of elements being partitioned.",
            "5. However, what complicates runtime analysis for Quicksort is that the the overall runtime will depend on where the pivot ends up.&#x20;",
            "6. #### Best Case",
            "7. For instance, the best case runtime is when the pivot chosen always lands in the middle (in other words, the partition always picks the median element of the array).&#x20;",
            '8. <figure><img src="https://lh3.googleusercontent.com/aPwCJk-kd_yMuLoWOEFGoOv-KL3CLHjIkhjF3jkIzzDDP-Km5ZMKweibs7zjVU3IrZ1TjYA_vaIr8AnBGL0NaX_0W8sKWKjd75knjSruW2wnradN9lrXBYNbbmJyCRo6992xcUhkcQhd4QJekQ9m27k" alt=""><figcaption><p>Best Case</p></figcaption></figure>',
            "9. In the best case, the total work done at each level is approximately O(N). To see this, the first partition pivots on one element and requires checking all N elements to determine whether they go on the right or the left of the pivot element. The second layer repeats this, but for N/2 elements on two separate pivots (since we are recursively partitioning on the two halves). Thus, each level is O(N) work.&#x20;",
            "10. The overall runtime becomes $$\\Theta(NH)$$, where H = # of layers = $$\\Theta(log N)$$. Thus, the total runtime for Quicksort in the best case is $$\\Theta(N log N)$$.",
            "11. #### Worst Case",
            "12. The worst case for runtime occurs when the pivot chosen at each partition lands at the beginning of the array. In this scenario, each layer still has to do O(N) work, but now there are H = # of layers = N layers, since every single element has to be pivoted on. Thus, the worst case runtime is $$\\Theta(N^2)$$.",
            '13. <figure><img src="https://lh6.googleusercontent.com/EDrTj7V1EJuK4gzbemhYfMvvjgEfJcqvdlw3EeM7d5Et1yacIXCPMQud1G45Igotd2sG-GRxVb9DAK7XMeMcoZD0D1fvxhKaZBFTXe7UY_V9GT2VY367PoD8QjZic7abc7iVFtdL1ZPev45Tm2SO1Yo" alt=""><figcaption><p>Worst Case: <span class="math">\\Theta(N^2)</span></p></figcaption></figure>',
            "14. #### Quicksort vs Mergesort Performance",
            "15. | Theoretical Analysis | Quicksort           | Mergesort           |\n| -------------------- | ------------------- | ------------------- |\n| Best Case            | $$\\Theta(N log N)$$ | $$\\Theta(N log N)$$ |\n| Worst Case           | $$\\Theta(N^2)$$     | $$\\Theta(N log N)$$ |",
            "16. In comparison, Mergesort seems to be a lot better, since Quicksort suffers from a theoretical worst case runtime of $$\\Theta(N^2)$$. So how can Quicksort be the fastest sort empirically? Quicksort's advantage empirically comes from the fact that on average, Quicksort is $$\\Theta(NlogN)$$.",
            "17. #### Quicksort's Performance Argument #1: 10% Case",
            "18. One argument for why Quicksort is the fastest sort empirically supposes that the pivot always ends up at least 10% from either edge.&#x20;",
            '19. <figure><img src="../.gitbook/assets/Screen Shot 2023-03-31 at 11.40.20 PM.png" alt=""><figcaption><p>Pivot is ~10% from the left edge.</p></figcaption></figure>',
            "20. If this is the case, then the runtime is still O(NH), where O(N) is the work at each level and H is the # of levels. Since the pivot always lands at least 10% from either edge, H is approximately $$\\log_{10/9} N$$= O(log N). Thus, overall runtime is O(N log N).",
            '21. <figure><img src="../.gitbook/assets/Screen Shot 2023-03-31 at 11.45.27 PM.png" alt=""><figcaption><p>10% case partition work at each level.</p></figcaption></figure>',
            "22. In other words, even if you are not lucky enough to have a pivot that is near the middle, if you can at least have a pivot that is always 10% from the edge, the runtime will be O(N log N).&#x20;",
            "23. #### Quicksort's Performance Argument #2: Quicksort is BST Sort",
            "24. From another lens, Quicksort can be seen as a form of BST sort. This is because the compareTo calls that Quicksort employs between each element and the pivot element in each partition is the same as the compareTo calls performed in BST insert.&#x20;",
            '25. <figure><img src="../.gitbook/assets/Screen Shot 2023-03-31 at 11.48.25 PM.png" alt=""><figcaption><p>BST insert</p></figcaption></figure>',
            '26. <figure><img src="../.gitbook/assets/Screen Shot 2023-03-31 at 11.49.08 PM.png" alt=""><figcaption><p>Quicksort partitions on the "same" BST elements as above.</p></figcaption></figure>',
            "27. Since random insertion into a BST takes O(N log N) time, Quicksort can be argued to have similar performance.&#x20;",
            "28. #### Empirical Quicksort Runtime",
            "29. Empirically, for N items, Quicksort uses an average of \\~2N ln N compares to complete (across 10,000 trials with N = 1000). For more information, [check out this link](http://www.informit.com/articles/article.aspx?p=2017754\\&seqNum=7).",
            "30. #### Avoiding the Worst Case of Quicksort",
            "31. As you can see from above, the performance of Quicksort (both in terms of order of growth and in constant factors) depends critically upon **how you select the pivot, how you partition around the pivot, and other optimizations that you can add to speed things up.**&#x20;",
            "32. Given certain conditions, Quicksort can result in $$\\Theta(N^2)$$, which is much worse than $$\\Theta(N log N)$$. Some of these conditions include **bad ordering,** where the array is already in sorted order (or almost sorted order), and **bad elements,** where the array has all duplicates.&#x20;\n",
        ],
        "30.-quicksort/30.1-partitioning.md": [
            "1. # 30.1 Partitioning",
            "2. Previously, we\u2019ve covered **selection sort, heap sort, merge sort, and insertion sort**.",
            "3. These sorts each have a basic idea powering them:&#x20;",
            "4. * Selection sort: find the smallest item, put it at the front\n* Heap sort: use MaxPQ to find the maximum element, put it at the front\n* Merge sort: merge two sorted halves into one sorted whole&#x20;\n* Insertion sort: determine where to insert the current item",
            "5. The core idea behind Quicksort involves **partitioning.**&#x20;",
            "6. Specifically, to partition an array a\\[], first pick an element x = a\\[i] as the pivot.&#x20;",
            '7. <figure><img src="https://lh6.googleusercontent.com/uFsnl6huFGAYsN7Tsk4-0F3mBDUDZ9Cobaeq3cWeDs4kWvE3d2-4Zyp3QLzNxuUipSOMJvSUmTlQWqpnasgMNkBqv6s4Cu-ndfIv-tkLJ8QhjibVJYwTEBmOQM6utO8z8M8S58wvuxMgiedvLqcllFY" alt=""><figcaption><p>In this array, x = 10 and is the pivot element. This array has not be partitioned on x = 10 yet. </p></figcaption></figure>',
            "8. Partitioning on that pivot element (x) involves rearrange a\\[] such that:&#x20;",
            "9. 1. x moves to some position j (can be the original position of i)\n2. All array elements to the left of x are less than or equal to x (<= x)&#x20;\n3. All array elements to the right of x are greater than or equal to x (>= x)",
            "10. In the following arrays, A, B, and C represent arrays that are valid partitions (with the pivot being x = 10), while D represents an invalid partition.",
            '11. <figure><img src="https://lh5.googleusercontent.com/SFSKi0Tx5zqOkkBagtljRD__IHEeIoGzOa3jipkmRt_lxYZKFh5mqEMOsLjy1LvSzgA4wXW5qHvMfx7im3vhw6fD-er0XTA9cc-fTmASeB1oeYksBPVaGFRkIt0JLjorRdsG8TDWguvHDad9NM4MTRg" alt=""><figcaption><p>D is an invalid partition because 4 is less than 10, but is to the right of the pivot after the partition. </p></figcaption></figure>',
            "12. \\\n",
        ],
        "30.-quicksort/30.2-quicksort-algorithm.md": [
            "1. # 30.2 Quicksort Algorithm",
            "2. The figure below shows a partition performed on the pivot of 5. Notice how 5 in its \u201cproper place\u201d (in other words, it\u2019s exactly where it should be if the entire array was sorted).",
            "3. This suggests that the partition procedure can be used recursively on the two halves to the left and to the right of the pivot element.&#x20;",
            '4. <figure><img src="https://lh6.googleusercontent.com/ZMiWPSUmOc95h-fBaxMB4I42Djyk0uAL_s8J9XwwqW4KHizpn1xf0wuwciRKa033jyMKK78Vul99b1xYa_fLUOVYFnFhLNZDZstdWSLv-M6Z84H_QD2YOmHeJGjsPOaxIlGWojDANKOPLZ3tVORvZQQ" alt=""><figcaption><p>After partitioning on the pivot element of 5, 5 is in the \u201cproper place\u201d and there are two halves left to be sorted. The left half are the four elements of [3, 2, 1, 4], and the right half consists of the three elements of [7, 8, 6]. </p></figcaption></figure>',
            "5. The properties that partitioning provides inspires Tony Hoare\u2019s Quicksort algorithm.&#x20;",
            "6. **Quicksort Algorithm**&#x20;",
            "7. To quicksort N items:&#x20;",
            "8. 1. Partition on the leftmost item as the pivot.&#x20;\n2. Recursively quicksort the left half.&#x20;\n3. Recursively quicksort the right half.&#x20;",
            '9. <figure><img src="https://lh6.googleusercontent.com/srbltrmdQThh9xwjjq-smTXOAVhav1ISWHf4eKX2yZlHr50kHlvZDmH36nNMwLWuMkOlzyApaJWvUdEA1ydHDYKbulYtVuqU6kOn9QbMWAraIOcQae7ymka3zCJKcgzd0u9SeubYP7as_PnnRnEU6Ak" alt=""><figcaption><p><a href="https://docs.google.com/presentation/d/1QjAs-zx1i0_XWlLqsKtexb-iueao9jNLkN-gW9QxAD0/edit?usp=sharing">Demo of quicksort is linked here</a></p></figcaption></figure>',
            "10. \\\n",
        ],
        "30.-quicksort/README.md": ["1. # 30. Quicksort", "2. "],
        "30.-quicksort/30.4-summary.md": [
            "1. # 30.4 Summary",
            "2. **Insertion Sort Sweet Spots.** We concluded our discussion of insertion sort by observing that insertion sort is very fast for arrays that are almost sorted, i.e. that have \u0398(N) inversions. It is also fastest for small N (roughly N\u226415).",
            "3. **Partitioning.** Partioning an array on a pivot means to rearrange the array such that all items to the left of the pivot are \u2264 the pivot, and all items to the right are \u2265 the pivot. Naturally, the pivot can move during this process.",
            "4. **Partitioning Strategies.** There are many particular strategies for partitioning. You are not expected to know any particular startegy.",
            "5. **Quicksort.** Partition on some pivot. Quicksort to the left of the pivot. Quicksort to the right.",
            "6. **Quicksort Runtime.** Understand how to show that in the best case, Quicksort has runtime \u0398(Nlog\u2061N), and in the worse case has runtime $$\u0398(N^2)$$.",
            "7. **Pivot Selection.** Choice of partitioning strategy and pivot have profound impacts on runtime. Two pivot selection strategies that we discussed: Use leftmost item and pick a random pivot. Understand how using leftmost item can lead to bad performance on real data.",
            "8. **Randomization.** Accept (without proof) that Quicksort has on average \u0398(Nlog\u2061N) runtime. Picking a random pivot or shuffling an array before sorting (using an appropriate partitioning strategy) ensures that we\u2019re in the average case.",
            "9. **Quicksort properties.** For most real world situations, quicksort is the fastest sort.\n",
        ],
        "30.-quicksort/30.5-exercises.md": [
            "1. # 30.5 Exercises",
            "2. ## Factual",
            "3. 1. Suppose we have a sorted array in Java. What is the best approach for finding a particular item in the array: binary search or linear iteration?\n2. Repeat the above exercise for a sorted array stored on physical tape.\n3. Given the array `[90, 50, 20, 30, 40, 10, 60, 50, 30]`, what would be the best pivot?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. Binary search. Binary search would take $$\\log N$$ time, where $$N$$ is the length of the array.",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. On a physical tape, the runtime of the algorithm will be dominated by the time to accelerate the physical piece of tape. In the worst case, binary search would involve scanning to the middle of the tape, then a quarter of the way back, then an 1/8th of the way forward, and so forth. In the worst case, we have to cover N/2 + N/4 + N/8 + ... + 1 or O(N) spaces on the tape. However, all that acceleration back and forth will result in an algorithm that is almost certainly slower in the worst case than just scanning forwards.",
            "11. </details>",
            "12. <details>",
            "13. <summary>Problem 3</summary>",
            "14. 40; it is the median which will evenly partition the array.",
            "15. </details>",
            "16. ## Procedural",
            "17. 1. If we have 10 items to sort, how many compares will it take to selection sort all of the items?\n2. What is the depth of the quicksort recursive tree in the best and worst case? Give an exact answer, not a theta bound.",
            "18. <details>",
            "19. <summary>Problem 1</summary>",
            "20. 45 total; first we do 9 compares to find the smallest item, then 8, then 7, etc. 1 + 2 + 3 + ... 9 = 45.",
            "21. </details>",
            "22. <details>",
            "23. <summary>Problem 2</summary>",
            "24. In the worst case, we make $$N$$ recursive calls if we always choose the smallest or largest item as our pivot. Thus, the recursive tree has depth $$N$$.",
            "25. In the best case, we always choose the median as our pivot. This halves the size of the array at each level, resulting in $$\\log_2 N$$ levels.",
            "26. </details>",
            "27. ## Metacognitive",
            "28. 1. Assume that we always pick the leftmost item as our pivot for quicksort. Identify the three types of arrays that will cause a worst-case runtime. For partitioning, assume that we partition into two arrays: items less than or equal to the pivot, and items greater than the pivot.",
            "29. <details>",
            "30. <summary>Problem 1</summary>",
            "31. The two types of arrays are:",
            "32. 1. Arrays already in sorted order. There will always be nothing to the left of the pivot, since the pivot will always be the smallest item.\n2. Arrays in reverse-sorted order. This has the same problem as (1), except that the pivot will always be the largest item.&#x20;\n3. Arrays where all items are equal. Everything will be partitioned into the left (less than or equal to array), and nothing in the greater-than array.",
            "33. </details>\n",
        ],
        "34.-sorting-and-algorithmic-bounds/34.4-summary.md": [
            "1. # 34.4 Summary",
            "2. **Math Problem Out of Nowhere 1.** We showed that $$N! \\in \\Omega(\\frac{N}{2}^{\\frac{N}{2}})$$.",
            "3. **Math Problem Out of Nowhere 2.** We showed that $$\\log(N!) \\in \\Omega(N \\log N)$$, and that $$N \\log N \\in \\Omega(\\log(N!))$$. Therefore $$\\log(N!) \\in \\Theta(N \\log N)$$.",
            "4. **Seeking a Sorting Lower Bound.** We\u2019ve found a number of sorts that complete execution in $$\\Theta(N \\log N)$$ time. This raises the obvious question: Could we do better? Let TUCS (which stands for \u201cThe Ultimate Comparison Sort\u201d) be the best possible algorithm that compares items and puts them in order. We know that TUCS\u2019s worst case runtime is $$O(N \\log N)$$ because we already know several algorithm whose worst case runtime is $$\\Theta(N \\log N)$$, and TUCS\u2019s worst case runtime is $$\\Omega(N)$$ because we have to at least look at every item. Without further discussion, this analysis so far suggest that might be able to do better than $$\\Theta(N \\log N)$$ worst case time.",
            "5. **Establishing a Sorting Lower Bound.** As a fanciful exercise, we played a game called puppy-cat-dog, in which we have to identify which of three boxes contains a puppy, cat, or dog. Since there are 3! = 6 permutations, we need at least $$ceil(\\lg(6)) = 3$$ questions to resolve the answer. In other words, if playing a game of 20 questions with 6 possible answers, we have to ask at least 3 questions to be sure we have the right answer. Since sorting through comparisons is one way to solve puppy-cat-dog, then any lower bound on the number of comparisons for puppy-cat-dog also applies to sorting. Given $$N$$ items, there are $$N!$$ permutations, meaning we need $$lg(N!)$$ questions to win the game of puppy-cat-dog, and by extension, we need at least $$lg(N!)$$ to sort $$N$$ items with yes/no questions. Since $$log(N!)=\\Theta(N \\log N)$$, we can say that the hypothetical best sorting algorithm that uses yes/no questions requires $$\\Omega(N \\log N)$$ yes/no questions. Thus, there is no comparison based algorithm that has a worst case that is a better order of growth than $$\\Theta(N \\log N)$$ compares.\n",
        ],
        "34.-sorting-and-algorithmic-bounds/34.2-math-problems-out-of-nowhere.md":
            [
                "1. # 34.2 Math Problems Out of Nowhere",
                "2. ## A Math Problem",
                "3. <details>",
                '4. <summary>Is <span class="math">N!\\in \\Omega((\\frac{N}{2})^\\frac{N}{2})</span>Prove your answer. [Recall that \u2208 \u03a9 can be informally be interpreted to mean >=. In other words, does factorial grow at least as quickly as <span class="math">(\\frac{N}{2})^\\frac{N}{2}</span>?</summary>',
                "5. 10!",
                "6. * 10 \\* 9 \\* 8 \\* 7 \\* 6 \\* \u2026 \\* 1",
                "7. 55",
                "8. * 5 \\* 5 \\* 5 \\* 5 \\* 5",
                "9. $$N!>(\\frac{N}{2})^\\frac{N}{2}$$ for large N, therefore $$N!\\in \\Omega((\\frac{N}{2})^\\frac{N}{2})$$",
                "10. </details>",
                "11. ## Another Math Problem",
                "12. <details>",
                '13. <summary>Given: <span class="math">N!>(\\frac{N}{2})^\\frac{N}{2}</span>, which we used to prove our answer to the previous problem. Show that <span class="math">log(N!) \\in \\Omega(N * logN)</span>. [Recall: log means an unspecified base]</summary>',
                "14. ",
                "15. We have that $$N!>(\\frac{N}{2})^\\frac{N}{2}$$",
                "16. * Taking the log of both sides, we have that $$log(N!)>log((\\frac{N}{2})^\\frac{N}{2})$$.\n* Bringing down the exponent we have that $$log(N!)>\\frac{N}{2}*log((\\frac{N}{2}))$$.\n* Discarding the unnecessary constant, we have $$log(N!)>\\Omega(N*log(\\frac{N}{2}))$$.\n* From there, we have that $$log(N!)>\\Omega(N*log(N))$$. \\[since $$log(\\frac{N}{2})$$ is the same thing asymptotically as $$log(N)$$]",
                "17. In other words, $$log(N!)$$ grows at least as quickly as $$N*log(N)$$.",
                "18. </details>",
                "19. ## Last Math Problem",
                "20. <details>",
                '21. <summary>In the previous problem, we showed that <span class="math">log(N!) \\in \\Omega(N * logN)</span>. Now show that <span class="math">N*logN \\in \\Omega(log(N!))</span>.</summary>',
                "22. Proof:",
                "23. * $$log(N!)=log(N)+log(N-1)+...+log(1)$$\n* $$N*logN=log(N)+log(N)+...+log(N)$$\n* Therefore $$N*logN \\in \\Omega(log(N!))$$",
                "24. </details>",
                "25. ## Omega and Theta",
                "26. <details>",
                '27. <summary>Given <span class="math">N*logN \\in \\Omega(log(N!))</span> and <span class="math">log(N!) \\in \\Omega(N * logN)</span>. Which of the following can we say?<br><br>A. <span class="math">N*logN \\in \\Theta(log(N!))</span><br>B. <span class="math">log(N!) \\in \\Theta(N * logN)</span><br>C. Both A and B<br>D. Neither</summary>',
                "28. ",
                "29. Answer: C. Both A and B",
                "30. </details>",
                "31. ## Summary",
                "32. We\u2019ve shown that $$N*logN \\in \\Theta(log(N!))$$.",
                "33. * In other words, these two functions grow at the same rate asymptotically.&#x20;",
                "34. As for why we did this, we will see in a little while...\n",
            ],
        "34.-sorting-and-algorithmic-bounds/README.md": [
            "1. ---\ndescription: By William Lee and Angel Aldaco\n---",
            "2. # 34. Sorting  and Algorithmic Bounds",
            "3. ",
        ],
        "34.-sorting-and-algorithmic-bounds/34.1-sorting-summary.md": [
            "1. # 34.1 Sorting Summary",
            "2. ## Other Desirable Sorting Properties: Stability",
            "3. #### A sort is said to be stable if order of equivalent items is preserved.",
            '4. <figure><img src="../.gitbook/assets/image (85).png" alt=""><figcaption></figcaption></figure>',
            "5. Equivalent items don\u2019t \u2018cross over\u2019 when being stably sorted.",
            "6. On the other hand...",
            '7. <figure><img src="../.gitbook/assets/image (138).png" alt=""><figcaption></figcaption></figure>',
            "8. Sorting instability can be really annoying! Wanted students listed alphabetically by section.",
            "9. \\\nArrays.sort()\n-------------",
            "10. In Java, Arrays.sort(someArray) uses:",
            "11. * Mergesort (specifically the TimSort variant) if someArray consists of Objects.\n* Quicksort if someArray consists of primitives.",
            '12. <figure><img src="https://lh3.googleusercontent.com/PDj3CTwryGwH-zcuR27Eu4VE0vxyJLiRVCAWuaCVrhbO51Cihnnsi6Zb3RGnMNPd0MJdvmAjdeD-8-r_emyVXqkKrxsN6cku7kD2eb_s7sRqpchoO4-FPPP_d2J0XpGF_5NZZHRnnMiJQaGZ_krE_6cOPQ=s2048" alt=""><figcaption></figcaption></figure>',
            '13. <figure><img src="https://lh4.googleusercontent.com/ouERSw5xhhN_7adslpMp58k5wTeOr8xt2r0ZdeQJnYOD6d43plwpVWaHvyqsCXizBkRrIg6y-CZOCYaVfzbpcAtPsLTU23V4ldc9EbI2sq1Lc9US33BmqpZuVeeZbRyB8WPuLTzSkDsQhSBxY2gpo0c0yg=s2048" alt=""><figcaption></figcaption></figure>',
            "14. <details>",
            "15. <summary>Why?</summary>",
            "16. * Quicksort isn\u2019t stable, but there\u2019s only one way to order them. Wouldn\u2019t have multiple types of orders.\n* Could sort by other things, say the sum of the digits.&#x20;\n* Order by the number of digits.\n* My usual answer: 5 is just 5. There are no different possible 5's.\n* When you are using a primitive value, they are the \u2018same\u2019. A 4 is a 4. Unstable sort has no observable effect.\n* There\u2019s really only one natural order for numbers, so why not just assume that\u2019s the case and sort them that way.&#x20;\n* By contrast, objects can have many properties, e.g. section and name, so equivalent items CAN be differentiated.\n* If you know there\u2019s only one way, can you force Java to use Quicksort?&#x20;",
            "17. </details>",
            "18. ## Sorting",
            "19. Sorting is a foundational problem.",
            "20. * Obviously useful for putting things in order.\n* But can also be used to solve other tasks, sometimes in non-trivial ways.\n  * Sorting improves duplicate findings from a naive N^2 to N log N.\n  * Sorting improves 3SUM from a naive N^3 to N^2.\n* There are many ways to sort an array, each with its own interesting tradeoffs and algorithmic features.",
            "21. Today we\u2019ll discuss the fundamental nature of the sorting problem itself: How hard is it to sort?",
            "22. ## Sorts Summary",
            '23. <figure><img src="../.gitbook/assets/screenshot 2023-04-14 at 1.43.52 AM.png" alt=""><figcaption></figcaption></figure>',
            "24. \\\n",
        ],
        "34.-sorting-and-algorithmic-bounds/34.5-exercises.md": [
            "1. # 34.5 Exercises",
            "2. ## Factual",
            "3. 1. Which of the following function(s) have the slowest order of growth in terms of Big Theta?\n   * [ ] $$N \\log N$$\n   * [ ] &#x20;$$\\log N!$$\n   * [ ] $$N^2$$\n   * [ ] $$N! \\log N!$$\n2. To solve puppy, cat, dog for 12 items, what is the theoretical minimum number of comparisons we have to make, based on the argument used in lecture? Please round your answer up to the nearest whole number.\n3. Which of the following statements are true?\n   * [ ] The optimal sorting algorithm takes at most $$\\Theta(N \\log N)$$ time.\n   * [ ] It is impossible to find a comparison-based sorting algorithm that takes less than $$\\Theta(N \\log N)$$ comparisons.\n   * [ ] It is impossible to find a comparison-based sorting algorithm that takes less than $$\\Theta(N \\log N)$$ time.\n   * [ ] It is impossible to find a sorting algorithm that takes less than $$\\Theta(N \\log N)$$ time.",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. In lecture, we proved that $$\\log N! \\in \\Theta(N \\log N)$$. Thus, both $$N \\log N$$ and $$\\log N!$$ have the same order of growth, and are slower than $$N^2$$ or $$N! \\log N!$$.",
            "7. * [x] $$N \\log N$$\n* [x] $$\\log N!$$\n* [ ] $$N^2$$\n* [ ] $$N! \\log N!$$",
            "8. </details>",
            "9. <details>",
            "10. <summary>Problem 2</summary>",
            "11. $$ceil(\\log_2 12!) = 29$$, based on the equation we saw in lecture.",
            "12. </details>",
            "13. <details>",
            "14. <summary>Problem 3</summary>",
            '15. * [x] **The optimal sorting algorithm takes at most** $$\\Theta(N \\log N)$$ **time**. We know of several sorts (for example, mergesort) that take $$\\Theta(N \\log N)$$ time in the worst case. So the hypothetical "best" sorting algorithm cannot be worse than this.\n* [x] **It is impossible to find a comparison-based sorting algorithm that takes less than** $$\\Theta(N \\log N)$$ **comparisons.** This is the comparison sorting bound proved in lecture.\n* [x] **It is impossible to find a comparison-based sorting algorithm that takes less than** $$\\Theta(N \\log N)$$ **time**. Each comparison must take at least constant time, so the time complexity of comparison-based sorts has the same lower bound as the number of comparisons.\n* [ ] **It is impossible to find a sorting algorithm that takes less than** $$\\Theta(N \\log N)$$ **time.** The lower bound proved in lecture only applies to comparison-based sorts. We will see in future lectures how to use non-comparison sorts to reduce this runtime even further.',
            "16. </details>",
            "17. ## Metacognitive",
            "18. 1. Suppose we add a new method to `Arrays.sort` that takes in an array of strings. What algorithm should `Arrays.sort(String[] x)` use?",
            "19. <details>",
            "20. <summary>Problem 1</summary>",
            "21. Quicksort. We don't need stability, since strings are only equal by `.equals` if they are exactly the same. Quicksort, then, is the best algorithm since it is empirically the fastest.",
            "22. </details>\n",
        ],
        "34.-sorting-and-algorithmic-bounds/34.3-theoretical-bounds-on-sorting.md":
            [
                "1. ---\ndescription: What is the best time we can get for sorting?\n---",
                "2. # 34.3 Theoretical Bounds on Sorting",
                "3. So far, the fastest sorts we have gone over have had $$\\Theta(N*logN)$$as the worse case time. This begs the question; can we get a sorting algorithm that would have a better asymptotic analysis?&#x20;",
                "4. ",
                "5. Let's say that we have \"the algorithm ultimate comparison sort\" known as TUCS which is the fastest possible comparison sort. We don't know the details of this yet-to-be-discovered algorithm as it has not been discovered yet. Let's additionally say that $$R(N)$$is the worst-case runtime of TUCS and give ourselves the challenge of finding the best $$\\Theta$$ and $$O$$ bounds for $$R(N)$$. (This might seem a little odd since we really don't know the details of it, but we can find $$R(N)$$).",
                "6. ",
                "7. As a starting point, we can go ahead and have the $$O$$ bound of $$R(N)$$ be $$O(N*logN)$$. Why? If this is the fastest sorting algorithm it must be at least just as good as the algorithms we already have! Thus it must at least be as good as Mergesort's worst case as otherwise we would have a contradiction in calling this algorithm TUCS.&#x20;",
                "8. ",
                "9. What about the starting point for the $$\\Omega$$ bound of $$R(N)$$? Since we don't quite know any details about it we can start by giving it the best possible runtime of $$\\Omega(1)$$. But can we make this tighter?&#x20;",
                "10. ![](<../.gitbook/assets/image (108).png>)\\\n\\\nIt turns out we can as if the algorithm sorts all of N elements, it must have gone through all of the N elements at some point of sorting it. Thus, we can arrive at a tighter bound of $$\\Omega(N)$$. But can we make it tighter? The answer is yes, but to see the argument for why we have to consider the following game.&#x20;",
                "11. ![](<../.gitbook/assets/image (135).png>)",
                "12. ### A Game of Cat and Mouse (and Dog)",
                "13. Let's say we place Tom the Cat, Jerry the Mouse, and Spike the Dog in opaque soundproof boxes labeled A, B, and C. We want to figure out which is which using a scale.",
                "14. ![](https://lh3.googleusercontent.com/KkM-v6NCPT33GePoBDJ9JgLyKPtfEMPlIU9H0prq3-AX2qTd4\\_BiKjCw4BCZ82Q1zTeaGPcJK15w2neC5PVAI2EAQjFBdj4bV7qZTz-uPUOg6o9IvyOsSFuu4AfEQFnAAKpc1eU4DIbMjWZUY5ZIMhHA8Q=s2048)",
                "15. Let's say we knew that Box A weighs less than Box B, and that Box B weighs less than Box C. Could we tell which is which?&#x20;",
                "16. ",
                "17. The answer turns out to be yes!&#x20;",
                '18. <figure><img src="../.gitbook/assets/image (101).png" alt=""><figcaption></figcaption></figure>',
                "19. We can find a sorted order by just considering these two inequalities. What if Box A weighs more than Box B, and that Box B weighs more than Box C? Could we still find out which is which?",
                "20. ",
                '21. <figure><img src="../.gitbook/assets/image (100).png" alt=""><figcaption></figcaption></figure>',
                "22. The answer turns out to be yes! So far, we have been able to solve this game with just two inequalities! Let's go ahead and try a third case scenario. Could we know which is which if Box A weighs less than Box B, but Box B weighs more than Box C?",
                '23. <figure><img src="../.gitbook/assets/image (92).png" alt=""><figcaption></figcaption></figure>',
                "24. The answer turns out to be no. It turns out to have two possible orderings:",
                "25. * a: mouse, c: cat, b: dog (sorted order: acb)\n* c: mouse, a: cat, b: dog (sorted order: cab)",
                "26. So while we were on a really great streak of solving the game with only two inequalities, we will need a third to solve all possibilities of the game. If we add the inequality a < c then this problem goes away and becomes solvable.&#x20;",
                '27. <figure><img src="../.gitbook/assets/image (151).png" alt=""><figcaption></figcaption></figure>',
                "28. Now that we've created this table, we can create a tree to help us solve this sorting game of cat and mouse (and dog).",
                '29. <figure><img src="../.gitbook/assets/image (158).png" alt=""><figcaption></figcaption></figure>',
                "30. But how can we make this generalizable for all sorting? We know that each leaf in this tree is a unique possible answer to how boxes should be sorted. So the number of ways the boxes can be sorted should turn out to be the number of leaves in the tree. That then begs the question of how many ways can N boxes be sorted? The answer turns out to be $$N!$$ ways as there are $$N!$$ permutations of a given set of elements.",
                "31. ",
                "32. So how many questions do we need to ask in order to know how to sort the boxes? We would need to find the number of levels we must go through to get our answer in the leaf.  Since it's a binary tree the minimum amount levels turn out $$lg(N!)$$levels to reach a leaf. (Note that $$lg$$ means (log base 2)).",
                "33. ",
                "34. So, using this game we have found that we must ask at least $$lg(N!)$$ questions about inequalities to find a proper way to sort it. Thus our lower bound for $$R(N)$$ is $$\\Omega(lg(N!))$$.\n",
            ],
        "35.-radix-sorts/35.3-msd-radix-sort.md": [
            "1. ---\ndescription: 'Basic idea: Just like LSD, but sort from leftmost digit towards the right.'\n---",
            "2. # 35.3 MSD Radix Sort",
            "3. <details>",
            "4. <summary>Suppose we sort by topmost digit, then middle digit, then rightmost digit. Will we arrive at the correct result?    </summary>",
            "5. No, this will result in sorting the array by right most digit only as we are not saving the topmost and middle digit.",
            "6. </details>",
            '7. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-15 at 2.55.34 AM.png" alt=""><figcaption><p>MSD Radix Sort (correct edition)</p></figcaption></figure>',
            "8. Notice first we sorted by leftmost digit. Then we grouped the data by the leftmost digit, so one group would start with a's, then the next group would start with b's, and so on and so forth. Then within our subgroups we would order by middle digit, and create newer subgroups. And finally we would break this up into further subgroups until we have all individual subproblems. This final result would be sorted.",
            "9. ### Runtimes",
            "10. Best Case.&#x20;",
            "11. * We finish in one counting sort pass, looking only at the top digit: $$\\Theta(N+R)$$",
            "12. Worst Case.",
            "13. * We have to look at every character, degenerating to LSD sort: $$\\Theta(WN+WR)$$",
            "14. ### Summary of Runtimes",
            '15. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-15 at 2.58.27 AM.png" alt=""><figcaption></figcaption></figure>',
            "16. ",
        ],
        "35.-radix-sorts/35.4-summary.md": [
            "1. # 35.4 Summary",
            "2. **Terminology.**",
            "3. * Radix - just another word for \u2018base\u2019 as in the base of a number system. For example, the radix for words written in lowercase English letters is 26. For number written in Arabic numerals it is 10.\n* Radix sort - a sort that works with one character at a time (by grouping objects that have the same digit in the same position).\n* Note: I will use \u2018character\u2019 and \u2018digit\u2019 interchangably in this study guide.",
            "4. **Counting Sort.** Allows you to sort $$N$$ keys that are integers between $$0$$ and $$R-1$$ in $$\\Theta(N+R)$$time. Beats linearithmic lower bound by avoiding any binary compares. This is a completely different philosophy for how things should be sorted. This is the most important concept for this lecture.",
            "5. **LSD.** In the LSD algorithm, we sort by each digit, working from right to left. Requires examination of $$\\Theta(WN)$$digits, where $$W$$is the length of the longest key. Runtime is $$\\Theta(WN+WR)$$, though we usually think of $$R$$ as a constant and just say $$\\Theta(WN)$$. The $$\\Theta(WR)$$ part of the runtime is due to the creation fo length $$R$$ arrows for counting sort. We usually do LSD sort using counting sort as a subroutine, but it\u2019s worth thinking about whether other sorts might work as well.",
            "6. **LSD vs Comparison Sorting.** Our comparison sorts, despite requiring $$\\Theta(N*logN)$$ time, can still be faster than LSD sort. For extremely large N, LSD sort will naturally win, but log N is typically pretty small. Know which algorithm is best in the two extreme cases of very long dissimilar strings and very long, nearly equal strings.",
            "7. **MSD.** In MSD sorting, we work from left to right, and solve each resulting subproblem independently. Thus, for each problem, we may have as many as $$R$$ subproblem. Worst case runtime is exactly the same as LSD sort, $$\\Theta(WN+WR)$$, though can be much better. In the very best case, where we only have to look at the top character (only possible for $$R>N$$), we have a runtime of $$\\Theta(N+R)$$.\n",
        ],
        "35.-radix-sorts/35.2-lsd-radix-sort.md": [
            "1. # 35.2 LSD Radix Sort",
            '2. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-15 at 2.44.24 AM.png" alt=""><figcaption><p>Least Significant Digit Radix Sort -- Using Count Sort</p></figcaption></figure>',
            "3. Notice in the above picture that we had a completely random input of numbers 22, 34, 41, etc. Then in the second box, we had sorted by right most digit, so 41, 41, 31 would come first as 1 is the lowest rightmost digit. Next would come 32, 22, 12, as 2 is the second least rightmost digit. Notice that we still have to take care of sorting 32, 22, and 12 which is why we move to the second right most digit in our last box which has everything sorted.&#x20;",
            "4. What is the runtime of LSD sort?",
            "5. * $$\\Theta(WN+WR)$$\n* $$N$$: Number of items, $$R$$: size of alphabet, $$W$$: Width of each item in # digits",
            '6. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-15 at 2.49.06 AM.png" alt=""><figcaption><p>Non-equal Key Lengths</p></figcaption></figure>',
            "7. ",
        ],
        "35.-radix-sorts/README.md": [
            "1. ---\ndescription: By Mihir Mirchandani\n---",
            "2. # 35. Radix Sorts",
            "3. ",
        ],
        "35.-radix-sorts/35.1-counting-sort.md": [
            "1. # 35.1 Counting Sort",
            "2. Imagine if instead of driving a slow Honda Civic, we started driving a fast Ferrari. Unfortunately, we won't actually be driving in a Ferrari today, but we will witness a blazing fast algorithm that's just as fast called Radix Sorts.&#x20;",
            "3. When sorting an array, sorting requires $$\\Omega(N \\log N)$$compare operations in the worst case (array is sorted in descending order). Thus, the ultimate comparison based sorting algorithm has a worst case runtime of $$\\Theta(N \\log N)$$.",
            "4. From an asymptotic perspective, that means no matter how clever we are, we can never beat Merge Sort\u2019s worst case runtime of $$\\Theta(N \\log N)$$. But what if we don't compare at all?",
            "5. &#x20;",
            '6. <figure><img src="../.gitbook/assets/Screen Shot 2023-04-15 at 2.26.42 AM.png" alt=""><figcaption><p>Left is original, right is ordered output</p></figcaption></figure>',
            "7. Essentially what just happened is that we first made a new array of the same size and then just copied all of the # indexes to the correct location. So first we look at 5 Sandra Vanilla Grimes and then copy this over to the 5th index in our new array.",
            "8. This does guarantee $$\\Theta(N)$$ worst case time. However what if we were working with&#x20;",
            "9. * Non-unique keys.\n* Non-consecutive keys.\n* Non-numerical keys.",
            "10. All of these cases are complex cases that aren't so simple to deal with. Essentially what we can do is create a simpler method which is to:",
            "11. * Count number of occurrences of each item.\n* Iterate through list, using count array to decide where to put everything.",
            "12. Bottom line, we can use counting sort to sort $$N$$ objects in $$\\Theta(N)$$ time.&#x20;\n",
        ],
        "35.-radix-sorts/35.5-exercises.md": [
            "1. # 35.5 Exercises",
            "2. ## Factual",
            "3. 1. If we use counting sort to sort all 61B students by the number of siblings they have, how long will the counting array be? Let $$N$$ be the total number of students and $$M$$ be the maximum number of siblings a student has.\n2. When performing a radix sort like LSD or MSD, which of the following sorting algorithms could we use for each digit?\n   * [ ] Counting sort\n   * [ ] Heapsort\n   * [ ] Mergesort\n   * [ ] Selection sort\n3. If we're sorting N lowercase english strings using MSD sort, what is the maximum number of subproblems after considering only the the most significant digit?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. The counting array will have $$M$$ entries. In general for counting sort, the count array's length only needs to be equal to the maximum number of unique elements we are counting.",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. Recall that the sub-sort for each digit must be stable. This rules out heapsort and selection sort.",
            "11. * [x] Counting sort\n* [ ] Heapsort\n* [x] Mergesort\n* [ ] Selection sort",
            "12. </details>",
            "13. <details>",
            "14. <summary>Problem 3</summary>",
            "15. 26, since there are 26 possible values for the first digit we're sorting on.",
            "16. </details>",
            "17. ## Metacognitive",
            "18. 1. What are some advantages of counting sort over quicksort?\n2. Why do we use LSD over counting sort?\n3. If we use MSD radix sort, we start with a single problem of size N, where N is the number of strings. Depending on the results on the most significant digit, we end up with a larger number of smaller subproblems, i.e. we're doing some divide and conquer. In the worst case, we end up with only one subproblem, also of size N. When does this happen?",
            "19. <details>",
            "20. <summary>Problem 1</summary>",
            "21. Many correct answers, but some advantages include:",
            "22. * For sufficiently large N, counting sort is faster (since it is linear as compared to N log N).\n* Counting sort is stable, since we scan from the first element to the last.\n* Counting sort doesn't use comparisons between elements, which might be useful if elements are not comparable.",
            "23. </details>",
            "24. <details>",
            "25. <summary>Problem 2</summary>",
            "26. Counting sort is technically faster than LSD, since counting sort doesn't require splitting up items by digits/letters. However, counting sort can require extremely large amounts of memory in some cases (for example, when sorting `String`s, you need one entry in an array for every possible `String`).&#x20;",
            "27. </details>",
            "28. <details>",
            "29. <summary>Problem 3</summary>",
            "30. This happens when items start with the same digit. In this case, all items end up in the same subgroup after the first iteration of MSD, and we recurse on a problem of size N.",
            "31. </details>\n",
        ],
        "39.-compression-complexity-p-np/39.4-p-np.md": [
            "1. # 39.4 P = NP",
            "2. ## Reductions",
            "3. It turns out that space-time-bounded compression reduces to 3SAT, INDSET, LONGESTPATH, and many other hard problems. (The actual proof of such reductions is incredibly complex and ommitted from this textbook).",
            '4. <figure><img src="../.gitbook/assets/image (155).png" alt=""><figcaption><p>Space-time-bounded compresion can be solved with LONGEST_PATH.</p></figcaption></figure>',
            "5. The reason that space-time-compression can be turned into longest paths (or any other problem mentioned above) is that all these problems are part of a **complexity class** known as **NP**. A property of such problem is that any NP problem can be reduced to any NP-complete problem, including longest paths.",
            "6. In subsequent section, we will briefly cover what P, NP, and complexity classes are. However, most of these topics are far beyond the scope of this textbook or course, and would be better served by taking an upper-level algorithms course.",
            "7. ## P and NP",
            "8. All yes/no problems can be divided into two main classes:",
            "9. * P: efficiently solvable problems.\n* NP: problems with solutions that are efficiently verifiable. This means that given an answer to the problem, you can efficiently check whether the answer is correct or not.",
            "10. Examples of problems in P include (note that P is a subset of NP):",
            "11. * Is this array sorted?&#x20;\n  * This can be solved by sorting the array using any sorting algorithm, and verified by checking that adjacent elements are increasing.\n* Does this array have duplicates?\n  * This can be solved with a double for-loop, and verified in a similar manner.",
            "12. Examples of problems in NP include:",
            "13. * Is there a solution to this 3SAT problem?\n  * Generating a solution to a 3SAT problem is difficult, but given an assignment of symbols to booleans, you can simply plug in the values and check that the equation is satisfied.\n* In graph G, does there exist a path from s to t of weight > k?\n  * Genearting a solution to this (essentially longest paths) is difficult, but given a path, you can easily verify if it is a valid path from s to t and that its weight is > k.",
            "14. Examples of problems not in NP include:",
            "15. * Is this the best chest move I can make next?\n  * There is no efficient way to verify that a chess move is indeed optimal, unless you draw out all possibilities for all subsequent moves.\n* What is the longest path?\n  * This is not a yes-no question.",
            "16. ## NP-Completeness",
            '17. An unexpected property of NP problems is that every NP problem reduces to every NP-complete problem. This reduction is also "efficient", in that the problem can be transformed (pre-processed and post-processed) in polynomial time.',
            "18. This also means that solving any NP-complete problem essentially solves all problems in NP. As of today, there are tens of thousands of known NP-complete problems, but none of them have been solved yet.",
            '19. <figure><img src="../.gitbook/assets/image (149).png" alt=""><figcaption><p>NP-complete problems and reductions.</p></figcaption></figure>',
            "20. ## P = NP?",
            "21. An open question in computer science is whether P = NP; in other words, are all problems with efficiently verifiable problems (NP) also efficiently solvable (P)?",
            "22. One reason to suggest that P = NP might be true is that checking an answer is always efficient. Thus, given the right pruning, could we efficiently zero in on an answer?\n",
        ],
        "39.-compression-complexity-p-np/39.1-models-of-compression.md": [
            "1. # 39.1 Models of Compression",
            "2. ## Comparing Compression Algorithms",
            "3. In the last chapter, we saw many approaches to compression. This raises an interesting question: for a given bitstream, what is the best algorithm for compression?",
            "4. For example, consider compression the text of Moby Dick using different compression formats. In this case, `bzip2` yields the best compression.",
            '5. <figure><img src="../.gitbook/assets/image (136).png" alt=""><figcaption><p>Different compression formats applied to <code>mobydick.txt</code></p></figcaption></figure>',
            "6. One might argue, however, that the best possible compression algorithm for `mobydick.txt` is simply as follows:",
            '7. <pre><code>D(B):\n<strong>    if input == 0:\n</strong>        return "Call me Ishamel. ...."\n    else:\n        return the text as is\n</code></pre>',
            "8. Using this as our decompression function, we can condense all of Moby Dick into a single bit!",
            "9. However, there is a problem with this approach. If we include the code for the decompression algorithm as part of the compressed model (recall compression model 2 from the previous chapter), we see that Moby Dick is not compressed to one bit, but actually requires _more_ bits than the original text!",
            "10. ## Decompression Algorithms",
            "11. Ultimately, the goal of a compresion algorithm is to find short sequences of bits that generate desired longer sequences of bits. Formally stated, our problem is as follows:",
            "12. * Given a sequence of bits `B`, find a shorter sequence `DA + C(B)` that produces `B` when fed into an interpreter. `DA` represents the bits for the decompression algorithm, while `C(B)` represents the compressed version of `B`.",
            '13. <figure><img src="../.gitbook/assets/image (153).png" alt=""><figcaption><p>Our compression model applied to <code>mobydick.txt</code>.</p></figcaption></figure>',
            "14. ## Improving Compression",
            "15. Recall the `HugPlant` example from the previous chapter. Using Huffman encoding, we can achieve a compression ratio of 25%.&#x20;",
            "16. However, using another algorithm we'll call `MysteryX` for now, we can compress `HugPlant.bmp` down to 29,432 bits! This achieves a 0.35% compression ratio. Out of the $$2^{8389594}$$ possible bistreams of length $$8,389,594$$, only one in $$2^{8360151}$$ can achieve such a compression ratio.",
            '17. <figure><img src="../.gitbook/assets/image (157).png" alt=""><figcaption><p>A mystery compression algorithm that outperforms Huffman encoding by a large margin.</p></figcaption></figure>',
            "18. What is `MysteryX`? Well, it's simply the Java code `HugPlant.java` written to generate the `.bmp` file! Going back to the model of self-extracting bits, we see the power of code and interpreters in compression. This leads us to two interesting questions:",
            "19. * **comprehensible compression:** given a target bitstream `B`, can we create an algorithm that outputs useful, readable Java code?\n* **optimal compression**: given a target bitstream `B`, can we find the _shortest_ possible program that outputs this bitstream?\n",
        ],
        "39.-compression-complexity-p-np/39.2-optimal-compression-kolmogorov-complexity.md":
            [
                "1. # 39.2 Optimal Compression, Kolmogorov Complexity",
                "2. ## Kolmogorov Complexity",
                "3. We define the **Kolmogorov complexity** of a bitstream `B` to be the shortest bitstream $$C_B$$ that outputs `B`. Let the _Java-Kolmogorov complexity_ $$K_J(B)$$ be the shortest Java program that generates `B`.",
                "4. Note that for any bitstream $$B$$, $$K(B)$$ definitely exists. However, finding and proving $$K(B)$$ might be difficult or even impossible.",
                "5. ### Languages and Complexity",
                "6. An important thing to note is that Kolmogorov complexity is language-independent. To run any program in one language in another, all I have to do is write an interpreter. For example, if I want to run a Python program that is not easily translatable to Java, I could instead just write a Java interpreter to read the text of the Python program and run it. In this case, $$K_J(B) \\leq K_P(B) + I$$, where $$I$$ is the length of the interpreter (a constant value).",
                "7. This highlights a very deep fact about Kolmogorov complexity: most bitstreams are fundamentally incompressible no matter which language we choose for our compression algorithm.",
                "8. Consider a bitstream of 1,000,000 bits. Out of all compression algorithms possible, only 1 in $$2^{4999999}$$ bitstreams have a change of being compressed by more than 50% (499,999 bits or less).",
                "9. ### Uncomputability",
                "10. Another important fact regarding Kolmogorov complexity is that it is impossible to compute. A proof of this fact is provided [here](https://en.wikipedia.org/w/index.php?title=Kolmogorov\\_complexity#Uncomputability\\_of\\_Kolmogorov\\_complexity).",
                '11. Practically, this means that it is impossible to write a "perfect" (optimal) compression algorithm, since we can\'t even compute the length of the shortest program!\n',
            ],
        "39.-compression-complexity-p-np/README.md": [
            "1. # 39. Compression, Complexity, P = NP",
            "2. ",
        ],
        "39.-compression-complexity-p-np/39.3-space-time-bounded-compression.md":
            [
                "1. # 39.3 Space/Time-Bounded Compression",
                '2. As described in the previous chapter, it is impossible to write the "perfect" compression algorithm that requires the fewest bits to output some bitstream $$B$$.&#x20;',
                "3. ## Space-Bounded Compression",
                "4. However, what about the problem of space-bounded compression? In this problem, we take in two inputs: a bitstream $$B$$ and a target size $$S$$. The goal, then, is to find a program of length $$\\leq S$$ that outputs $$B$$.&#x20;",
                "5. It turns out that such a problem is also uncomputable. If it were, then we could simply binary search on different values of $$S$$ to find the optimal compression program size, which is impossible as shown in te previous section.",
                "6. ## Space-Time-Bounded Compression",
                "7. What if we take our problem from above, and add a constraint that we can run at most $$T$$ lines of bytecode?",
                "8. It might seem unintuitive, but this kind of problem is actually solvable. We will use the following algorithm:",
                "9. ```\nfor length L = 1....S:\n    for each possible program P of length L:\n        while (P is running && !(B is outputted) && lines_executed < T):\n            run the next line of P\n```",
                "10. The runtime of this algorithm is $$O(T * 2^S)$$, and in the end, it will either output some program `P` that has the correct output and is bounded by $$T$$ and $$S$$, or return that no such program is possible.",
                "11. ## Efficient Bounded Compression",
                "12. The runtime above is exponential in $$S$$. Thus, we might ask if it's possible to solve the space-time-bounded compression problem _efficiently_. As we'll see in the next chapter, this depends on our definition of efficiency.\n",
            ],
        "39.-compression-complexity-p-np/39.5-exercises.md": [
            "1. # 39.5 Exercises",
            "2. ## Factual",
            "3. 1. True/false: suppose we have a 45000-bit program in Python that outputs a bitstream B. What is the maximum size of an interpreter written in Java that proves the Java-Kolmogorov complexity is less than 100,000?\n2. What are the two defining properties of an NP problem?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. The interpreter must be 55,000 bits or less. If it is, we can simply decompress the bitstream by running the Python program in the interpreter, using less than 100000 bits total (program + interpreter).",
            "7. </details>",
            "8. <details>",
            "9. <summary>Problem 2</summary>",
            "10. * The problem is a yes-no problem.\n* A solution to the problem is efficiently verifiable.",
            "11. </details>",
            "12. ## Procedural",
            "13. 1. What is the probability that some sequence of 1 million bits could be compressed to 900,000 bits or less?",
            "14. <details>",
            "15. <summary>Problem 1</summary>",
            "16. There are $$2^{1000000}$$ bit sequences of length 1 million, but only $$2^0 + 2^1 + ... 2^{899999} + 2^{900000} = 2(2^{900000}) - 1 \\approx 2^{900001}$$ bit sequences of length 900,000 or less. This means that the probability of any bitsequence being compressed to 900,000 bits or less is $$\\frac{2^{900001}}{2^{1000000}} = \\frac{1}{2^{99999}}$$.",
            "17. </details>\n",
        ],
        "16.-adts-and-bsts/16.3-bst-definitions.md": [
            "1. # 16.3 BST Definitions",
            "2. Trees are composed of nodes and edges that connect those nodes.&#x20;",
            "3. **Constraint**: there is only one path between any two nodes.",
            "4. In some trees, we select a **root** node which is a node that has no parents.",
            "5. A tree also has **leaves**, which are nodes with no children.",
            "6. In the picture below, the green structures are valid trees, while the pink structure is not (since it has a cycle).",
            '7. <figure><img src="../.gitbook/assets/image (58).png" alt=""><figcaption><p>Examples of valid and invalid trees</p></figcaption></figure>',
            "8. Relating this to the original tree structure we came up with earlier, we can now introduce new constraints to the already existing constraints. This creates more specific types of trees, two examples being Binary Trees and Binary Search Trees.",
            "9. * **Binary Trees**: in addition to the above requirements, also hold the binary property constraint. That is, each node has either 0, 1, or 2 children.\n* **Binary Search Trees**: in addition to all of the above requirements, also hold the property that For every node X in the tree:\n  * Every key in the left subtree is less than X\u2019s key.\n  * Every key in the right subtree is greater than X\u2019s key. \\*\\*Remember this property!! We will reference it a lot throughout the duration of this module and 61B.",
            '10. <figure><img src="../.gitbook/assets/image (139).png" alt=""><figcaption><p>A valid BST: every key in the left subtree is smaller than its parent, and every key in the right subtree is larger.</p></figcaption></figure>',
            "11. Here is the BST module we'll be using for the rest of this chapter:",
        ],
        "16.-adts-and-bsts/16.2-binary-search-trees.md": [
            "1. # 16.2 Binary Search Trees",
            "2. Linked Lists are great, but it takes a long time to search for an item, even if the list is sorted! What if the item is at the end of the list? That would take linear time!",
            "3. We know that for an array, we can use binary search to find an element faster. Specifically, in $$\\log n$$ time. For a short explanation of binary search, check out this [link](https://www.geeksforgeeks.org/binary-search/).",
            "4. In binary search, we know the list is sorted, so we can use this information to narrow our search. First, we look at the middle element. If it is bigger than the element we are searching for, we look to the left of it. If it is smaller than the element we are searching for, we look to the right. We then look at the middle element of the respective halves and repeat the process until we find the element we are looking for (or don't find it because the list doesn't contain it).",
            "5. But how do we run binary search for a linked list? We would have to traverse all the way to the middle in order to check the element there, which takes linear time just on its own!",
            "6. One optimization we can implement is to have a reference to the middle node. This way, we can get to the middle in constant time. Then, if we flip the nodes' pointers, which allows us to traverse to both the left and right halves, we can decrease our runtime by half!",
            '7. <figure><img src="../.gitbook/assets/image (57).png" alt=""><figcaption><p>A linked list with a middle pointer</p></figcaption></figure>',
            "8. But we can further optimize by adding pointers to the middle of each recursive half like so. Now, if you stretch this structure vertically, you will see a tree! This specific tree is called a **binary tree** because each juncture splits in 2.",
            '9. <figure><img src="../.gitbook/assets/image (11).png" alt=""><figcaption><p>A linked list with recursive middle pointers is a binary tree!</p></figcaption></figure>',
            '10. {% embed url="https://www.youtube.com/watch?v=AcRKQOe0zYg" %}\n',
        ],
        "16.-adts-and-bsts/16.5-bsts-as-sets-and-maps.md": [
            "1. # 16.5 BSTs as Sets and Maps",
            "2. We can use a BST to implement the `Set` ADT. If we use a BST, we can decrease the runtime of `contains` to $$\\log(n)$$ because of the BST property which enables us to use binary search!",
            "3. We can also make a binary tree into a map by having each BST node hold `(key,value)` pairs instead of singular values. We will compare each element's key in order to determine where to place it within our tree.\n",
        ],
        "16.-adts-and-bsts/16.6-summary.md": [
            "1. # 16.6 Summary",
            "2. Abstract data types (ADTs) are defined in terms of operations, not implementation.Several useful ADTs:",
            "3. * Disjoint Sets, Map, Set, List.\n* Java provides Map, Set, List interfaces, along with several implementations.",
            "4. We\u2019ve seen two ways to implement a Set (or Map):",
            "5. * ArraySet: \u0398(N) operations in the worst case.\n* BST: \u0398(logN) operations if tree is balanced.",
            "6. BST Implementations:",
            "7. * Search and insert are straightforward (but insert is a little tricky).\n* Deletion is more challenging. Typical approach is \u201cHibbard deletion\u201d.\n",
        ],
        "16.-adts-and-bsts/16.4-bst-operations.md": [
            "1. # 16.4 BST Operations",
            '2. {% embed url="https://www.youtube.com/watch?v=PLyDf3_J7Cc" %}',
            "3. ### Search",
            "4. To search for something, we employ binary search, which is made easy due to the BST property.&#x20;",
            "5. We know that the BST is structured such that all elements to the right of a node are greater and all elements to the left are smaller. Knowing this, we can start at the root node and compare it with the element, X, that we are looking for. If X is greater to the root, we move on to the root's right child. If its smaller, we move on to the root's left child. We repeat this process recursively until we either find the item or we get to a leaf, in which case the tree does not contain the item.",
            "6. ```java\nstatic BST find(BST T, Key sk) {\n   if (T == null)\n      return null;\n   if (sk.equals(T.key))\n      return T;\n   else if (sk \u227a T.key)\n      return find(T.left, sk);\n   else\n      return find(T.right, sk);\n}\n```",
            '7. If our tree is relatively "bushy", the find operation will run in $$\\log n$$ time because the height of the tree is $$\\log n$$.',
            "8. ### Insert",
            "9. We **always** insert at a leaf node!",
            "10. First, we search in the tree for the node. If we find it, then we don't do anything. If we don't find it, we will be at a leaf node already. At this point, we can just add the new element to either the left or right of the leaf, preserving the BST property.",
            '11. {% embed url="https://www.youtube.com/watch?v=otDvoMb8UqE" %}',
            "12. ```java\nstatic BST insert(BST T, Key ik) {\n  if (T == null)\n    return new BST(ik);\n  if (ik \u227a T.key)\n    T.left = insert(T.left, ik);\n  else if (ik \u227b T.key)\n    T.right = insert(T.right, ik);\n  return T;\n}\n```",
            "13. ### Delete",
            "14. Deleting from a binary tree is a little bit more complicated because whenever we delete, we need to make sure we reconstruct the tree and still maintain its BST property. Let's break this problem down into three categories:",
            "15. * the node we are trying to delete has no children\n* has 1 child\n* has 2 children",
            "16. #### Deletion: No Children",
            "17. If the node has no children, it is a leaf, and we can just delete its parent pointer and the node will eventually be swept away by the [garbage collector](https://stackoverflow.com/questions/3798424/what-is-the-garbage-collector-in-java).",
            "18. #### Deletion: One Child",
            "19. If the node only has one child, we know that the child maintains the BST property with the parent of the node because the property is recursive to the right and left subtrees. Therefore, we can just reassign the parent's child pointer to the node's child and the node will eventually be garbage collected.",
            "20. #### Deletion: Two Children",
            "21. If the node has two children, the process becomes a little more complicated because we can't just assign one of the children to be the new root. This might break the BST property.",
            "22. Instead, we choose a new node to replace the deleted one.",
            "23. We know that the new node must:",
            "24. * be > than everything in left subtree.\n* be < than everything right subtree.",
            "25. In the below tree, we show which nodes would satisfy these requirements given that we are trying to delete the `dog` node.",
            '26. <figure><img src="../.gitbook/assets/image (7).png" alt=""><figcaption><p>Possible candidates to replace <code>dog</code> after deletion</p></figcaption></figure>',
            "27. To find these nodes, you can just take the right-most node in the left subtree or the left-most node in the right subtree. Then, we replace the `dog` node with either `cat` or `elf` and then remove the old `cat` or `elf` node.",
            "28. This is called **Hibbard deletion**, and it gloriously maintains the BST property amidst a deletion.\n",
        ],
        "16.-adts-and-bsts/README.md": [
            "1. # 16. ADTs and BSTs",
            "2. In this Chapter we will discuss:",
            "3. * Abstract Data Types\n* Binary Search Tree\n* BST Definitions\n* BST Operations\n* Sets vs. Maps, Summary",
            "4. Additionally the video playlist is one that accompanies this chapter is the following:",
            '5. {% embed url="https://www.youtube.com/watch?v=aFOSePlOExw" %}\n',
        ],
        "16.-adts-and-bsts/16.7-exercises.md": [
            "1. # 16.7 Exercises",
            "2. ## Factual",
            "3. 1. What is the best and worst-case height of a BST?",
            "4. <details>",
            "5. <summary>Problem 1</summary>",
            "6. If we insert everything in order, the worst-case height of $$\\Theta(N)$$ results. In the best case of a perfectly balanced BST, the best-case height is $$\\Theta(\\log N)$$.",
            "7. </details>",
            "8. ## Procedural",
            "9. 1. Suppose that a certain BST has keys that are integers between 1 and 10. During the search for 5, which of the following sequences of keys are possible?\n   * [ ] 10, 9, 8, 7, 6, 5\n   * [ ] 4, 10, 8, 7, 5, 3\n   * [ ] 1, 10, 2, 9, 3, 8, 4, 7, 6, 5\n   * [ ] 1, 2, 6, 8, 9, 5\n2. Consider the below BST. What is the result after deleting 4 using Hibbard deletion, choosing the sucessor as the replacement?",
            '10. <figure><img src="../.gitbook/assets/image (154).png" alt=""><figcaption></figcaption></figure>',
            "11. 3. Suppose we implement the Stack ADT using an array. What is the worst case runtime of a `push` operation with this underlying data structure?",
            "12. <details>",
            "13. <summary>Problem 1</summary>",
            '14. * [x] `10, 9, 8, 7, 6, 5`: possible; this is the situation where we have a worst-case linear BST.\n* [ ] `4, 10, 8, 7, 5, 3`: not possible; we terminate our search once we reach the desired node.\n* [x] `1, 10, 2, 9, 3, 8, 4, 7, 6, 5`: possible; the idea is that we should always search in the correct "direction" of our target node. If our target node is greater than our current node, then we should go to the right, and our next node should be larger. If our target node is less than our current node, then we should go to the left, and our next node should be smaller.\n* [ ] `1, 2, 6, 8, 9, 5`: not possible; note that this violates the constraint described above. When we reach `8`, we should move to its left branch since our target node `5` is smaller, so we would never search `9`.',
            "15. </details>",
            "16. <details>",
            "17. <summary>Problem 2</summary>",
            "18. ![](<../.gitbook/assets/image (44).png>)",
            "19. </details>",
            "20. <details>",
            "21. <summary>Problem 3</summary>",
            "22. The worst-case runtime is $$\\Theta(N)$$, since a `push` might cause us to resize the underlying array.",
            "23. </details>",
            "24. ## Metacognitive",
            "25. 1. If inserting our data into a BST in random order yields $$\\log N$$ height with high probability, why don't we just shuffle our data before inserting into the BST?\n2. When we do Hibbard deletion from a BST, we always choose the successor as a replacement. The successor is guaranteed to only have zero or one child--why?",
            "26. <details>",
            "27. <summary>Problem 1</summary>",
            "28. Often in real-world applications, we don't have all our data at once. For example, imagine you're collecting time-based data that you insert into a BST each time a new value is reported. There is no easy way to shuffle your data when you only get one or a few points at a time.",
            "29. </details>",
            "30. <details>",
            "31. <summary>Problem 2</summary>",
            "32. By definition, the successor is the maximum value in the subtree. Suppose, for the sake of contradiction, that the sucessor had two children. Then, it is not the maximum, since it is less than its right child. This is a contradiction, since we said the sucessor is the maximum value in the subtree. As such, the successor is guaranteed to have one child or less (if it has one child, it is its left child). Previous 16.6 Summary Next 17. Tree Traversals and Graphs",
            "33. </details>\n",
        ],
        "16.-adts-and-bsts/16.1-abstract-data-types.md": [
            "1. # 16.1 Abstract Data Types",
            "2. An Abstract Data Type (ADT) is defined only by its operations, not by its implementation.&#x20;",
            "3. For example in Project 1A, we developed an `ArrayDeque` and a `LinkedListDeque` that had the same methods, but how those methods were written was very different. In this case, we say that `ArrayDeque` and `LinkedListDeque` are _implementations_ of the `Deque` ADT.&#x20;",
            "4. From this description, we see that ADT's and interfaces are somewhat related. Conceptually, `Deque` is an interface for which `ArrayDeque` and `LinkedListDeque` are its implementations. In code, in order to express this relationship, we have the `ArrayDeque` and `LinkedListDeque` classes inherit from the `Deque` interface.",
            "5. Some commonly used ADT's are:",
            "6. * Stacks: Structures that support last-in first-out retrieval of elements\n  * `push(int x)`: puts x on the top of the stack\n  * `int pop()`: takes the element on the top of the stack\n* **Lists**: an ordered set of elements\n  * `add(int i)`: adds an element\n  * `int get(int i)`: gets element at index i\n* **Sets**: an unordered set of unique elements (no repeats)\n  * `add(int i)`: adds an element\n  * `contains(int i)`: returns a boolean for whether or not the set contains the value\n* **Maps**: set of key/value pairs\n  * `put(K key, V value)`: puts a key value pair into the map\n  * `V get(K key)`: gets the value corresponding to the key",
            "7. Note: the bolded ADT's are a subinterfaces of a bigger overarching interface called `Collections.`",
            "8. Below we show the relationships between the interfaces and classes. Interfaces are in white, classes are in blue.",
            '9. <figure><img src="../.gitbook/assets/image (133).png" alt=""><figcaption><p>Common interfaces in Java and their implementations</p></figcaption></figure>',
            "10. ADT's allow us to make use of object oriented programming in an efficient and elegant way. For example, you saw in Project 1C how you can use an ArrayDeque or a LinkedListArrayDeque interchangeably because they are both part of the Deque ADT.",
            "11. In the following chapters, we will work on defining some more ADT's and enumerating their different implementations.",
            '12. {% embed url="https://www.youtube.com/watch?v=aFOSePlOExw" %}\n',
        ],
        "7.-testing.md": [
            "1. # 7. Testing",
            '2. ### Testing and Selection Sort <a href="#testing-and-selection-sort" id="testing-and-selection-sort"></a>',
            "3. One of the most important skills an intermediate to advanced programmer can have is the ability to tell when your code is correct. In this chapter, we'll discuss how you can write tests to evaluate code correctness. Along the way, we'll also discuss an algorithm for sorting called Selection Sort.",
            '4. #### A New Way <a href="#a-new-way" id="a-new-way"></a>',
            "5. When you write a program, it may have errors. In a classroom setting, you gain confidence in your code's correctness through some combination of user interaction, code analysis, and autograder testing, with this last item being of the greatest importance in many cases, particularly as it is how you earn points.",
            "6. Autograders, of course, are not magic. They are code that the instructors write that is fundamentally not all that different from the code that you are writing. In the real world, these tests are written by the programmers themselves, rather than some benevolent Josh-Hug-like third party.",
            "7. In this chapter, we'll explore how we can write our own tests. Our goal will be to create a class called `Sort` that provides a method `sort(String[] x)` that destructively sorts the strings in the array `x`.",
            "8. As a totally new way of thinking, we'll start by writing `testSort()` first, and only after we've finished the test, we'll move on to writing the actual sorting code.",
            '9. #### Ad Hoc Testing <a href="#a-d-hoc-testing" id="a-d-hoc-testing"></a>',
            "10. Writing a test for `Sort.sort` is relatively straightforward, albeit tedious. We simply need to create an input, call `sort`, and check that the output of the method is correct. If the output is not correct, we print out the first mismatch and terminate the test. For example, we might create a test class as follows:",
            '11. ```java\npublic class TestSort {\n    /** Tests the sort method of the Sort class. */\n    public static void testSort() {\n        String[] input = {"i", "have", "an", "egg"};\n        String[] expected = {"an", "egg", "have", "i"};\n        Sort.sort(input);\n        for (int i = 0; i < input.length; i += 1) {\n            if (!input[i].equals(expected[i])) {\n                System.out.println("Mismatch in position " + i + ", expected: " + expected + ", but got: " + input[i] + ".");\n                break;\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        testSort();\n    }\n}\n```',
            "12. We can test out our test by creating a blank `Sort.sort` method as shown below:",
            "13. ```java\npublic class Sort {\n    /** Sorts strings destructively. */\n    public static void sort(String[] x) {        \n    }\n}\n```",
            "14. If we run the `testSort()` method with this blank `Sort.sort` method, we'd get:",
            "15. ```sh\nMismatch in position 0, expected: an, but got: i.\n```",
            "16. The fact that we're getting an error message is a good thing! This means our test is working. What's very interesting about this is that we've now created a little game for ourselves to play, where the goal is to modify the code for `Sort.sort` so that this error message no longer occurs. It's a bit of a psychological trick, but many programmers find the creation of these little mini-puzzles for themselves to be almost addictive.",
            "17. In fact, this is a lot like the situation where you have an autograder for a class, and you find yourself hooked on the idea of getting the autograder to give you its love and approval. You now have the ability to create a judge for your code, whose esteem you can only win by completing the code correctly.",
            '18. **Important note:** You may be asking "Why are you looping through the entire array? Why don\'t you just check if the arrays are equal using `==`? ". The reason is, when we test for equality of two objects, we cannot simply use the `==` operator. The `==` operator compares the literal bits in the memory boxes, e.g. `input == expected` would test whether or not the addresses of `input` and `expected` are the same, not whether the values in the arrays are the same. Instead, we used a loop in `testSort`, and print out the first mismatch. You could also use the built-in method `java.util.Arrays.equals` instead of a loop.',
            "19. While the single test above wasn't a ton of work, writing a suite of such _ad hoc_ tests would be very tedious, as it would entail writing a bunch of different loops and print statements. In the next section, we'll see how the `org.junit` library saves us a lot of work.",
            '20. #### JUnit Testing <a href="#junit-testing" id="junit-testing"></a>',
            "21. The Google Truth library provides a number of helpful methods and useful capabilities for simplifying the writing of tests. For example, we can replace our simple _ad hoc_ test from above with:",
            '22. ```java\nimport static com.google.common.truth.Truth.assertThat;\npublic class TestSort {\n   /** Tests the sort method of the Sort class. */\n   public static void testSort() {\n       String[] input = {"cows", "dwell", "above", "clouds"};\n       String[] expected = {"above", "clouds", "cows", "dwell"};\n       Sort.sort(input);\n\n       assertThat(input).isEqualTo(expected);\n   }\n\n   public static void main(String[] args) {\n       testSort();\n   }\n}\n\n```\n\n**This code is much simpler**, and does more or less the exact same thing, i.e. if the arrays are not equal, it will tell us the first mismatch. For example, if we run `testSort()` on a `Sort.sort` method that does nothing, we\'d get:\n\n```sh\nException in thread "main" arrays first differed at element [0]; expected:<[an]> but was:<[i]>\n    at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:55)\n    at org.junit.Assert.internalArrayEquals(Assert.java:532)\n    ...\n```\n\nWhile this output is a little uglier than our _ad hoc_ test, we\'ll see at the very end of this chapter how to make it nicer.\n\n#### Selection Sort <a href="#selection-sort" id="selection-sort"></a>\n\nBefore we can write a `Sort.sort` method, we need some algorithm for sorting. Perhaps the simplest sorting algorithm around is "selection sort." Selection sort consists of three steps:\n\n* Find the smallest item.\n* Move it to the front.\n* Selection sort the remaining N-1 items (without touching the front item).\n\nFor example, suppose we have the array `{6, 3, 7, 2, 8, 1}`. The smallest item in this array is `1`, so we\'d move the `1` to the front. There are two natural ways to do this: One is to stick the `1` at the front and slide all the numbers over, i.e. `{1, 6, 3, 7, 2, 8}`. However, the much more efficient way is to simply swap the `1` with the old front (in this case `6`), yielding `{1, 3, 7, 2, 8, 6}`.\n\nWe\'d simply repeat the same process for the remaining digits, i.e. the smallest item in `... 3, 7, 2, 8, 6}` is `2`. Swapping to the front, we get `{1, 2, 7, 3, 8, 6}`. Repeating until we\'ve got a sorted array, we\'d get `{1, 2, 3, 7, 8, 6}`, then `{1, 2, 3, 6, 8, 7}`, then finally `{1, 2, 3, 6, 7, 8}`.\n\nWe could mathematically prove the correctness of this sorting algorithm on any arrays by using the concept of invariants that was originally introduced in chapter 2.4, though we will not do so in this textbook. Before proceeding, try writing out your own short array of numbers and perform selection sort on it, so that you can make sure you get the idea.\n\nNow that we know how selection sort works, we can write in a few short comments in our blank `Sort.sort` method to guide our thinking:\n\n```java\npublic class Sort {\n    /** Sorts strings destructively. */\n    public static void sort(String[] x) { \n           // find the smallest item\n           // move it to the front\n           // selection sort the rest (using recursion?)\n    }\n}\n```\n\nIn the following sections, I will attempt to complete an implementation of selection sort. I\'ll do so in a way that resembles how a student might approach the problem, so **I\'ll be making a few intentional errors along the way**. These intentional errors are a good thing, as they\'ll help demonstrate the usefulness of testing. If you spot any of the errors while reading, don\'t worry, we\'ll eventually come around and correct them.\n\n#### findSmallest <a href="#findsmallest" id="findsmallest"></a>\n\nThe most natural place to start is to write a method for finding the smallest item in a list. As with `Sort.sort`, we\'ll start by writing a test before we even complete the method. First, we\'ll create a dummy `findSmallest` method that simply returns some arbitrary value:\n\n```java\npublic class Sort {\n    /** Sorts strings destructively. */\n    public static void sort(String[] x) { \n           // find the smallest item\n           // move it to the front\n           // selection sort the rest (using recursion?)\n    }\n\n    /** Returns the smallest string in x. */\n    public static String findSmallest(String[] x) {\n        return x[2];\n    }\n}\n```',
            "23. Obviously this is not a correct implementation, but we've chosen to defer actually thinking about how `findSmallest` works until after we've written a test. Using the `org.junit` library, adding such a test to our `TestSort` class is very easy, as shown below:",
            '24. ```java\npublic class TestSort {\n   @Test\n   public void testFindSmallest() {\n       String[] input = {"rawr", "a", "zaza", "newway"};\n       String expected = "zaza";\n       String actual = Sort.findSmallest(input);\n       assertThat(actual).isEqualTo(expected);\n   }\n}\n```',
            "25. As with `TestSort.testsort`, we then run our `TestSort.testFindSmallest` method to make sure that it fails. When we run this test, we'll see that it actually passes, i.e. no message appears. This is because we just happened to hard code the correct return value `x[2]`. Let's modify our `findSmallest` method so that it returns something that is definitely incorrect:",
            "26. ```java\n/** Returns the smallest string in x. */\npublic static String findSmallest(String[] x) {\n    return x[3];\n}\n```",
            "27. After making this change, when we run `TestSort.testFindSmallest`, we'll get an error, which is a good thing:",
            '28. ```sh\nException in thread "main" java.lang.AssertionError: expected:<[an]> but was:<[null]>\n    at org.junit.Assert.failNotEquals(Assert.juava:834)\n    at TestSort.testFindSmallest(TestSort.java:9)\n    at TestSort.main(TestSort.java:24)\n```',
            "29. As before, we've set up for ourselves a little game to play, where our goal is now to modify the code for `Sort.findSmallest` so that this error no longer appears. This is a smaller goal than getting `Sort.sort` to work, which might be even more addictive.",
            "30. Side note: It might have seem rather contrived that I just happened to return the right value `x[2]`. However, when I was recording this lecture video, I actually did make this exact mistake without intending to do so!",
            "31. Next we turn to actually writing `findSmallest`. This seems like it should be relatively straightforward. If you're a Java novice, you might end up writing code that looks something like this:",
            "32. ```java\n/**  Returns the smallest string in x. */\npublic static String findSmallest(String[] x) {\n    String smallest = x[0];\n    for (int i = 0; i < x.length; i += 1) {\n        if (x[i] < smallest) {\n            smallest = x[i];\n        }\n    }\n    return smallest;\n}\n```",
            "33. However, this will yield the compilation error \"< cannot be applied to 'java.lang.String'\". The issue is that Java does not allow comparisons between Strings using the < operator.",
            "34. When you're programming and get stuck on an issue like this that is easily describable, it's probably best to turn to a search engine. For example, we might search \"less than strings Java\" with Google. Such a search might yield a Stack Overflow post like [this one](https://stackoverflow.com/questions/5153496/how-can-i-compare-two-strings-in-java-and-define-which-of-them-is-smaller-than-t).",
            "35. One of the popular answers for this post explains that the `str1.compareTo(str2)` method will return a negative number if `str1 < str2`, 0 if they are equal, and a positive number if `str1 > str2`.",
            "36. Incorporating this into our code, we might end up with:",
            "37. ```java\n/** Returns the smallest string in x. \n  * @source Got help with string compares from https://goo.gl/a7yBU5. */\npublic static String findSmallest(String[] x) {\n    String smallest = x[0];\n    for (int i = 0; i < x.length; i += 1) {\n        int cmp = x[i].compareTo(smallest);\n        if (cmp < 0) {\n            smallest = x[i];\n        }\n    }\n    return smallest;\n}\n```",
            "38. Note that we've used a `@source` tag in order to cite our sources. I'm showing this by example for those of you who are taking 61B as a formal course. This is not a typical real world practice.",
            "39. Since we are using syntax features that are totally new to us, we might lack confidence in the correctness of our `findSmallest` method. Luckily, we just wrote that test a little while ago. If we try running it, we'll see that nothing gets printed, which means our code is probably correct.",
            "40. We can augment our test to increase our confidence by adding more test cases. For example, we might change `testFindSmallest` so that it reads as shown below:",
            '41. ```java\npublic static void testFindSmallest() {\n    String[] input = {"i", "have", "an", "egg"};\n    String expected = "an";\n\n    String actual = Sort.findSmallest(input);\n    assertThat(actual).isEqualTo(expected);     \n\n    String[] input2 = {"there", "are", "many", "pigs"};\n    String expected2 = "are";\n\n    String actual2 = Sort.findSmallest(input2);\n    assertThat(actual2).isEqualTo(expected2);  \n}\n```',
            "42. Rerunning the test, we see that it still passes. We are not absolutely certain that it works, but we are much more certain that we would have been without any tests.",
            '43. #### Swap <a href="#swap" id="swap"></a>',
            "44. Looking at our `sort` method below, the next helper method we need to write is something to move an item to the front, which we'll call `swap`.",
            "45. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n       // find the smallest item\n       // move it to the front\n       // selection sort the rest (using recursion?)\n}\n```",
            "46. Writing a `swap` method is very straightforward, and you've probably done so before. A correct implementation might look like:",
            "47. ```java\npublic static void swap(String[] x, int a, int b) {\n    String temp = x[a];\n    x[a] = x[b];\n    x[b] = temp;\n}\n```",
            "48. However, for the moment, let's introduce an intentional error so that we can demonstrate the utility of testing. A more naive programmer might have done something like:",
            "49. ```java\npublic static void swap(String[] x, int a, int b) {    \n    x[a] = x[b];\n    x[b] = x[a];\n}\n```",
            "50. Writing a test for this method is quite easy with the help of JUnit. An example test is shown below. Note that we have also edited the main method so that it calls `testSwap` instead of `testFindSmallest` or `testSort`.",
            '51. ```java\npublic class TestSort {\n    ...    \n\n    /** Test the Sort.swap method. */\n    public static void testSwap() {\n        String[] input = {"i", "have", "an", "egg"};\n        int a = 0;\n        int b = 2;\n        String[] expected = {"an", "have", "i", "egg"};\n\n        Sort.swap(input, a, b);\n        assertThat(expected).isEqualTo(input);\n    }\n\n    public static void main(String[] args) {\n        testSwap();\n    }\n}\n```',
            "52. Running this test on our buggy `swap` yields an error, as we'd expect.",
            '53. ```sh\nException in thread "main" arrays first differed in element [2]; expected:<[i]> but was:<[an]>\n    at TestSort.testSwap(TestSort.java:36)\n```',
            "54. It's worth briefly noting that it is important that we call only `testSwap` and not `testSort` as well. For example, if our `main` method was as below, the entire `main` method will terminate execution as soon as `testSort` fails, and `testSwap` will never run:",
            "55. ```java\npublic static void main(String[] args) {\n    testSort();\n    testFindSmallest();\n    testSwap();\n}\n```",
            "56. We will learn a more elegant way to deal with multiple tests at the end of this chapter that will avoid the need to manually specify which tests to run.",
            "57. Now that we have a failing test, we can use it to help us debug. One way to do this is to set a breakpoint inside the `swap` method and use the visual debugging feature in IntelliJ. If you would like more information about and practice on debugging, check out [Lab3](https://sp19.datastructur.es/materials/lab/lab3/lab3). Stepping through the code line-by-line makes it immediately clear what is wrong (see video or try it yourself), and we can fix it by updating our code to include a temporary variable as that the beginning of this section:",
            "58. ```java\npublic static void swap(String[] x, int a, int b) {\n    String temp = x[a];\n    x[a] = x[b];\n    x[b] = temp;\n}\n```",
            "59. Rerunning the test, we see that it now passes.",
            '60. #### Revising findSmallest <a href="#revising-findsmallest" id="revising-findsmallest"></a>',
            "61. Now that we have multiple pieces of our method done, we can start trying to connect them up together to create a `Sort` method.",
            "62. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n       // find the smallest item\n       // move it to the front\n       // selection sort the rest (using recursion?)\n}\n```",
            "63. It's clear how to use our `findSmallest` and `swap` methods, but when we do so, we immediately realize there is a bit of a mismatch: `findSmallest` returns a `String`, and `swap` expects two indices.",
            "64. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n       // find the smallest item\n       String smallest = findSmallest(x);\n\n       // move it to the front\n       swap(x, 0, smallest);\n\n       // selection sort the rest (using recursion?)\n}\n```",
            "65. In other words, what `findSmallest` should have been returning is the index of the smallest String, not the String itself. Making silly errors like this is normal and really easy to do, so don't sweat it if you find yourself doing something similar. Iterating on a design is part of the process of writing code.",
            "66. Luckily, this new design can be easily changed. We simply need to adjust `findSmallest` to return an `int`, as shown below:",
            "67. ```java\npublic static int findSmallest(String[] x) {\n    int smallestIndex = 0;\n    for (int i = 0; i < x.length; i += 1) {\n        int cmp = x[i].compareTo(x[smallestIndex]);\n        if (cmp < 0) {\n            smallestIndex = i;\n        }\n    }\n    return smallestIndex;\n}\n```",
            "68. Since this is a non-trivial change, we should also update `testFindSmallest` and make sure that `findSmallest` still works.",
            '69. ```java\npublic static void testFindSmallest() {\n    String[] input = {"i", "have", "an", "egg"};\n    int expected = 2;\n\n    int actual = Sort.findSmallest(input);\n    assertThat(actual).isEqualTo(expected); \n\n    String[] input2 = {"there", "are", "many", "pigs"};\n    int expected2 = 1;\n\n    int actual2 = Sort.findSmallest(input);\n    assertThat(actual2).isEqualTo(expected2); \n}\n```',
            "70. After modifying `TestSort` so that this test is run, and running `TestSort.main`, we see that our code passes the tests. Now, revising sort, we can fill in the first two steps of our sorting algorithm.",
            "71. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n   // find the smallest item\n   // move it to the front\n   // selection sort the rest (using recursion?)\n   int smallestIndex = findSmallest(x);\n   swap(x, 0, smallestIndex);\n}\n```",
            "72. All that's left is to somehow selection sort the remaining items, perhaps using recursion. We'll tackle this in the next section.",
            "73. Reflecting on what we've accomplished, it's worth noting how we created tests first, and used these to build confidence that the actual methods work before we ever tried to use them for anything. This is an incredibly important idea, and one that will serve you well if you decide to adopt it.",
            '74. #### Recursive Helper Methods <a href="#recursive-helper-methods" id="recursive-helper-methods"></a>',
            "75. To begin this section, consider how you might make the recursive call needed to complete `sort`:",
            "76. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n   int smallestIndex = findSmallest(x);\n   swap(x, 0, smallestIndex);\n   // recursive call??\n}\n```",
            "77. For those of you who are used to a language like Python, it might be tempting to try and use something like slice notation, e.g.",
            "78. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n   int smallestIndex = findSmallest(x);\n   swap(x, 0, smallestIndex);\n   sort(x[1:])\n}\n```",
            "79. However, there is no such thing in Java as a reference to a sub-array, i.e. we can't just pass the address of the next item in the array.",
            "80. This problem of needing to consider only a subset of a larger array is very common. A typical solution is to create a private helper method that has an additional parameter (or parameters) that delineate which part of the array to consider. For example, we might write a private helper method also called `sort` that consider only the items starting with item `start`.",
            "81. ```java\n/** Sorts strings destructively starting from item start. */\nprivate static void sort(String[] x, int start) { \n    // TODO\n}\n```",
            "82. Unlike our public sort method, it's relatively straightforward to use recursion now that we have the additional parameter `start`, as shown below. We'll test this method in the next section.",
            "83. ```java\n/** Sorts strings destructively starting from item start. */\nprivate static void sort(String[] x, int start) { \n   int smallestIndex = findSmallest(x);\n   swap(x, start, smallestIndex);\n   sort(x, start + 1);\n}\n```",
            "84. Now that we have a helper method, we need to set up the correct original call. If we set the start to 0, we effectively sort the entire array.",
            "85. ```java\n/** Sorts strings destructively. */\npublic static void sort(String[] x) { \n   sort(x, 0);\n}\n```",
            "86. This approach is quite common when trying to use recursion on a data structure that is not inherently recursive, e.g. arrays.",
            '87. #### Debugging and Completing Sort <a href="#debugging-and-completing-sort" id="debugging-and-completing-sort"></a>',
            "88. Running our `testSort` method, we immediately run into a problem:",
            '89. ```sh\nException in thread "main" java.lang.ArrayIndexOutOfBoundsException: 4\n    at Sort.swap(Sort.java:16)\n```',
            "90. Using the Java debugger, we see that the problem is that somehow `start` is reaching the value 4. Stepping through the code carefully (see video above), we find that the issue is that we forgot to include a base case in our recursive `sort` method. Fixing this is straightforward:",
            "91. ```java\n/** Sorts strings destructively starting from item start. */\nprivate static void sort(String[] x, int start) { \n   if (start == x.length) {\n       return;\n   }\n   int smallestIndex = findSmallest(x);\n   swap(x, start, smallestIndex);\n   sort(x, start + 1);\n}\n```",
            "92. Rerunning this test again, we get another error:",
            '93. ```sh\nException in thread "main" arrays first differed at element [0]; \n   expected<[an]> bit was:<[have]>\n```',
            "94. Again, with judicious use of the IntelliJ debugger (see video), we can identify a line of code whose result does not match our expectations. Of note is the fact that I debugged the code at a higher level of abstraction than you might have otherwise, which I achieve by using `Step Over` more than `Step Into`. As discussed in lab 3, debugging at a higher level of abstraction saves you a lot of time and energy, by allowing you to compare the results of entire function calls with your expectation.",
            '95. Specifically, we find that when sorting the last 3 (out of 4) items, the `findSmallest` method is giving as the 0th item (`"an"`) rather than the 3rd item (`"egg"`) when called on the input `{"an", "have", "i", "egg"}`. Looking carefully at the definition of `findSmallest`, this behavior is not a surprise, since `findSmallest` looks at the entire array, not just the items starting from position `start`. This sort of design flaw is very common, and writing tests and using the debugger is a great way to go about fixing them.',
            "96. To fix our code, we revise `findSmallest` so that it takes a second parameter `start`, i.e. `findSmallest(String[] x, int start)`. In this way, we ensure that we're finding the smallest item only out of the last however many are still unsorted. The revision is as shown below:",
            "97. ```java\npublic static int findSmallest(String[] x, int start) {\n    int smallestIndex = start;\n    for (int i = start; i < x.length; i += 1) {\n        int cmp = x[i].compareTo(x[smallestIndex]);\n        if (cmp < 0) {\n            smallestIndex = i;\n        }\n    }\n    return smallestIndex;\n}\n```",
            "98. Given that we've made a significant change to one of our building blocks, i.e. `findSmallest`, we should ensure that our changes are correct.",
            "99. We first modify `testFindSmallest` so that it uses our new parameter, as shown below:",
            '100. ```java\npublic static void testFindSmallest() {\n    String[] input = {"i", "have", "an", "egg"};\n    int expected = 2;\n\n    int actual = Sort.findSmallest(input, 0);\n    assertThat(actual).isEqualTo(expected);        \n\n    String[] input2 = {"there", "are", "many", "pigs"};\n    int expected2 = 2;\n\n    int actual2 = Sort.findSmallest(input2, 2);\n    assertThat(actual2).isEqualTo(expected2); \n}\n```',
            "101. We then modify `TestSort.main` so that it runs `testFindSmallest`. This test passes, strongly suggesting that our revisions to `findSmallest` were correct.",
            "102. We next modify `Sort.sort` so that it uses the new `start` parameter in `findSmallest`:",
            "103. ```java\n/** Sorts strings destructively starting from item start. */\nprivate static void sort(String[] x, int start) { \n   if (start == x.length) {\n       return;\n   }\n   int smallestIndex = findSmallest(x, start);\n   swap(x, start, smallestIndex);\n   sort(x, start + 1);\n}\n```",
            '104. We then modify `TestSort` so that it runs `TestSort.sort` and voila, the method works. We are done! You have now seen the "new way" from the beginning of this lecture, which we\'ll reflect on for the remainder of this chapter.',
            '105. #### Reflections on the Development Process <a href="#reflections-on-the-development-process" id="reflections-on-the-development-process"></a>',
            "106. When you're writing and debugging a program, you'll often find yourself switching between different contexts. Trying to hold too much in your brain at once is a recipe for disaster at worst, and slow progress at best.",
            "107. Having a set of automated tests helps reduce this cognitive load. For example, we were in the middle of writing `sort` when we realized there was a bug in `findSmallest`. We were able to switch contexts to consider `findSmallest` and establish that it was correct using our `testFindSmallest` method, and then switch back to `sort`. This is in sharp contrast to a more naive approach where you would simply be calling `sort` over and over and trying to figure out if the behavior of the overall algorithm suggests that the `findSmallest` method is correct.",
            "108. As an analogy, you could test that a parachute's ripcord works by getting in an airplane, taking off, jumping out, and pulling the ripcord and seeing if the parachute comes out. However, you could also just pull it on the ground and see what happens. So, too, is it unnecessary to use `sort` to try out `findSmallest`.",
            "109. As mentioned earlier in this chapter, tests also allow you to gain confidence in the basic pieces of your program, so that if something goes wrong, you have a better idea of where to start looking.",
            "110. Lastly, tests make it easier to refactor your code. Suppose you decide to rewrite `findSmallest` so that it is faster or more readable. We can safely do so by making our desired changes and seeing if the tests still work.",
            "111. #### More Testing Features",
            "112. If we add `@Test` before a method AND make the function non-static, green arrows appear.",
            "113. * The single green arrow by testSort means \u201crun this function\u201d.\n* The double green arrow means run all tests in this class.",
            "114. ![](https://lh7-us.googleusercontent.com/AxG53TwAIPggHLPD\\_XgOBQkOXuaAfUs5lyTY7\\_WBhpeS8EmNiNFppveTHIXU0HUgji\\_NxgpI6v6wAfGJfQuMDoRkSFm78\\_fL5PT\\_1QkjfiEyrTv7GSIPhWOSX9P\\_RhKgw6XKc\\_-M0K-992\\_ktzGMtEokSQ=s2048)",
            "115. The reason why the function has to be non-static is unclear, though this probably has to do with things happening behind the scene.",
            "116. One added benefit of doing this is that IntelliJ will now gamify bug fixing and design. You have concrete mini-goals and your progress is summarized in bottom left. You win when you get green checks for every test.",
            "117. ![](https://lh7-us.googleusercontent.com/BRu\\_eKYg7yRI7jAqgGXnHzN4CvNiKWtVMq-SbDC9vzjS2Vd8rNbGB-GBUWcGxyinejZWzO-dmWbJ2WGa3jRDpJ56yPXHExjkUJYfCL46V7stzGSFT2dOO0FsrIQqgOLdE6iFfrVvCfr0n6IN1EjQcRXYnw=s2048)",
            '118. #### Testing Philosophy <a href="#testing-philosophy" id="testing-philosophy"></a>',
            "119. **Correctness Tool #1: Autograder**",
            "120. Let's go back to ground zero. The autograder was likely the first correctness tool you were exposed to. Our autograder is in fact based on JUnit plus some extra custom libraries.",
            "121. There are some great benefits to autograders. Perhaps most importantly, it verifies correctness for you, saving you from the tedious and non-instructive task of writing all of your own tests. It also gamifies the assessment process by providing juicy points as an incentive to acheiving correctness. This can also backfire if students spend undue amounts of time chasing final points that won't actually affect their grade or learning.",
            "122. However, autograders don't exist in the real world and relying on autograders can build bad habits. One's workflow is hindered by sporadically uploading your code and waiting for the autograder to run. _Autograder Driven Development_ is an extreme version of this in which students write all their code, fix their compiler errors, and then submit to the autograder. After getting back errors, students may try to make some changes, sprinkle in print statements, and submit again. And repeat. Ultimately, you are not in control of either your workflow or your code if you rely on an autograder.",
            "123. **Correctness Tool #2: JUnit Tests**",
            "124. JUnit testing, as we have seen, unlocks a new world for you. Rather than relying on an autograder written by someone else, you write tests for each piece of your program. We refer to each of these pieces as a unit. This allows you to have confidence in each unit of your code - you can depend on them. This also helps decrease debugging time as you can isolate attention to one unit of code at a time (often a single method). Unit testing also forces you to clarify what each unit of code should be accomplishing.",
            "125. There are some downsides to unit tests, however. First, writing thorough tests takes time. It's easy to write incomplete unit tests which give a false confidence to your code. It's also difficult to write tests for units that depend on other units (consider the `addFirst` method in your `LinkedListDeque`).",
            "126. _**Test-Driven Development (TDD)**_",
            "127. TDD is a development process in which we write tests for code before writing the code itself. The steps are as follows:",
            "128. 1. Identify a new feature.\n2. Write a unit test for that feature.\n3. Run the test. It should fail.\n4. Write code that passes the test. Yay!\n5. Optional: refactor code to make it faster, cleaner, etc. Except now we have a reference to tests that should pass.",
            "129. Test-Driven Development is not required in this class and may not be your style but unit testing in general is most definitely a good idea.",
            "130. **Correctness Tool #3: Integration Testing**",
            "131. Unit tests are great but we should also make sure these units work properly together ([unlike this meme](https://media.giphy.com/media/3o7rbPDRHIHwbmcOBy/giphy.gif)). Integration testing verifies that components interact properly together. JUnit can in fact be used for this. You can imagine unit testing as the most nitty gritty, with integration testing a level of abstraction above this.",
            "132. The challenge with integration testing is that it is tedious to do manually yet challenging to automate. And at a high level of abstraction, it's easy to miss subtle or rare errors.",
            "133. As a summary, you should **definitely write tests but only when they might be useful!** Taking inspiration from TDD, writing your tests before writing code can also be very helpful in some cases.\n",
        ],
    },
};
